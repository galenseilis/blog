{
  "hash": "5d7ea125d45123b01290e7512c73bd34",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Using the DKW(M) Inequality to Calculate Sample Size\"\nauthor: \"Galen Seilis\"\ndate: \"2024-07-30\"\ncategories: [DKWM Inequality, Statistics, Cumulative Probability Distribution, Nonparametric Statistics]\nbibliography: references.bib\ndraft: true\n---\n\n\n## Derivations of Sample Size Bounds\n\nIn this section I am going to cover the mathematics needed to derive the sample size bounds.\n\n### Univariate Case\n\nThe two-sided estimate for the DKWM inequality is given by\n\n$$\\Pr \\left[ \\sup_{x \\in \\mathbb{R}} \\lvert F_n(x) - F(x) \\rvert > \\epsilon \\right] \\leq 2 e^{-2n\\epsilon^2}$$\n\nLet us define\n\n$$\\alpha \\triangleq 1 - \\Pr \\left[ \\sup_{x \\in \\mathbb{R}} \\lvert F_n(x) - F(x) \\rvert > \\epsilon \\right] = \\Pr \\left[ \\sup_{x \\in \\mathbb{R}} \\lvert F_n(x) - F(x) \\rvert \\leq \\epsilon \\right]$$\n\nso that\n\n\n$$1 - \\alpha \\leq 2 e^{-2n\\epsilon^2}.$$\n\nWe can directly solve for an upper bound on $n$:\n\n$$n \\leq \\frac{\\ln 2 - \\ln (1 - \\alpha)}{2 \\epsilon^2}$$\n\nClearly the sample size is an positive integer whereas the bound we have place on it is not. We can take the ceil round to simplify our conclusion about the sample size:\n\n\n$$n \\leq \\left\\lceil \\frac{\\ln 2 - \\ln (1 - \\alpha)}{2 \\epsilon^2} \\right\\rceil$$\n\n\n### Multivariate Case\n\nThe [multivariate DKW inequality](https://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality#Multivariate_case) is given by\n\n$$\\Pr \\left[ \\sup_{t \\in \\mathbb{R}^k} \\left| F_n(t) - F(t)  \\right| > \\epsilon \\right] \\leq (n+1)k e^{-2n\\epsilon^2},$$\n\nwhere $F_n(t)$ is the [eCDF](https://en.wikipedia.org/wiki/Empirical_distribution_function), $F(t)$ is the population [CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function), $\\epsilon \\in \\mathbb{R}_{>0}$, and $n,k \\in \\mathbb{N}$.\n\nWith the multivariate approach we cannot take the direct approach of algebraically obtaining an exact answer for an upper bound on $n$. I tried to apply the [Lambert W function](https://en.wikipedia.org/wiki/Lambert_W_function), but I did not find a way to make that approach work (@647066).\n\nThe approach I found fruitful is due to @647096, and I will go over the steps below. The first step is to define \n\n$$f(n; p, k, \\epsilon) = \\ln (1 + n) - \\ln \\left( \\frac{p}{k} \\right) - 2 \\epsilon^2 n$$\n\nwhere \n\n$$p \\triangleq \\Pr \\left[ \\sup_{t \\in \\mathbb{R}^k} \\left| F_n(t) - F(t)  \\right| > \\epsilon \\right]$$\n\nfor the purposes of brevity. Notice that $f$ is a monotonic function of the upper bound, which means that it will preserve the location of optima.\n\nNext we will find where $f$ is at its maximum at a fixed $p$, $k$,  and $\\epsilon$. This optimization problem can be expressed by:\n\n$$n_* = \\arg \\max_n f(n; p, k, \\epsilon)$$\n\nWe can solve this optimization problem exactly using univariable calculus. First we find the derivative:\n\n$$\\frac{df}{dn} = \\frac{1}{1+n} - 2 \\epsilon^2$$\n\nNext we assign the derivative to be zero, and solve for the optimal value of $n$.\n\n$$\\frac{df}{dn} := 0 \\implies 0=\\frac{1}{1+n_*} - 2 \\epsilon^2 \\iff n_* = \\frac{1}{2 \\epsilon^2} - 1$$\n\nNow let us consider the Taylor series of $f$ at $n_*$.\n\n::: {#92124b15 .cell execution_count=1}\n``` {.python .cell-code}\nimport sympy\n\nn = sympy.Symbol('n', real=True, positive=True)\np = sympy.Symbol('p', real=True, positive=True)\nk = sympy.Symbol('k', integer=True, positive=True)\nepsilon = sympy.Symbol('\\\\epsilon', real=True, positive=True)\n\nf = sympy.log(1 + n) - sympy.log(p / k) - 2 * epsilon ** 2 * n\nf.series(n, 1 / (2 * epsilon ** 2) - 1, 3)\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=2}\n$\\displaystyle -1 - \\log{\\left(\\frac{p}{k} \\right)} + \\log{\\left(\\frac{1}{2 \\epsilon^{2}} \\right)} + 2 \\epsilon^{2} - 2 \\epsilon^{4} \\left(n + 1 - \\frac{1}{2 \\epsilon^{2}}\\right)^{2} + O\\left(\\left(n - \\frac{1 - 2 \\epsilon^{2}}{2 \\epsilon^{2}}\\right)^{3}; n\\rightarrow \\frac{1 - 2 \\epsilon^{2}}{2 \\epsilon^{2}}\\right)$\n:::\n:::\n\n\nSince this Taylor series goes on forever, we can choose a finite number of terms as an approximation of $f$. We know that this produces a polynomial which is guaranteed to have no more roots than the degree of the polynomial because of the fundamental theorem of algebra. There will always be exactly the degree number of roots on the complex plane, but we only care about non-negative real roots.\n\n## Applications\n\nA plausible application is to prescribe the sample size for simulations, including discrete event simulation.\n\nSometimes it is possible to get large IID samples, in which cases you could prescribe the sample size to be collected.\n\nAlthough not a bound on the sample size itself, the DKW(M) inequality can also be used to \n\n## Limitations\n\nThere are two substantial limitations of these sample size bounds. \n\nThe first comes from the assumption of the theorem that we must be considering independent sampling.\n\nThe second limitation is that the bound will tend to be quite large. In a sense this limitation comes from the generality of this bound. Some distributions converge more slowly than others, and such a general bound must hold for even the most slowly converging distributios.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}