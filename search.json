[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Galen Seilis",
    "section": "",
    "text": "Galen is a science and technology enthusiast dedicated to life-long learning. Through the application of ethics, reason, and observation, he believes we can tackle problems in medicine, the environment, economics, and exploration."
  },
  {
    "objectID": "posts/how-to-compile-python-on-linux/index.html",
    "href": "posts/how-to-compile-python-on-linux/index.html",
    "title": "How to Compile CPython on Debian-Based Linux",
    "section": "",
    "text": "This is a short blog post to remind myself how to compile CPython from its source.\nYou need to get the source files for Python as you need to give the C compiler (and other tools) the needed instructions for producing machine code. The CPython source is available on Github. Using git, you can download with\n$ git clone https://github.com/python/cpython.git\nYou should install build-essential which provides tooling for building Debian packages. This can be done with apt:\n$ sudo apt install build-essential\nNext install these assorted packages:\n$ sudo apt install libssl-dev zlib1g-dev libncurses5-dev libncursesw5-dev libreadline-dev libsqlite3-dev libgdbm-dev libdb5.3-dev libbz2-dev liblzma-dev libffi-dev\nHere is a brief description of each package:\n\n\n\nInstall\nPackage\nDescription\n\n\n\n\nlibssl-dev\nSecure Sockets Layer toolkit - development files\nThis package is part of the OpenSSL project’s implementation of the SSL and TLS crypographic protocols for secure communication over the internet.\n\n\nzlib1g-dev\nCompression library - development\nzlib is a library implementing the deflate compression method found in gzip and PKZIP.\n\n\nlibncurses5-dev\nTransitional pacakge for libncurses-dev\nPackage prociding libncurses5-dev.\n\n\nlibncursesw5-dev\nTransitional package for libncurses-dev\nPackage providing libncursesw5-dev.\n\n\nlibreadline-dev\nGNU readline and history libraries, development files\nThe GNU readline library aids in the consistency of user interface across discrete programs that need to provide a command line interace\n\n\nlibsqlite3-dev\nlibsqlite3-dev\nSQLite is a C library that implements an SQL database engine.\n\n\nlibgdbm-dev\nGNU dbm database routines (development files)\nGNU dbm (‘gdbm’) is a library of database functions that use extensible hashing and works similarly to the standard UNIX ‘dbm’ functions.\n\n\nlibdb5.3-dev\nBerkeley v5.3 Database Libraries [development]\nThis is the development package which contains headers and static libraries for the Berkely v5.3 database library\n\n\nlibbz2-dev\nHigh-quality block-sorting file compressor library - development\nStatic libraries and include files for the bzip2 compressor library\n\n\nliblzma-dev\nXZ-format compression library - development files\nXZ is the successor to the Lempel-Ziv/Markov-chain Algorithm compression format, which provides memory-hungry but powerful compressoin (often better than bzip2) and fast, easy decompression.\n\n\nlibffi-dev\nForeign Fucntion Interface library (development files)\nThis package contains the headers and static library files necessary for building programs which use libffi\n\n\n\nNow run the configuration tool that the Python dev’s have kindly prepared. It will prepare a makefile for everything you need to build CPython.\n$ ./configure --with-pydebug\nThe --with-pydebug tells configure to use a debugging hook.\nFinally, you can just run make.\n$ make\nThat’s pretty much it. If you want to silence the large standard output, you can add an -s. By default make will compile the first target specified in the make file, which for this project is actually the entirety of CPython itself. You can specify special build targets related to building, testing, installation, and other topics."
  },
  {
    "objectID": "posts/combining-kedro-with-rye/index.html",
    "href": "posts/combining-kedro-with-rye/index.html",
    "title": "Combining Kedro with Rye",
    "section": "",
    "text": "I recently asked on the Kedro Slack channel about what experience people have had with combining Kedro with package management tools in Python such as PDM, Poetry, Hatch or Rye. juanlu gave a couple of options. You can either initialize a Kedro project first and then add the package manager, or add the package manager first and use kedro-init to fill in a Kedro project. Which one is more appropriate will depend on what already exists in your project. Kedro should be compatible with PEP-compliant packages (see discussion here) and also Poetry. I’m not sure about Rye.\nkedro-init is in its infancy (e.g. still needing documentation), but I figured I would try it out with Rye since that is what I am currently using on my personal machine."
  },
  {
    "objectID": "posts/combining-kedro-with-rye/index.html#introduction",
    "href": "posts/combining-kedro-with-rye/index.html#introduction",
    "title": "Combining Kedro with Rye",
    "section": "",
    "text": "I recently asked on the Kedro Slack channel about what experience people have had with combining Kedro with package management tools in Python such as PDM, Poetry, Hatch or Rye. juanlu gave a couple of options. You can either initialize a Kedro project first and then add the package manager, or add the package manager first and use kedro-init to fill in a Kedro project. Which one is more appropriate will depend on what already exists in your project. Kedro should be compatible with PEP-compliant packages (see discussion here) and also Poetry. I’m not sure about Rye.\nkedro-init is in its infancy (e.g. still needing documentation), but I figured I would try it out with Rye since that is what I am currently using on my personal machine."
  },
  {
    "objectID": "posts/combining-kedro-with-rye/index.html#example",
    "href": "posts/combining-kedro-with-rye/index.html#example",
    "title": "Combining Kedro with Rye",
    "section": "Example",
    "text": "Example\nFirst, lets initialize a Rye-managed project called try-kedro-init.\n$ rye init try-kedro-init\nsuccess: Initialized project in /home/galen/projects/try-kedro-init\n  Run `rye sync` to get started\nNow change directory into the project path.\n$ cd try-kedro-init/\nAdd the kedro-init package to try-kedro-init’s packages, inlcuding Kedro itself.\n$ rye add kedro-init\nInitializing new virtualenv in /home/galen/projects/try-kedro-init/.venv\nPython version: cpython@3.12.3\nAdded kedro-init&gt;=0.1.0 as regular dependency\nReusing already existing virtualenv\nGenerating production lockfile: /home/galen/projects/try-kedro-init/requirements.lock\nGenerating dev lockfile: /home/galen/projects/try-kedro-init/requirements-dev.lock\nInstalling dependencies\nResolved 55 packages in 12ms\n   Built try-kedro-init @ file:///home/galen/projects/try-kedro-init\n   Built antlr4-python3-runtime==4.9.3\nDownloaded 35 packages in 3.23s\nInstalled 55 packages in 16ms\n + antlr4-python3-runtime==4.9.3\n + arrow==1.3.0\n + attrs==23.2.0\n + binaryornot==0.4.4\n + build==1.2.1\n + cachetools==5.3.3\n + certifi==2024.6.2\n + chardet==5.2.0\n + charset-normalizer==3.3.2\n + click==8.1.7\n + cookiecutter==2.6.0\n + dynaconf==3.2.5\n + fastjsonschema==2.20.0\n + fsspec==2024.6.1\n + gitdb==4.0.11\n + gitpython==3.1.43\n + idna==3.7\n + importlib-metadata==7.2.1\n + importlib-resources==6.4.0\n + installer==0.7.0\n + jinja2==3.1.4\n + kedro==0.19.6\n + kedro-init==0.1.0\n + markdown-it-py==3.0.0\n + markupsafe==2.1.5\n + mdurl==0.1.2\n + more-itertools==10.3.0\n + omegaconf==2.3.0\n + packaging==24.1\n + parse==1.20.2\n + platformdirs==4.2.2\n + pluggy==1.5.0\n + pre-commit-hooks==4.6.0\n + pygetimportables==0.2.1\n + pygments==2.18.0\n + pyproject-hooks==1.1.0\n + python-dateutil==2.9.0.post0\n + python-slugify==8.0.4\n + pytoolconfig==1.3.1\n + pyyaml==6.0.1\n + requests==2.32.3\n + rich==13.7.1\n + rope==1.13.0\n + ruamel-yaml==0.18.6\n + ruamel-yaml-clib==0.2.8\n + six==1.16.0\n + smmap==5.0.1\n + text-unidecode==1.3\n + toml==0.10.2\n + tomlkit==0.12.5\n + try-kedro-init==0.1.0 (from file:///home/galen/projects/try-kedro-init)\n + types-python-dateutil==2.9.0.20240316\n + urllib3==2.2.2\n + validate-pyproject==0.18\n + zipp==3.19.2\nDone!\nNow run kedro-init from within Rye’s virtual environment.\n$ rye run kedro-init .\n[08:33:20] Looking for existing package directories                                                                                                                                                                                cli.py:25\n[08:33:25] Initialising config directories                                                                                                                                                                                         cli.py:25\n           Creating modules                                                                                                                                                                                                        cli.py:25\n           🔶 Kedro project successfully initialised!\nJust for the sake of example, create an example pipeline.\n$ rye run kedro pipeline create example_pipeline\nUsing pipeline template at: '/home/galen/projects/try-kedro-init/.venv/lib/python3.12/site-packages/kedro/templates/pipeline'\nCreating the pipeline 'example_pipeline': OK\n  Location: '/home/galen/projects/try-kedro-init/src/try_kedro_init/pipelines/example_pipeline'\nCreating '/home/galen/projects/try-kedro-init/tests/pipelines/example_pipeline/test_pipeline.py': OK\nCreating '/home/galen/projects/try-kedro-init/tests/pipelines/example_pipeline/__init__.py': OK\nCreating '/home/galen/projects/try-kedro-init/conf/base/parameters_example_pipeline.yml': OK\n\nPipeline 'example_pipeline' was successfully created.\nNow take a look at the path tree to see what has been created.\n$ tree .\n.\n├── conf\n│   ├── base\n│   │   └── parameters_example_pipeline.yml\n│   └── local\n├── pyproject.toml\n├── README.md\n├── requirements-dev.lock\n├── requirements.lock\n├── src\n│   └── try_kedro_init\n│       ├── __init__.py\n│       ├── pipeline_registry.py\n│       ├── pipelines\n│       │   └── example_pipeline\n│       │       ├── __init__.py\n│       │       ├── nodes.py\n│       │       └── pipeline.py\n│       ├── __pycache__\n│       │   ├── __init__.cpython-312.pyc\n│       │   └── settings.cpython-312.pyc\n│       └── settings.py\n└── tests\n    └── pipelines\n        └── example_pipeline\n            ├── __init__.py\n            └── test_pipeline.py\n\n11 directories, 15 files\nThe catalog.yml and parameters.yml files were not made by default, but they are just plaintext files that can be readily added. There is parameters_example_pipeline.yml for the pipeline we just created.\n$ touch conf/base/catalog.yml\nThere also is not a data path by default, which should exist at the root of the project. We can also add that.\n $ mkdir data\nLet us create an example CSV dataset at data/example_data.csv with the following contents:\nID,Name,Age,Email\n1,John Doe,28,john.doe@example.com\n2,Jane Smith,34,jane.smith@example.com\n3,Bob Johnson,45,bob.johnson@example.com\n4,Alice Williams,23,alice.williams@example.com\n5,Michael Brown,37,michael.brown@example.com\nThen add an entry to conf/base/catalog.yml:\nexample_dataset:\n  type: pandas.CSVDataset\n  filepath: ./data/example_data.csv\n  load_args:\n    sep: \",\"\nNow update src/try_kedro_init/pipelines/example_pipeline/pipeline.py from this\n\"\"\"\nThis is a boilerplate pipeline 'example_pipeline'\ngenerated using Kedro 0.19.6\n\"\"\"\n\nfrom kedro.pipeline import Pipeline, pipeline\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([])\nto this:\n\"\"\"\nThis is a boilerplate pipeline 'example_pipeline'\ngenerated using Kedro 0.19.6\n\"\"\"\n\nfrom kedro.pipeline import Pipeline, pipeline, node\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([\n        node(\n            func=print,\n            inputs=['example_dataset'],\n            outputs=None\n            )\n        ])\nNow install kedro-datasets and pandas:\n$ rye add kedro-datasets pandas\nAdded kedro-datasets&gt;=3.0.1 as regular dependency\nAdded pandas&gt;=2.2.2 as regular dependency\nReusing already existing virtualenv\nGenerating production lockfile: /home/galen/projects/try-kedro-init/requirements.lock\nGenerating dev lockfile: /home/galen/projects/try-kedro-init/requirements-dev.lock\nInstalling dependencies\nResolved 61 packages in 14ms\n   Built try-kedro-init @ file:///home/galen/projects/try-kedro-init\nDownloaded 1 package in 217ms\nUninstalled 1 package in 0.29ms\nInstalled 5 packages in 45ms\n + numpy==2.0.0\n + pandas==2.2.2\n + pytz==2024.1\n - try-kedro-init==0.1.0 (from file:///home/galen/projects/try-kedro-init)\n + try-kedro-init==0.1.0 (from file:///home/galen/projects/try-kedro-init)\n + tzdata==2024.1\nDone!\nFinally, run the Kedro pipeline:\n$ rye run kedro run\n[07/01/24 09:18:48] INFO     Kedro project try-kedro-init                                                                                                                                                                     session.py:324\n[07/01/24 09:18:49] INFO     Using synchronous mode for loading and saving data. Use the --async flag for potential performance gains.                                                                               sequential_runner.py:64\n                             https://docs.kedro.org/en/stable/nodes_and_pipelines/run_a_pipeline.html#load-and-save-asynchronously                                                                                                          \n                    INFO     Loading data from example_dataset (CSVDataset)...                                                                                                                                           data_catalog.py:508\n                    INFO     Running node: print([example_dataset]) -&gt; None                                                                                                                                                      node.py:361\n   ID            Name  Age                       Email\n0   1        John Doe   28        john.doe@example.com\n1   2      Jane Smith   34      jane.smith@example.com\n2   3     Bob Johnson   45     bob.johnson@example.com\n3   4  Alice Williams   23  alice.williams@example.com\n4   5   Michael Brown   37   michael.brown@example.com\n                    INFO     Completed 1 out of 1 tasks                                                                                                                                                              sequential_runner.py:90\n                    INFO     Pipeline execution completed successfully.                                                                                                                                                        runner.py:119\nMy provisional conclusion is that Kedro and Rye are compatible."
  },
  {
    "objectID": "posts/combining-kedro-with-rye/index.html#versions",
    "href": "posts/combining-kedro-with-rye/index.html#versions",
    "title": "Combining Kedro with Rye",
    "section": "Versions",
    "text": "Versions\nRye configuration:\n$ rye --version\nrye 0.35.0\ncommit: 0.35.0 (a1dbc56d4 2024-06-24)\nplatform: linux (x86_64)\nself-python: cpython@3.12.3\nsymlink support: true\nuv enabled: true\nMy operating system:\n$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 22.04.4 LTS\nRelease:        22.04\nCodename:       jammy"
  },
  {
    "objectID": "posts/unzip-inaturalist-observations-ubuntu/index.html",
    "href": "posts/unzip-inaturalist-observations-ubuntu/index.html",
    "title": "Unzip Your iNaturalist Observations On A Ubuntu System",
    "section": "",
    "text": "Note\n\n\n\nThis post was migrated from my iNaturalist journal to my Jekyll blog on 2023-02-26. It was then migrated to my Quarto blog on 2024-07-27.\n\n\nSuppose - you have just downloaded an export of iNaturalist data, - the data file is a zipped comma-separated value(CSV) text file, - and you are running a Ubuntu system.\nBegin by opening up a BASH environment.\nGo to the path where the file was downloaded to:\ncd /path/to/folder\nThen run the unzip command:\nunzip observations-&lt;ID&gt;.csv.zip\nYou should find that you now have the uncompressed CSV file. You can check with:\nls observations-&lt;ID&gt;.csv"
  },
  {
    "objectID": "posts/automorphism-orbits-of-graphlets/index.html",
    "href": "posts/automorphism-orbits-of-graphlets/index.html",
    "title": "Automorphism Orbits of Graphlets",
    "section": "",
    "text": "Back in May 2020 I released a video on YouTube for the HackSeq event:\n\n\nBut I figure I could give some written description as well, which is what the rest of this blog post covers.\nA graph is a 2-tuple containing a set of edges and a set of vertices, and the set of edges is a subset of the cartesian product of the set of vertices with itself. We can think the vertices as ‘things’ and the set of edges as a binary relation between them.\nSometimes graphs are called “networks” when either the vertices (often called “nodes” when discussing networks) or edges have additional attributes, however this usage is not universally accepted. Three common properties of graph edges are whether they are directed, signed, or weighted. In the case of directed edges, some sort of ‘directionality’ is associated with the edges, and we’d say that (u,v) and (v,u) are considered distinct for two vertices u and v from the graph. A graph with directed edges is called a directed graph or a digraph. In the case of signed edges, each edge is assigned a ‘positive’ or ‘negative’ value, and we call such a graph a signed graph. In the case of weighted edges, each edge is assigned a numerical value, and we call such a graph a weighted graph. If a graph doesn’t have directed, signed, or weighted edges and doesn’t have multiple edges for any given pair of vertices, then it is called a simple graph.\nA graphlet is a specific type of graph, inheriting all the properties of graphs but also being a weakly-connected induced subgraph. For a graph to be subgraph, there exists another graph such that the subgraph’s vertex set is a subset of the other graph’s vertex set and the subgraph’s edge set is a subset of the other graph’s edge set. A subgraph being induced can be thought of procedurally, by first selecting any subset of the vertices and then also selecting all edges that connect those selected vertices. The property of weakly-connected can be considered for any graph, and means that there exists a path between any pair of vertices in the graph.\nA function is injective if it satisfies that f(x)=f(y) implies that x=y.\n\n\n\nhttps://mathworld.wolfram.com/images/eps-gif/Injection_1000.gif\n\n\nA function is surjective if it satisfies for any element b in the image that there exists an element a of the domain for which b=f(a).\n\n\n\nhttps://mathworld.wolfram.com/images/eps-gif/Surjection_1000.gif\n\n\nA bijection is a function that is both injective (one-to-one) and surjective (onto) from one set to another (these two sets can be the same set).\n\n\n\nhttps://mathworld.wolfram.com/images/eps-gif/Bijection_1000.gif\n\n\nA graph isomorphism is a bijection f between two graphs (which can be the same graph in the case of graph automorphisms) such that any two vertices u and v in the first graph are adjacent if-and-only-if f(u) and f(v) are adjacent.\n\n\n\nhttps://upload.wikimedia.org/wikipedia/commons/9/9a/Graph_isomorphism_a.svg\n\n\n\n\n\nhttps://upload.wikimedia.org/wikipedia/commons/8/84/Graph_isomorphism_b.svg\n\n\nA graph automorphism is a graph isomorphism where the domain graph and the image graph are the same graph.\n\n\n\nhttps://mathworld.wolfram.com/images/eps-gif/GraphAutomorphismGridGraph_1000.gif\n\n\nAn equivalence relation is a binary relation that is reflexive, symmetric, and transitive. An equivalence class is a subset of a set such that all members of the subset adhere to an equivalence relation. A (vertex) orbit automorphism is an equivalence class from the vertex set of a graph under the action of a graph automorphism. Multiple graph automorphisms are possible, and the set of all automorphisms with composition of permutations of the vertex set is called a permutation group.\nSince graphlets are graphs, and orbit automorphisms can be found within graphs, we can find orbit automorphisms within graphlets. The idea behind enumeration of orbit automorphisms of graphlets is to count the number of times each vertex of a graph participates in each orbit automorphism of each graphlet from a set of graphlets. While each finite graph has a finite number of distinct (i.e. mutually non-isomorphic) graphlets, considering every conceivable graphlet would be computationally infeasible. Instead of considering all graphlets of a graph, a constraint is often imposed where graphlets containing only 2-3 vertices are considered.\nEnumeration of orbit automorphisms of graphlets has been used to characterize correlation networks of coexpression of genes, and characterize the role of traders in the world trade network."
  },
  {
    "objectID": "posts/c-execution-for-quarto/index.html",
    "href": "posts/c-execution-for-quarto/index.html",
    "title": "Executable C Code in Quarto",
    "section": "",
    "text": "In Executable Rust Code in Quarto I made a rough implementation of having Rust code compiled and its output rendered.\nWith some small adjustments we can do the same for other languages, including C. Here is a “Hello, World” example.\n#include &lt;stdio.h&gt;\n\nint main() {\n    printf(\"Hello, World!\\n\");\n    return 0;\n}\nHello, World!\nSimilar to the lessons learned from the Rust implementation, there is plausibly a better implementation with an entirely different starting point."
  },
  {
    "objectID": "posts/starting-data-science/index.html",
    "href": "posts/starting-data-science/index.html",
    "title": "Starting Data Science",
    "section": "",
    "text": "I’ve been thinking about Data Science lately, and I recently watched a YouTube video describing how to get started learning prerequisite knowledge for this field. While I am skeptical of the use of buzzwords, I think data science does reflect a loose collection of ideas and tools that are interesting and useful. Some of these subtopics include statistics, algorithms, databases, machine learning, and other miscellaneous topics within computer science and mathematics. I’m not partial to the term, but I am to the associated skillset.\nIn the interest of learning this subject in a structured way, I am going to go through the curricula suggested by Giles McMullen-Klein. I’ll modify it as it suites my needs or interests, but it serves as a simple template to get started with."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Executable C Code in Quarto\n\n\n\n\n\n\nQuarto\n\n\nC\n\n\nLua\n\n\nPandoc\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nExecutable Rust Code in Quarto\n\n\n\n\n\n\nQuarto\n\n\nRust\n\n\nLua\n\n\nPandoc\n\n\n\n\n\n\n\n\n\nJul 26, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Compile CPython on Debian-Based Linux\n\n\n\n\n\n\nPython\n\n\nCPython\n\n\nC\n\n\ncompilation\n\n\nmake\n\n\nDebian\n\n\nLinux\n\n\n\n\n\n\n\n\n\nJul 25, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nMy Quarto Blog\n\n\n\n\n\n\nblog\n\n\nQuarto\n\n\nJekyll\n\n\nMermaid\n\n\nGraphviz\n\n\ndot\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nCombining Kedro with Rye\n\n\n\n\n\n\nKedro\n\n\nRye\n\n\nPython\n\n\nCPython\n\n\nProject Management\n\n\nPackage Management\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nA Python CLI Example to Log the Execution Trace\n\n\n\n\n\n\nComputer Programming\n\n\nPython\n\n\nTracing\n\n\nScripting\n\n\nCLI\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nAutomorphism Orbits of Graphlets\n\n\n\n\n\n\nMath\n\n\nGraph Theory\n\n\nGraphlets\n\n\nAutomorphisms\n\n\nAutomorphism Orbits\n\n\nAutomorphism Orbits of Graphlets\n\n\nIsomorphisms\n\n\nGaphs\n\n\nRelations\n\n\nBinary Relations\n\n\nSets\n\n\nVertices\n\n\nEdges\n\n\nSubsets\n\n\nCartesian Products\n\n\nNetworks\n\n\ndirected Graphs\n\n\nDigraphs\n\n\nSigned Graphs\n\n\nWeighted Graphs\n\n\nSimple Graphs\n\n\nFunctions\n\n\nInjective Functions\n\n\nSurjective Functions\n\n\nBijective Functions\n\n\nGraph Isomorphism\n\n\nGraph Automorphisms\n\n\nEquivalence Relations\n\n\nReflexive Relations\n\n\nSymmetric Relations\n\n\nTransitive Relations\n\n\nEquivalence Classes\n\n\nVertex Orbit Automorphisms\n\n\nPermutation Groups\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Plot a Basic Pie Chart Of Taxa At A Given Taxonomic Rank Using Pandas And Matplotlib\n\n\n\n\n\n\nPython\n\n\nVolunteer\n\n\niNaturalist\n\n\nData\n\n\nPlotting\n\n\nMatplotlib\n\n\nPandas\n\n\nCitizen Science\n\n\n\n\n\n\n\n\n\nMar 16, 2022\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nUnzip Your iNaturalist Observations On A Ubuntu System\n\n\n\n\n\n\nBASH\n\n\nVolunteer\n\n\niNaturalist\n\n\nData\n\n\nCitizen Science\n\n\nLinux\n\n\nUbuntu\n\n\n\n\n\n\n\n\n\nMar 16, 2022\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nQuantEcon 1 Numba\n\n\n\n\n\n\nPython\n\n\nScipy\n\n\nNumPy\n\n\ntimeit\n\n\noptimization\n\n\nNumba\n\n\n\n\n\n\n\n\n\nApr 9, 2019\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nQuantEcon 1 Scipy Submodules\n\n\n\n\n\n\nPython\n\n\nScipy\n\n\nNumPy\n\n\nscipy.optimize\n\n\nNewton Raphson method\n\n\nBrents method\n\n\ntimeit\n\n\nbisect method\n\n\noptimization\n\n\n\n\n\n\n\n\n\nMar 28, 2019\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nStarting Data Science\n\n\n\n\n\n\nData Science\n\n\n\n\n\n\n\n\n\nMar 23, 2019\n\n\nGalen Seilis\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Seilis, Galen. The Importance of Partial Pooling with Northern Health Data. Research and Knowledge Translation Newsletter\n\n\nSeilis, Galen. Project: Advanced Analytics. Research and Knowledge Translation Newsletter."
  },
  {
    "objectID": "posts/pie-chart-inaturalist/index.html",
    "href": "posts/pie-chart-inaturalist/index.html",
    "title": "How to Plot a Basic Pie Chart Of Taxa At A Given Taxonomic Rank Using Pandas And Matplotlib",
    "section": "",
    "text": "Note\n\n\n\nThis post was migrated from my iNaturalist journal to my Jekyll blog on 2023-02-26. It was then migrated to my Quarto blog on 2024-07-27.\n\n\nSuppose you have an exported CSV of iNaturalist observations, observations-&lt;ID&gt;.csv.\nTo follow this tutorial you will have to have Python installed, and the Matplotlib and Pandas packages installed. Pandas is not necessary, but it makes things convenient enough that I recommend using it here. In other contexts you may wish to plot pie charts without Pandas.\nAssuming you have PIP installed, you can install Pandas and Matplotlib as follows:\npip install matplotlib pandas\nAlthough, since Pandas actually uses Matplotlib as a dependency for plotting, it might suffice to simply use:\npip install pandas\nNext, you must create a script file with the *.py extension. We can do fancier things with paths, but let us create the file pie_taxa.py using BASH.\ntouch pie_taxa.py\nNow let us write some lines of code in pie_taxa.py. First we need to import the required packages.\nimport matplotlib.pyplot as plt\nimport pandas as pd\nNext we can load our data using the pd.read_csv function, which assumes a CSV format by default. It has many other parameters, including changing the delimiter (see the docs), but we are fine with the defaults here.\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(\"observations-&lt;ID&gt;.csv\")\nNow, as if by magic (but actually the hard work of software developers), we can create the plot in a single line of code. Let us do it for the kingdom level, which will require us knowing that this is represented by the taxon_kingdom_name column in our data file.\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(\"observations-&lt;ID&gt;.csv\")\ndf['taxon_kingdom_name'].value_counts().plot.pie()\nThere are a few things going on in the previous line of code. First is that df['taxon_kingdom_name'] has selected only the taxon_kingdom_name column. This is next passed to the value_counts() which counts the occurrences of each kingdom in that column and returns a Pandas series object with this information, and then we finally call the plot.pie method on this series object which… well… makes the pie chart.\nIf you run the code at this point you may be surprised to not actually see a plot appear anywhere. If you ran the code from the command line you might have seen something like &lt;AxesSubplot:ylabel='taxon_kingdom_name'&gt;. This is because creating the instructions of what goes on the drawing canvas is different from graphically rendering it. In order to do that, we can call plt.show().\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(\"observations-&lt;ID&gt;.csv\")\ndf['taxon_kingdom_name'].value_counts().plot.pie()\nplt.show()\nRunning the above, you should see a window pop up. It has various settings for resizing, reshaping, zooming, and saving your figure.\nWhat if you didn’t want to look at Kingoms, but rather orders, or families, etc? You would simply use a different column instead of taxon_kingdom_name. Here is a table of these similar columns:\n\n\n\nTaxonomic Rank\n\n\n\n\ntaxon_kingdom_name\n\n\ntaxon_phylum_name\n\n\ntaxon_subphylum_name\n\n\ntaxon_superclass_name\n\n\ntaxon_class_name\n\n\ntaxon_subclass_name\n\n\ntaxon_superorder_name\n\n\ntaxon_order_name\n\n\ntaxon_suborder_name\n\n\ntaxon_superfamily_name\n\n\ntaxon_family_name\n\n\ntaxon_subfamily_name\n\n\ntaxon_supertribe_name\n\n\ntaxon_tribe_name\n\n\ntaxon_subtribe_name\n\n\ntaxon_genus_name\n\n\ntaxon_genushybrid_name\n\n\ntaxon_species_name\n\n\ntaxon_hybrid_name\n\n\ntaxon_subspecies_name\n\n\ntaxon_variety_name\n\n\ntaxon_form_name\n\n\n\nHappy plotting."
  },
  {
    "objectID": "posts/quantecon-2-numba/index.html",
    "href": "posts/quantecon-2-numba/index.html",
    "title": "QuantEcon 1 Numba",
    "section": "",
    "text": "Introduction\nNumba is a Python library that provides an open source just-in-time compiler that allows a coder to mark selected parts of their code to be compiled for faster execution. As someone interested in computation at any scale, from calculating \\(13 \\times 19\\) (mental arithmetic is not my forte) to analyzing the behaviour of tens of thousands of genes or hundreds of thousands of IP addresses. I am not one to squeeze every inch of performance out of something small that was only meant to run once as a proof of concept, but it can be worth speeding up tasks that are either huge or will be repeated.\nLet’s get into how to use Numba – hang on! Why not just use compiled languages like C, C++ or FORTRAN? Well, herein lies one of meta-problems of development that requires some optimization. Coding in Python is useful for quickly coding up proofs of concept, but properties like its dynamic typing slow it down compared to memory-managed code in C. Coding in C will give a faster execution for the same code, but will often require more time and degugging to get ready for deployment. Using Python with Numba is an attempt to get the best of both worlds, and in practice is not much slower than software compiled from well-written low-level languages.\n\n\nJIT\nThe first way we can use Numba to speed up our code is by compiling a function so that future executions can use the compiled version, removing the need to compile at runtime. Let’s take a function that gives us the first n Fibonnacci numbers, and see how it performs.\n\nimport numpy as np\nfrom timeit import timeit\n\ndef fib(n):\n    '''\n    Adjusted from:\n    https://lectures.quantecon.org/py/numba.html\n    https://en.wikipedia.org/wiki/Fibonacci_number\n    https://www.geeksforgeeks.org/program-for-nth-fibonacci-number/\n    '''\n\n    if n == 1:\n        return np.ones(1)\n    elif n &gt; 1:\n        x = np.empty(n)\n        x[0] = 1\n        x[1] = 1\n        for i in range(2,n):\n            x[i] =  x[i-1] + x[i-2]\n        return x\n    else:\n        print('WARNING: Check validity of input.')\n\nprint(timeit('fib(10)', globals={'fib':fib}))\n\n2.0311123809988203\n\n\nRunning the above code on my laptop gives around 2 seconds to run the function \\(1000000\\) times according to timeit, which is reasonable for small \\(n\\) but let’s see if we can speed this up with the Numba’s jit.\n\nfrom numba import jit\nimport numpy as np\nfrom timeit import timeit\n\ndef fib(n):\n    '''\n    ARGUMENTS:\n    n: Max index to calculate Fibonacci numbers up to (int)\n    RETURNS\n    x: Array of Fibonnacci numbers (numpy.ndarray)\n\n    NOTES:\n    Adjusted from:\n    https://lectures.quantecon.org/py/numba.html\n    https://en.wikipedia.org/wiki/Fibonacci_number\n    https://www.geeksforgeeks.org/program-for-nth-fibonacci-number/\n    '''\n\n    if n == 1:\n        return np.ones(1)\n    elif n &gt; 1:\n        x = np.empty(n)\n        x[0] = 1\n        x[1] = 1\n        for i in range(2,n):\n            x[i] =  x[i-1] + x[i-2]\n        return x\n    else:\n        print('WARNING: Check validity of input.')\n\nfib = jit(fib)\n\nprint(timeit('fib(10)', globals={'fib':fib}))\n\n0.6980576210007712\n\n\nRunning the above code with jit brought the execution time down to about 0.7 seconds, which is faster than the original function.\n\n\nVectorizing\nAnother approach to speeding up code is vectorizing, where multiple operations are applied to each entry directly instead of producing multiple intermediate arrays as operations are applied. Originally I had wanted to use our fib function, but I quickly learned that it is not vectorizable because it cannot be made into a universal function. For a function to be universal, it is necessary that they map scalars into scalers, and map arrays into arrays. With that in mind, we’ll vectorize a suitable function to show how the time performance is improved. Let’s start by timing the unvectorized function.\nimport numpy as np\nimport quantecon as qe\n\ndef f(x, y):\n    return np.exp(np.abs(x -y**3)) *  np.cos(x**2 + y**2)\n\n# generate data\ngrid = np.linspace(-3, 3, 1000)\nx, y = np.meshgrid(grid, grid)\n\nstart = qe.util.tic()\nf(x, y)\nend = qe.util.toc()\nOn my machine the execution time was about \\(0.05\\) seconds, which isn’t half-bad by itself. Not let’s run the same code in vectorized form.\nfrom numba import vectorize\nimport numpy as np\nimport quantecon as qe\n\n@vectorize\ndef f(x, y):\n    return np.exp(np.abs(x -y**3)) *  np.cos(x**2 + y**2)\n\n# generate data\ngrid = np.linspace(-3, 3, 1000)\nx, y = np.meshgrid(grid, grid)\n\nf(x,y) # precompile\n\nstart = qe.util.tic()\nf(x, y)\nend = qe.util.toc()\nThis vectorized form took about \\(0.0042\\) seconds to execute, which is about \\(12\\) times faster! This is a clear demonstration that vectorizing functions is worthwhile as scalability becomes an issue.\nBecause the vectorization of this function means that each element of the array is calculated independently, we can further attempt to speed this calculation up by calculating elements in parallel! We do that by telling the decorator the element types (we’ll use float64), and that the target is function should be done in parallel.\nfrom numba import vectorize\nimport numpy as np\nimport quantecon as qe\n\n@vectorize('float64(float64, float64)', target='parallel')\ndef f(x, y):\n    return np.exp(np.abs(x -y**3)) *  np.cos(x**2 + y**2)\n\n# generate data\ngrid = np.linspace(-3, 3, 1000)\nx, y = np.meshgrid(grid, grid)\n\nf(x,y) # precompile\n\nstart = qe.util.tic()\nf(x, y)\nend = qe.util.toc()\nThis last acceleration to make the calculations parallel squeezed the execution time down to \\(0.0031\\) seconds. This is only \\(0.0011\\) seconds faster than without the parallel execution, but still a worthwhile addition to the toolkit for doing independent calculations.\n\n\nConclusion\nUsing Numba allows us an easy way to increase the performance of functions in Python without going to a lower-abstraction language such as C or FORTRAN. Some functions will be more suitable to @jit than @vectorize based on the type of operations and whether the function is universal (or can be made into a universal function). These accelerations in performance becomes increasingly valuable as the amount of data being processed becomes large!"
  },
  {
    "objectID": "posts/python-clie-log-trace/index.html",
    "href": "posts/python-clie-log-trace/index.html",
    "title": "A Python CLI Example to Log the Execution Trace",
    "section": "",
    "text": "This is just a short script implementing a logger of the trace of a Python program’s execution.\nimport datetime\nfrom typing import Optional, Callable, Any, Tuple\nimport sys\n\nimport click\n\ndef trace_function(frame: Any, event: str, arg: Any) -&gt; Optional[Callable]:\n    \"\"\"\n    Trace function for monitoring function calls.\n\n    Args:\n        frame (frame): The current frame being executed.\n        event (str): The event type triggering the trace function.\n        arg (Any): The argument associated with the event.\n\n    Returns:\n        Optional[Callable]: The trace function or None to stop tracing.\n    \"\"\"\n    if not hasattr(trace_function, 'log_initialized'):\n        # Initialize log file with column titles if not already done\n        with open('trace_log.txt', 'w') as log_file:\n            log_file.write(\"Timestamp | Event | Function | File | Line | Argument\\n\")\n        trace_function.log_initialized = True\n\n    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    file_name = frame.f_globals.get('__file__', 'unknown')\n    log_entry = f\"{current_time} | {event} | {frame.f_code.co_name} | {file_name} | {frame.f_lineno} | {arg}\\n\"\n    with open('trace_log.txt', 'a') as log_file:\n        log_file.write(log_entry)\n    return trace_function\n\n@click.command()\n@click.argument('target_script', type=click.Path(exists=True))\ndef trace(target_script: str) -&gt; None:\n    \"\"\"\n    Trace function to monitor the execution of a target script.\n\n    Args:\n        target_script (str): Path to the target script to be traced.\n\n    Returns:\n        None\n    \"\"\"\n    # Set the trace function\n    sys.settrace(trace_function)\n\n    # Run the target script\n    with open(target_script, 'r') as script_file:\n        exec(script_file.read(), {})\n\n    # Disable the trace function\n    sys.settrace(None)\n\nif __name__ == '__main__':\n    trace()\nUsing it is straightforward:\n$ Python trace_util.py you_python_script.py"
  },
  {
    "objectID": "posts/rust-execution-for-quarto/index.html",
    "href": "posts/rust-execution-for-quarto/index.html",
    "title": "Executable Rust Code in Quarto",
    "section": "",
    "text": "The following is a Lua filter which looks through a qmd file for Rust code associated with {rust}, compiles that code using rustc, runs the compiled Rust program and collects its output, and inserts the output to be rendered by pandoc.\nlocal io = require(\"io\")\nlocal os = require(\"os\")\nlocal tempfile = require(\"os\").tmpname\nlocal log_file\n\n-- Function to initialize the log file\nlocal function init_log()\n  log_file = io.open(\"rust_executor_debug.log\", \"w\")\nend\n\n-- Function to log messages to file and stderr\nlocal function log(...)\n  local args = {...}\n  for i = 1, #args do\n    args[i] = tostring(args[i])\n  end\n  local message = table.concat(args, \" \")\n  if log_file then\n    log_file:write(message .. \"\\n\")\n    log_file:flush()\n  end\n  io.stderr:write(message .. \"\\n\")\n  io.stderr:flush()\nend\n\n-- Helper function to execute Rust code and return the output\nlocal function execute_rust_code(code)\n  local temp_file = tempfile() .. \".rs\"\n  log(\"Temporary Rust file:\", temp_file)\n  local source_file, err = io.open(temp_file, \"w\")\n  if not source_file then\n    log(\"Failed to create source file:\", err)\n    error(\"Failed to create source file: \" .. err)\n  end\n\n  source_file:write(code)\n  source_file:close()\n\n  local temp_bin = tempfile()\n  log(\"Temporary binary file:\", temp_bin)\n\n  local compile_command = \"rustc \" .. temp_file .. \" -o \" .. temp_bin .. \" 2&gt;&1\"\n  log(\"Compile Command:\", compile_command)\n  local compile_pipe = io.popen(compile_command)\n  local compile_output = compile_pipe:read(\"*a\")\n  local compile_result = compile_pipe:close()\n\n  if compile_result ~= true then\n    os.remove(temp_file)\n    log(\"Rust compilation failed. Output:\", compile_output)\n    error(\"Rust compilation failed. Output: \" .. compile_output)\n  end\n\n  local exec_command = temp_bin .. \" 2&gt;&1\"\n  log(\"Exec Command:\", exec_command)\n  local exec_pipe = io.popen(exec_command)\n  local output = exec_pipe:read(\"*a\")\n  exec_pipe:close()\n\n  local ok, rm_err = pcall(function()\n    os.remove(temp_file)\n    os.remove(temp_bin)\n  end)\n  if not ok then\n    log(\"Failed to clean up temporary files:\", rm_err)\n    error(\"Failed to clean up temporary files: \" .. rm_err)\n  end\n\n  log(\"Output:\", output)\n  return output\nend\n\nlocal echo_global = true\n\nfunction Meta(meta)\n  if meta.echo ~= nil then\n    echo_global = pandoc.utils.stringify(meta.echo) == \"true\"\n  end\nend\n\n-- Lua filter function\nfunction CodeBlock(elem)\n  if not log_file then\n    init_log()\n  end\n\n  local is_rust_code = elem.attr.classes:includes(\"{rust}\")\n  if is_rust_code then\n    log(\"Processing Rust code block\")\n    local output = execute_rust_code(elem.text)\n    output = output:gsub(\"%s+$\", \"\")\n    local blocks = {}\n\n    if echo_global then\n      -- Render Rust code as a formatted block\n      table.insert(blocks, pandoc.CodeBlock(elem.text, {class=\"rust\"}))\n    end\n\n    -- Always return the output\n    table.insert(blocks, pandoc.Para(pandoc.Str(output)))\n\n    return blocks\n  else\n    log(\"Skipping non-Rust code block\")\n  end\nend\n\n-- Ensure log file is closed properly at the end\nfunction Pandoc(doc)\n  if log_file then\n    log_file:close()\n  end\n  return doc\nend\nLet’s try some examples.\nHere is some Rust code that will be executed and rendered.\nfn main() {\n        println!(\"Galen Seilis is learning Rust!\");\n        println!(\"Time to get Rusty!\");\n}\nGalen Seilis is learning Rust!\nTime to get Rusty!\nNow let us try some Rust code that will not be executed.\nfn main() {\n    println!(\"Meow\");\n}\nNow let us run a longer example from Rust by Example.\nfn main() {\n    // Integer addition\n    println!(\"1 + 2 = {}\", 1u32 + 2);\n\n    // Integer subtraction\n    println!(\"1 - 2 = {}\", 1i32 - 2);\n    // TODO ^ Try changing `1i32` to `1u32` to see why the type is important\n\n    // Scientific notation\n    println!(\"1e4 is {}, -2.5e-3 is {}\", 1e4, -2.5e-3);\n\n    // Short-circuiting boolean logic\n    println!(\"true AND false is {}\", true && false);\n    println!(\"true OR false is {}\", true || false);\n    println!(\"NOT true is {}\", !true);\n\n    // Bitwise operations\n    println!(\"0011 AND 0101 is {:04b}\", 0b0011u32 & 0b0101);\n    println!(\"0011 OR 0101 is {:04b}\", 0b0011u32 | 0b0101);\n    println!(\"0011 XOR 0101 is {:04b}\\n\\n\\n\", 0b0011u32 ^ 0b0101);\n    println!(\"1 &lt;&lt; 5 is {}\", 1u32 &lt;&lt; 5);\n    println!(\"0x80 &gt;&gt; 2 is 0x{:x}\", 0x80u32 &gt;&gt; 2);\n\n    // Use underscores to improve readability!\n    println!(\"One million is written as {}\", 1_000_000u32);\n}\n1 + 2 = 3\n1 - 2 = -1\n1e4 is 10000, -2.5e-3 is -0.0025\ntrue AND false is false\ntrue OR false is true\nNOT true is false\n0011 AND 0101 is 0001\n0011 OR 0101 is 0111\n0011 XOR 0101 is 0110\n\n\n\n1 &lt;&lt; 5 is 32\n0x80 &gt;&gt; 2 is 0x20\nOne million is written as 1000000\nIn the current state there are a couple of glaring issues I have with this implementation. The first is that Rust code blocks will be run regardless of whether echo: false is used. The second is that all the outputs are being rendered on a single, notwithstanding Quarto’s line wrapping.\nThere is also an enhancement which is desirable, which is to render other types of things from Rust that are not just plaintext. Instead of developing this kind of functionality myself, it would make sense to take a closer look at integrating tools such as the Evcxr Jupyter kernel."
  },
  {
    "objectID": "posts/my-quarto-blog/index.html",
    "href": "posts/my-quarto-blog/index.html",
    "title": "My Quarto Blog",
    "section": "",
    "text": "I am switching to Quarto for my blog. My Jekyll blog is available here, and I might move some of the posts over to the new blog over time.\nCreating this blog was really easy. Was my Jekyll blog really complicated to setup? There were some technical hurdles around getting extra behaviour on my original blog, but overall it wasn’t extremely hard. Why am I switching to Quarto?\nWhat Quarto provides (that I want) is code execution followed by rendering the output of the code. I can put my code examples right into the blog post, and if something like a plot is produced then that plot will show on my blog.\nIn constrast, with Jekyll, I needed to\n\nmake the plot\nmove the plot to an images folder\nreference to the plot’s path in the blog post.\n\nIt wasn’t terrible, and it is possible that I just didn’t figure out how to make this easier with Jekyll, but it was quickly apparent to me that Quarto makes this easy.\nThis includes mermaid diagrams:\n\n\n\n\n\n---\ntitle: Example Git diagram\n---\ngitGraph\n   commit\n   commit\n   branch develop\n   checkout develop\n   commit\n   commit\n   checkout main\n   merge develop\n   commit\n   commit\n\n\n\n\n\n\nI can also easily prepare Graphiz diagrams provided that I supply some valid dot notation:\n\ndigraph finite_state_machine {\n    fontname=\"Helvetica,Arial,sans-serif\"\n    node [fontname=\"Helvetica,Arial,sans-serif\"]\n    edge [fontname=\"Helvetica,Arial,sans-serif\"]\n    rankdir=LR;\n    node [shape = doublecircle]; 0 3 4 8;\n    node [shape = circle];\n    0 -&gt; 2 [label = \"SS(B)\"];\n    0 -&gt; 1 [label = \"SS(S)\"];\n    1 -&gt; 3 [label = \"S($end)\"];\n    2 -&gt; 6 [label = \"SS(b)\"];\n    2 -&gt; 5 [label = \"SS(a)\"];\n    2 -&gt; 4 [label = \"S(A)\"];\n    5 -&gt; 7 [label = \"S(b)\"];\n    5 -&gt; 5 [label = \"S(a)\"];\n    6 -&gt; 6 [label = \"S(b)\"];\n    6 -&gt; 5 [label = \"S(a)\"];\n    7 -&gt; 8 [label = \"S(b)\"];\n    7 -&gt; 5 [label = \"S(a)\"];\n    8 -&gt; 6 [label = \"S(b)\"];\n    8 -&gt; 5 [label = \"S(a)\"];\n}\n\n\n\n\n\n\nfinite_state_machine\n\n\n\n0\n\n\n0\n\n\n\n2\n\n2\n\n\n\n0-&gt;2\n\n\nSS(B)\n\n\n\n1\n\n1\n\n\n\n0-&gt;1\n\n\nSS(S)\n\n\n\n3\n\n\n3\n\n\n\n4\n\n\n4\n\n\n\n8\n\n\n8\n\n\n\n6\n\n6\n\n\n\n8-&gt;6\n\n\nS(b)\n\n\n\n5\n\n5\n\n\n\n8-&gt;5\n\n\nS(a)\n\n\n\n2-&gt;4\n\n\nS(A)\n\n\n\n2-&gt;6\n\n\nSS(b)\n\n\n\n2-&gt;5\n\n\nSS(a)\n\n\n\n1-&gt;3\n\n\nS($end)\n\n\n\n6-&gt;6\n\n\nS(b)\n\n\n\n6-&gt;5\n\n\nS(a)\n\n\n\n5-&gt;5\n\n\nS(a)\n\n\n\n7\n\n7\n\n\n\n5-&gt;7\n\n\nS(b)\n\n\n\n7-&gt;8\n\n\nS(b)\n\n\n\n7-&gt;5\n\n\nS(a)\n\n\n\n\n\n\n\n\nThis is definitely desired behaviour."
  },
  {
    "objectID": "posts/quantecon-1-scipy-submodules/index.html",
    "href": "posts/quantecon-1-scipy-submodules/index.html",
    "title": "QuantEcon 1 Scipy Submodules",
    "section": "",
    "text": "As someone with previous background in Python, I’ve been blasting my way through the basics of the Quantecon curricula. One of the joys of self-directed learning is that, with discipline, you can speed through familar material and really camp out with the new material. With that in mind, I’ve decided to further play with finding solutions (x-intercepts) of some single variable functions.\nFirst of all, let’s find ourselves an interesting function. I’ve chosen \\(f(x) = \\sin(x) \\exp(-x)\\) because I’ve always enjoyed its degradating oscillations, but also because I expect this equation to have solutions. Since any integer multiple \\(k\\) of \\(\\pi\\) will result in \\(\\sin(x) = 0\\) when \\(x = k \\pi\\), we know that \\(f(k \\pi) = 0\\) as well. While I’m quite late (or too early, depending on how you see it) for calculating \\(\\pi\\) on \\(\\pi\\) Day, let’s take \\(k = 1\\) to find \\(\\pi\\) anyway!\n\nBisection Method\nThe first method mentioned on QuantEcon is the bisection algorithm, which essentially treats finding solutions to a function as a binary search problem. There are two parameters that are needed to get started with the bisection algorithm, an initial lower bound and an initial upper bound on the search space. Not only do we need two such parameters, but our choice of these two numbers can change what solution is found. Let’s consider the following example where we look on the interval \\([-10, 10]\\).\n\nimport numpy as np\nfrom scipy.optimize import bisect\n# Define a single-variable function to find solutions in\nf = lambda x: np.sin(x) * np.exp(-x)\n# try out the bisection algorithm\nprint(bisect(f, -10, 10))\n\n0.0\n\n\nWe were looking for \\(x = \\pi\\), but we got \\(x = 0\\) instead. If there are multiple solutions within your search interval, the algorithm won’t necessarily converge on the one that you wanted, nor will it report to you there were multiple solutions. Knowing ahead of time that we’d like to calculate \\(\\pi\\), and that \\(3 &lt; \\pi &lt; 4\\), let’s rerun the bisection algorithm on \\([3, 4]\\).\n\nprint(bisect(f, 3, 4))\n\n3.1415926535901235\n\n\nThat gives us a value pretty close to \\(\\pi\\), correct to the \\(11\\)th digit anyway.\n\n\nNewton-Raphson method\nThe Newton-Raphson method is a calculus-based method that iteratively steps towards a solution. Like the bisection method, it requires a number decided ahead of time but this time this chosen number is an initial guess or starting point. Unlike the bisection method, the Newton-Raphson method does not have bounds set on the search so a continuous function over the real numbers can be searched indefinitely. To prevent the algorithm searching for too long, a hyperparameter limiting the number of iterations (steps) is included if a stable solution is not converged upon (default is \\(50\\) steps).\n\nfrom scipy.optimize import newton\n# Define a single-variable function to find solutions in\n# try out the Newton-Raphson algorithm\nprint(newton(f, 0.2))\n\n3.6499361606787994e-14\n\n\nWhile \\(x = 0.2\\) is not that far off from Pi, the local derivatives are going to point the steps to descend toward zero. Notice that the solution we got was not exactly zero, but rather the first solution found within a predefined tolerance of \\(1.48 \\times 10^{-8}\\). What you don’t see from the code is the that shape of the curve, which if you plot our function you’ll see there is a local maxima between \\(x = 0\\) and \\(x = \\pi\\) at \\(x = \\frac{\\pi}{4}\\). Relative to this hill, our estimate is analogous to a ball rolling in the direction of steepest descent. This analogy breaks down for solutions separated by a local minima as the method is not equivalent to steepest descent even though it is based on the local derivative. Another issue that can come about is picking an initial value close to an extrema because the results can be unstable, allowing incredibly large jumps across the domain. Therefore, we should be cautious about our choice of initial guess by doing some exploration of function’s properties before attempting to estimate the solution. Let’s retry with a more suitable initial value.\n\n# try out the Newton-Raphson algorithm\nprint(newton(f, np.pi / 4 + 1))\n\n3.1415926535897936\n\n\nThat is clearly closer to \\(\\pi\\) than \\(3.6499361606787994 \\times 10^{-14}\\), and being accurate for the first \\(16\\) digits suggests that it was more precise than the bisection algorithm under these parameters.\n\n\nBrent’s method\nThe QuantEcon course points out that Bisection is more robust (stable) than Newton-Raphson’s method, but it is also slower. An alternative approach that balances this tradeoff is Brent’s method which includes bounds and garantees solutions for computable functions. Let’s give this approach a try on our function on \\([3, 4]\\).\n\nfrom scipy.optimize import brentq\n# try out the Brent's algorithm\nprint(brentq(f, 3, 4))\n\n3.141592653589788\n\n\nLooks like this estimation of \\(\\pi\\) was correct for the first \\(13\\) digits, which was better than Bisection but worse than Newton-Raphson.\n\n\nPerformance comparison with timeit\nLast of all, it would be interesting to compare the time performance of each of these solution-finding approaches. Let’s do that with timeit.\n\nfrom timeit import timeit\nprint(timeit(stmt='bisect(f, 3, 4)',\\\n    globals={'bisect':bisect, 'f':f},\\\n    number=100000) / 100000)\nprint(timeit(stmt='newton(f, np.pi / 4 + 1)',\\\n    globals={'newton':newton, 'f':f, 'np':np},\\\n    number=100000) / 100000)\nprint(timeit(stmt='brentq(f, 3, 4)',\\\n    globals={'brentq':brentq, 'f':f},\\\n    number=100000) / 100000)\n\n0.00010017173054002341\n0.0001319278220400156\n2.3580086980000487e-05\n\n\nWe find under this setup that the slowest algorithm was the Newton-Raphson’s method, followed by the bisection method by a factor of \\(\\frac{1}{5}\\), and final Brent’s method being about an order of magnitude faster! So Brent’s method gave us more accurate digits in the solution, at least for \\(x = \\pi\\), and also performed faster than the other two methods. Does this mean that Brent’s method is always the best method? Not necessarily. We should be open to the possibility of tradeoffs not discussed on QuantEcon, as well as there being a panoply of algorithms out available in code repositories."
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Integrating Causal Inference Methods into Northern Health’s Analytics Work\n\n\nObjective: This talk aims to highlight the integration of modern causal inference methods into the decision-making processes at Northern Health (NH). I present a comprehensive approach to incorporate causal inference principles into NH’s analytics work, emphasizing the significance of causality in healthcare research and its alignment with NH’s mission, vision, and strategic plans.\nMethods: The presentation outlines our approach, which involves the development of a systematic causal model development process. These methods are designed to equip NH’s data scientists and researchers with the skills necessary to apply modern causal inference techniques effectively.\nResults: I discuss the theoretical foundations and the framework for integrating modern causal inference methodologies into NH’s projects. I highlight the general challenges and benefits of applying causal inference methods in healthcare research, including addressing incomplete or corrupted data, accounting for unmeasured confounds, and estimating heterogeneous treatment effects at the individual level. Finally, I will report on the current state of a project to estimate the causal impact of COVID-19 on a time series data set in NH.\nLessons Learned: Causality has deep roots in philosophy, and modern causal inference offers a rigorous framework to address the complexities of healthcare data analysis. I emphasize the critical distinction between causal inference and traditional statistics and demonstrate how misleading statistics can misguide decision-making. I stress that causal inference methodologies are in harmony with NH’s mission, vision, and strategic objectives, enhancing the quality of inferences drawn from healthcare data.\n\n\nA Friendly Introduction to Statistical Forecasting\nA Moosy Proposal: Estimating Average Direction of Moose Travel from Weak Information\nAdventures in Non-Negative Canonical Polyadic Decomposition\nHow Correlation Really Works\nIntroduction to Interval Arithmetic\nIntroduction to Using ARIMA\nExample of Training a SARIMAX Model\nReview of a Study Using SARIMAX Guest lecture: Relations and Graphs\nA Gentle Introduction to Geometric Deep Learning{:target=“_blank”}\n\n\n\n\nPractical Approaches to Faster and Leaner Python{:target=“_blank”}\n\n\n\n\nA Gentle Introduction to L-Systems{:target=“_blank”}\nEnumeration of Automorphism Orbits of Graphlets (HackSeq){:target=“_blank”}"
  }
]