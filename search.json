[
  {
    "objectID": "ideas.html",
    "href": "ideas.html",
    "title": "Blog Ideas",
    "section": "",
    "text": "Some ideas for blog posts:\n\nHow to choose software tools with respect to development effort and the Turing tar pit.\n\nAsychronous programming is not required for DES, but is it better?\n\nAll data is spatiotemporal data, and we often neglect that fact. It has never been so easy to collect the time and location of measurements. Some measurements are paired to sets of time and space rather than individual points.\n\nBCData –&gt; Get some geospatial data –&gt; Load data using Geopandas –&gt; Plot using Folium.\n\nIbis integration with Kedro example.\n\nUML used to be considered a tool for designing programs. That purpose has been mostly abandoned in modern software development. The primary use case for UML IMO is to go the other way; generate UML from code. ‘Sometimes’ a UML diagram can provide a useful bird’s eye pictureo of the structure of a program, which can help a programmer unfamiliar with some code get up and running.\n\nGo over the common software design patterns, showing code examples of each one.\n\nhttps://en.wikipedia.org/wiki/Software_design_pattern\nhttps://en.wikipedia.org/wiki/Design_Patterns\n\n\nAdversarial learning of simple probability distribution parameters. Could make for a good explainer of aversarial learning. Not sure if it is really better than other parametrization approaches though, but it is at least generative by construction.\n\nDiscuss the time-to-event vs time sweeping in simulation. The former tends to produce faster programs when the event density is relatively low. The latter tends to be faster when high event density (in time) is high and local state updates can be parallelized.\n\nAsyncio example of file IO.\n\nChaos isn’t just about high sensitivty to initial conditions. It is also about topological transitivity and density of orbits. IMO, the topological transitivity might be the most important property. Topological mixing is a special case of topological transitivity which may be easier to explain.\n\nQuadrant rule in BC silviculture assumes a type of spatial homogeneity within a plot. Even if a plot is ostensibly a “cedar carpet”, the density may not be truly uniform. A broader set of principles based on partitioning the plot and using symmetries generalizes the quadrant rule.\n\nThere is/was a fixed percentage agreement required between a silviculture surveyor and an auditor. I recall a professional forester assuring me that this ensures a reasonable expectation of agreement bewteen surveyor and auditor because the percentages “scale up”. They could be right, but I’m skeptical about this.\nAs the number of trees increases we should expect the variance of the counts to increase until even highly-skilled professionals will tend not to agree with each other within that fixed percentage agreement. Under an assumption of statistical independence we can easily show that this is true. Whether such a number of trees is counted in practice is not known to me, and mathematically it depends on the parameters.\n\nWhat distribution do the counts within a silviculture plot follow?\nI expect that the number of natural regen trees within an untreated area (i.e. not planted) follows an inhomogenous Poisson point process for the number of trees. Recently-planted trees should follow a probability distribution that is close to uniform (but not exactly uniform because it is more like a grid).\nRegardless of the number of trees, the number of trees that a suveyor counts will count has both false positives and false negatives. A first approximation could be a binomial distribution with some probability of sucessfully counting each tree. But not all trees are equally easy to spot, motivating us towards something like a Poisson binomial distribution. This approach may be more realistic, but if we can get away with a binomial likelihood with a Beta distributed prior on the probability of detection that might be more feasible. Maybe we could even show that some operation like marginalization gives us the binomial distribution from the Poisson binomial.\nOnce we have chosen a suitable mathematical model, we could show\n\nMarginal distributions are underrated, but despite their simplicity they get a bad rep. People often feel the need to stratify their variables, thereby conditioning the distribution they’ve interested in. This creates higher variance estimates and does not take any advantage of pooling. Partial pooling is a great way to go, but I think there are sometimes simpler analyses based on the marginal distributions that answer the same questions. Complete pooling is sometimes an appropriate modelling choice.\n\nExplore a sequence of location-composed exponential distributions. Although each step is memoryless, the overall stochastic process is not.\n\nExplore Kedro integration with Quarto.\n\nAsynchrony vs concurrency in programming.\n\nImplement metropolis hasting algorithm from scratch in Rust/Python.\n\nThe fastest discrete event simulation are not those which are developed within a framework that provides a huge number of features, or provide abstractions to generalize across huge spans of use cases. Rather, it will be the implementations that take the greatest advantage of speciality rather than generality. To know what you can assume and lean into that will produce a faster simulation product. This is one argument in favour of bespoke simulations over framework tools or enterprise development software.\n\nGive an example of a FIFO M/M/1 queue implementation using a framework tool (e.g. Ciw) vs a minimumal implementation in Python.\nAlso show an implementation in Rust to further emphasize the point that being able to assume data types is an example of the overall notion that specialty can provide better performance.\n\nThere are, of course, downsides to specialized softare. They can sometimes be less configurable out of the box. If what you need for a given project actually is a large multifeatured tool, then writing it yourself might only provide minor benefits.\nI think that when you really have no idea what you need, which often happens when you’re building a product the first time, going for a framework tool that is highly generalized and flexible makes sense for obtaining and MVP. But you’re product will plausibly be slower, and possibly more complicated under the hood than needed.\nIf the required feature set stabilizes, then it may be time to look at writing something that is specialized to solve the problem well rather than solve it generically.\n\nOverview of important Git features.\n\nCausal structure of discrete event simulation from a Pearlian causal inference point of view.\n\nCreate an example hybrid DES system when parts of the smooth state inform the discrete system and vice versa.\n\nKedro catalog can contain “metadata” which can be used to put any information you want into it. It is intended by the Kedro devs for plugins, but you can really put any info you want there. This includes things like annotations of variables, which can be programmatically loaded later to provide glossaries of terms and similar things.\n\nProvide an example statistical model of service completion rate where the number of servers is an integer and the completion rates are statistically dependent. The purpose of this exercise is to provide a statistical model that allows servers to be cooperative or competitive in service completion rates, which “competition” could just mean negatively related rates, without modelling individual servers as data structures in a simulation.\nA first approximation could look like this. Suppose a total service comletion rate \\(\\mu\\), which can be decomposed into the rates for each server:\n\\[\\mu := \\sum_i \\mu_i\\]\nwhere \\(\\mu_i\\) is an individual server’s service completion rate. However, this misses having some form of statistical dependence. To get that we can assume a multivariate log-normal distribution over the individual server’s rates.\n\nNumba doc examples vs Rust.\n\nCuPy examples vs Rust.\n\nBayesian graphlets analysis.\n\nWriting software to be highly general is an approach to managing uncertainty about the future use cases. It lowers the risk of later finding that you need to extend the software, and optimistically hopes that it lowersthe risk of later finding that you need to extend the software when that is difficult.\nIn a sense the number of features that are implemented prospectively without a planned usage is a measure of how little the developers actually know about the software they need to build. Sometimes\nOne of the alternatives to writing highly generalized code is to write code to be extensible rather than general.\nThe primary downside of aiming for generality is the up-front development effort. You can go to great lengths to create a majestic cathedral when all you needed was a tent.\nA secondary downside is the performance of the software itself. While I would not take it as an absolute rule, the tendency I have observed is that highly generalized software (e.g. bloated with features) is slower than specialized software.\n\nGive an example of using Ziggy PyDust.\n\nhttps://github.com/spiraldb/ziggy-pydust\n\n\nImplement reservoir sampling based on algorithms on Wiki: https://en.wikipedia.org/wiki/Reservoir_sampling\n\nCompare SciPy and PyMC.\n\n\n\n\n\n\n\nSciPy\nPyMC\n\n\n\n\nmaximum likelihood estimation\nfull posterior and posterior predictive sampling\n\n\nmostly designed for simple distribution\ndesigned for custom distributions\n\n\n\nWhile SciPy does support some multivariate sampling tools, such as the mutlvariate normal distribution, it isn’t out-of-the-box well-suited to the typical multivariate distributions we can expect from real data. Often the strategy I have seen taken with SciPy is to stratify the data in such a way that each grouping in the data follows a nice family of distribution. That approach introduces a tradeoff of not pooling shared information about the statistical populations.\nPyMC’s largest downside on a given problem for which SciPy could also be used is that it is a more complicated process. SciPy provides relatively quick fitting and sampling.\nSciPy is not well-suited to custom probability distributions. Its underlying fitting methods using inverse transform sampling which only works when the CDF is invertible. Furthermore, it will attempt to use the CDF in an optimization problem to find inverses numerically. That’s smart from a math point of view, but it is computationally inefficient in practice.\nPyMC is a relatively easy win for mixed models. If you have various grouping in the data and you think each population is from the same family of distribution, differing only in the parametres, then PyMC’s coordinates makes that easy. More complicated structures are also possible.\nI tried to develop some more complicated models using SciPy’s distributions combined with its optimization library. I found that mixtures of gamma distributions are particularly tricky, and it wasn’t uncommon for the modelling approach to be revised multiple times.\nPyMC models can sometimes sample poorly, and there is likewise a whole bag of tricks to refine the model or sampling or data to improve its performance. However, I noticed that it does mixtures quite well whereas I have had “mixed results” (pun intended) with SciPy.\nSciPy being based on a maximum likelihood estimation (MLE) approach means that it assumes that the data was sampled from the likelihood distribution. In this approach the family of distributions is assumed and we take the view that it is just a matter of finding the specific parameters.\nPyMC being primarly based on full posterior inference means something very different. The data does not need to be seen as a sample from the likelihood at all! The posterior is defined via Baye’s theorem to be proportional to the product of the likelihood of the data and the prior. That is, the likelihood refines the prior using the data.\n\nImplement the various statistical and DES models you have seen in healthcare.\n\nPrepare some posts on uncertainty quantification.\n\nModel a stochastic matrix by having each row be a multinomial logistic regression.\n\nModel a stochastic matrix by having each row follow a Dirichlet distribution.\n\nExplain what a compact topological space is, and why it matters as a modelling assumption.\n\nPython exceptions should be preferentially used for exceptional behavior.\n\nProvide an example of using Nelsie.\n\nExplain how I think about outliers and anomalies.\nEssentially two types: - Rare events - Very different events - “One of these things is not like the other” game. - This might eventually be the same as the rare events when taken under the certain light.\n\nImplement various DES example using the Hades Python package.\nhttps://ki-oss.github.io/hades/\n\nImplement various DES examples using Python coroutines.\n\nShow a modelling example where it would be difficult to model in terms of sampling from the marginal distributions. Discuss the potential consequences for discrete event simulation.\n\nDiscuss the usage of popularity as a measure of software quality.\nAFAIK the only context where the popularity might be invoked is to make an argument that a given tool should be used because it is popular, or at least more popular than some alternative.\nWhen you’re in some kind of mode of work where you have to make rapid decisions it makes sense to use heuristics. I don’t think that context usually applies for selecting software. Further, I don’t think that using popularity as a heuristic is all that reliable and it would make the world of software pretty awful if we all just looked to the most popular tool as the tool to use. That’s because the popularity itself would just be due to preferential attachement effects and not due to the quality of the software at all.\nI will say that popular software has often gotten “something” right. Sometimes it is obvious what that is (e.g. Ruff’s static analysis is fast). Sometimes it isn’t obvious. Sometimes it isn’t the software itself but rather that it arrived on the scene early enough to have a big adoption advantage.\n\nHomomorphism between causal graphs and computation graphs on random variables.\n\nGo over the mathematical formalism of DES. I think it might be Zeigler that is a good reference for this. - Discrete event system specification - Timed-event system - Transition system\n\nProgrammer have to practice building thing in order to learn how to build great things. But organizations often do not want you to build and rebuild the product multiple times to get it right. Iteratively rebuilding can be perceived as a source of unecessary cost. In there lays a trade off. If you don’t allow your programmers to iteratively rebuild systems then they will not get the experience to build that particular system in a great way. But if you allo your programmers to iteratively rebuild systems then it will take more time and cost more. Prototyping can offer some compromise, but I expect that overall there is a Pareto front.\nProgrammers can learn skills and concepts that transfer to future projects, which alleviates some of this problem.\nThe business should also understand that if they want to produce the various best thing of some type that it is going to require substantial cost, one way or another.\n\nContrast event-based DES from process-based DES. This distinction is commonly made, but rarely defined. Providing an explanation of what these terms mean could be beneficial.\n\nExplore whether detailed balance in MCMC can be generalized.\n\nWrite a Rust program which creates and occupies a SQL DB, and then queries from it.\n\nWrite a Rust program which loads a CSV file.\n\nWrite a Rust program which loads a yaml file.\n\nRun Mathematica in Quarto. Maybe use the Python Wolfram client package.\n\nExplain what performance ratcheting is.\nIn common parlance, performance ratcheting is when the expectations for an employee’s performance increase endlessly. This requires some mathematical terminology to make precise.\nSuppose a cumulative increasing function is applied to the history of performance of the employee, which is used to define what is expected (but not necessarily the mathematical expection) of the employee’s performance.\nExamples: - maximum - mean\nIt might be worth exploring what is better than this.\n\nWrite the mathematics and PyMC code for an autoregressive model in which the time lags used follow a discrete probability distribution. That is, which time lags are used is random.\n\nSimulate random delayed differential equations with PyMC.\nIt might require integration with other packages. - ddeint - pydelay - jitcdde - scipy (there are some examples on stack overflow)\n\nShow an example of using ordinal regression to model count data.\nThe advantage of this approach is that it can take on a wide variety of shapes that classic count distributions cannot.\nAFAIK, this would have the disadvantage of being bounded from above. This would mean choosing a finite number of bins. The largest bin would actually need to represent “X or greater”, which means the upper bin would not really act like a count. I wonder if Frank Harrel or some other statistician has found something nicer than this.\n\nDoes using asynchronous programming make the locality of behaviour better or worse?\n\nUsing the stack tracing tool in Python, implement a tool which collects the file and line number as a program executes. This can be visualized as a heatmap where the row of the heatmap is the computation step and the column indexes the files that are used. The entries are the row number of the file for the instruction being executed. From there we can define some metrics for how much we’re jumping around.\n\nGive an introduction to interval arithmetic.\n\nGive a worked example of using Samuelson’s inequality.\nhttps://en.wikipedia.org/wiki/Samuelson%27s_inequality\n\nGive an example of Bayesian non-negative polyadic decomposition using PyMC.\nProvide similar, but probabilistic, diagrams similar to those shown by Kolda: https://www.youtube.com/watch?v=L8uT6hgMt00\n\nShow an example of Bayesian discrete event simulation where the DES is occurring within the…"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Integrating Causal Inference Methods into Northern Health’s Analytics Work\n\n\nObjective: This talk aims to highlight the integration of modern causal inference methods into the decision-making processes at Northern Health (NH). I present a comprehensive approach to incorporate causal inference principles into NH’s analytics work, emphasizing the significance of causality in healthcare research and its alignment with NH’s mission, vision, and strategic plans.\nMethods: The presentation outlines our approach, which involves the development of a systematic causal model development process. These methods are designed to equip NH’s data scientists and researchers with the skills necessary to apply modern causal inference techniques effectively.\nResults: I discuss the theoretical foundations and the framework for integrating modern causal inference methodologies into NH’s projects. I highlight the general challenges and benefits of applying causal inference methods in healthcare research, including addressing incomplete or corrupted data, accounting for unmeasured confounds, and estimating heterogeneous treatment effects at the individual level. Finally, I will report on the current state of a project to estimate the causal impact of COVID-19 on a time series data set in NH.\nLessons Learned: Causality has deep roots in philosophy, and modern causal inference offers a rigorous framework to address the complexities of healthcare data analysis. I emphasize the critical distinction between causal inference and traditional statistics and demonstrate how misleading statistics can misguide decision-making. I stress that causal inference methodologies are in harmony with NH’s mission, vision, and strategic objectives, enhancing the quality of inferences drawn from healthcare data.\n\n\nA Friendly Introduction to Statistical Forecasting\nA Moosy Proposal: Estimating Average Direction of Moose Travel from Weak Information\nAdventures in Non-Negative Canonical Polyadic Decomposition\nHow Correlation Really Works\nIntroduction to Interval Arithmetic\nIntroduction to Using ARIMA\nExample of Training a SARIMAX Model\nReview of a Study Using SARIMAX Guest lecture: Relations and Graphs\nA Gentle Introduction to Geometric Deep Learning{:target=“_blank”}\n\n\n\n\nPractical Approaches to Faster and Leaner Python{:target=“_blank”}\n\n\n\n\nA Gentle Introduction to L-Systems{:target=“_blank”}\nEnumeration of Automorphism Orbits of Graphlets (HackSeq){:target=“_blank”}"
  },
  {
    "objectID": "posts/rust-run-from-python/index.html",
    "href": "posts/rust-run-from-python/index.html",
    "title": "Build and Run a Rust Project from Quarto Using Python",
    "section": "",
    "text": "In a previous post I used a Lua extension to compile a Rust file using the rustc compiler. I ran into multiple problems.\nUsing rustc rather than Cargo means that I miss out on a lot of the build tools, and it is also less conventional for Rust projects.\nThere were also issues with my plugin. It did not correctly turn off echo either locally to a code block, or to the global setting in the preamble of the Quarto file. It also did not put code on a new line. Further, I got feedback that this might not be supported. I also learned from a discussion answer on the Quarto Github discussion board that there are tools that might be better.\nWhile other tools like evcxr look appealing, I have not looked into how to exactly integrate it with Quarto yet.\nBut there is a low-hanging fruit we can take advantage of here. We can certainly use Python subprocess library to indirectly orchestrate building and running a rust project. It also allows us to capture the output as text and return that into a Jupyter notebook. So that’s exaxtly what I made:\nimport subprocess\nimport os\nimport stat\n\ndef log_permissions(path):\n    st = os.stat(path)\n    permissions = stat.filemode(st.st_mode)\n    print(f\"Permissions for {path}: {permissions}\")\n\ndef compile_and_run_rust(target_file):\n    # Get the directory and the file name\n    target_dir = os.path.dirname(target_file)\n    target_name = os.path.basename(target_dir)  # Adjusted to get the correct target name\n\n    # Ensure Cargo.toml exists in the target directory\n    cargo_toml_path = os.path.join(target_dir, 'Cargo.toml')\n    if not os.path.exists(cargo_toml_path):\n        raise FileNotFoundError(\"Cargo.toml not found in the target directory.\")\n\n    # Compile the Rust project\n    try:\n        build_process = subprocess.run(\n            ['cargo', 'build', '--release'],\n            cwd=target_dir,\n            check=True,\n            capture_output=True,\n            text=True\n        )\n    except subprocess.CalledProcessError as e:\n        print(f\"Compilation Error: {e.stderr}\")\n        return\n\n    # Find the compiled executable\n    target_exe = os.path.join(target_dir, 'target', 'release', target_name)\n    if os.name == 'nt':\n        target_exe += '.exe'\n\n    if not os.path.exists(target_exe):\n        raise FileNotFoundError(\"Compiled executable not found.\")\n\n    if os.name != 'nt':\n        try:\n            os.chmod(target_exe, stat.S_IRWXU | stat.S_IRGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)\n            os.chmod(os.path.dirname(target_exe), stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)\n        except PermissionError as e:\n            print(f\"Error setting permissions: {e}\")\n            return\n\n    # Run the compiled executable and capture its output\n    try:\n        run_process = subprocess.run(\n            [target_exe],\n            check=True,\n            capture_output=True,\n            text=True\n        )\n        output = run_process.stdout\n        return output\n    except subprocess.CalledProcessError as e:\n        print(f\"Execution Error: {e.stderr}\")\n        return\n\n# Example usage\nif __name__ == \"__main__\":\n    output = compile_and_run_rust('../posts/rust-run-from-python/hello/main.rs')\n    print(output)\nPython Jupyter notebooks run Python in interactive mode, so it is slightly less convenient for importing Python files. Nonetheless this can be done by inserting our script into the path using the sys library. Once we have imported the run_rust file, we can call the compile_and_run_rust pointing to a Rust project path that is locally stored\nLet us start a Rust project called “hello”.\ncargo init hello\nI also added a loop with a println macro just so we can see how this approach handles keeping newline characters. Here is the Rust code in hello/src/main.rs.\nfn main() {\n\n    let mut count = 0;\n\n    loop {\n        count = count + 1;\n        println!(\"{} Hello, world!\", count);\n        if count &gt; 11 {\n            break\n        }\n    }\n}\nThe above Rust code should print a series of lines each starting with a number, with the numbers ranging from 1 to 12.\nWith all that setup, we can now try using the run_rust.compile_and_run_rust process caller.\n\nimport sys\nsys.path.insert(1, '../../scripts')\n\nimport run_rust\n\nprint(run_rust.compile_and_run_rust('./hello/'))\n\n1 Hello, world!\n2 Hello, world!\n3 Hello, world!\n4 Hello, world!\n5 Hello, world!\n6 Hello, world!\n7 Hello, world!\n8 Hello, world!\n9 Hello, world!\n10 Hello, world!\n11 Hello, world!\n12 Hello, world!\n\n\n\nAnd there we have it! We can further try again, but with #| echo: false in the Python code block to turn off echo:\n\n\n1 Hello, world!\n2 Hello, world!\n3 Hello, world!\n4 Hello, world!\n5 Hello, world!\n6 Hello, world!\n7 Hello, world!\n8 Hello, world!\n9 Hello, world!\n10 Hello, world!\n11 Hello, world!\n12 Hello, world!\n\n\n\nIn conclusion, this approach using Python itself and Python Jupyter notebooks to compile, run, and display the printed output from a Rust program. It succeeds in preserving newline characters, and echo works locally. It also works globally for the whole file. The only remaining thing to watch out for in particular if Quarto’s automatic freezing of posts will not detect if you have changed the Rust code; you may need to change your qmd file in some way."
  },
  {
    "objectID": "posts/python-ciw-g-g-1-queue/index.html",
    "href": "posts/python-ciw-g-g-1-queue/index.html",
    "title": "Implementing a G/G/1 Queue in Ciw",
    "section": "",
    "text": "Ciw is a Python package for simulating queueing networks.\nThe two G’s in G/G/1 do not have to be the same distribution, and respectively can be any distribution with non-negative support.\nWe will use a Hyperexponential distribution for the arrival distribution. A hyperexponential distribution is exactly a mixture distribution of exponential distributions. This has an interpretation of there being an implicit set of arrival processes that each have their own distinct but independent exponential arrival times. In this case we will choose a mixture of four such arrival processes with distinct arrival rates:\n\\[\\begin{align} U_1 \\sim & \\text{Exponential}\\left( \\frac{1}{9} \\right) \\\\ U_2 \\sim & \\text{Exponential}\\left( \\frac{1}{5} \\right) \\\\ U_3 \\sim & \\text{Exponential}\\left( \\frac{1}{6} \\right) \\\\ U_4 \\sim & \\text{Exponential}\\left( 1 \\right) \\end{align}\\]\nThe following mixture distribution for the arrival times will be used:\n\\[T_{\\text{arrivals}} \\sim \\frac{1}{5} f_{U_1} + \\frac{1}{10} f_{U_2} + \\frac{3}{5} f_{U_3} + \\frac{1}{10} f_{U_4}\\]\nWe will use a gamma distribution for the sake of example. A gamma distribution is the result of a sum of independent exponentially-distributed random variable, and thus for this example we have an interpretation that the servicing is implicitly a multi-step process where each step has an exponentially-distributed completion time."
  },
  {
    "objectID": "posts/python-ciw-g-g-1-queue/index.html#introduction",
    "href": "posts/python-ciw-g-g-1-queue/index.html#introduction",
    "title": "Implementing a G/G/1 Queue in Ciw",
    "section": "",
    "text": "Ciw is a Python package for simulating queueing networks.\nThe two G’s in G/G/1 do not have to be the same distribution, and respectively can be any distribution with non-negative support.\nWe will use a Hyperexponential distribution for the arrival distribution. A hyperexponential distribution is exactly a mixture distribution of exponential distributions. This has an interpretation of there being an implicit set of arrival processes that each have their own distinct but independent exponential arrival times. In this case we will choose a mixture of four such arrival processes with distinct arrival rates:\n\\[\\begin{align} U_1 \\sim & \\text{Exponential}\\left( \\frac{1}{9} \\right) \\\\ U_2 \\sim & \\text{Exponential}\\left( \\frac{1}{5} \\right) \\\\ U_3 \\sim & \\text{Exponential}\\left( \\frac{1}{6} \\right) \\\\ U_4 \\sim & \\text{Exponential}\\left( 1 \\right) \\end{align}\\]\nThe following mixture distribution for the arrival times will be used:\n\\[T_{\\text{arrivals}} \\sim \\frac{1}{5} f_{U_1} + \\frac{1}{10} f_{U_2} + \\frac{3}{5} f_{U_3} + \\frac{1}{10} f_{U_4}\\]\nWe will use a gamma distribution for the sake of example. A gamma distribution is the result of a sum of independent exponentially-distributed random variable, and thus for this example we have an interpretation that the servicing is implicitly a multi-step process where each step has an exponentially-distributed completion time."
  },
  {
    "objectID": "posts/python-ciw-g-g-1-queue/index.html#simulation",
    "href": "posts/python-ciw-g-g-1-queue/index.html#simulation",
    "title": "Implementing a G/G/1 Queue in Ciw",
    "section": "Simulation",
    "text": "Simulation\nA G/G/1 queue as described can be implemented and simulated using Ciw in the following way.\n\nimport ciw\n\nciw.seed(2018)\n\narrival_dist = ciw.dists.HyperExponential(rates=[9, 5, 6, 1], probs=[0.2, 0.1, 0.6, 0.1])\nservice_dist = ciw.dists.Gamma(shape=0.6, scale=1.2)\nHORIZON = 365\n\nnetwork = ciw.create_network(\n    arrival_distributions = [arrival_dist],\n    service_distributions = [service_dist],\n    number_of_servers = [1]\n    )\n    \nsimulation = ciw.Simulation(network)\nsimulation.simulate_until_max_time(HORIZON)\nrecords = simulation.get_all_records()"
  },
  {
    "objectID": "posts/python-ciw-g-g-1-queue/index.html#results",
    "href": "posts/python-ciw-g-g-1-queue/index.html#results",
    "title": "Implementing a G/G/1 Queue in Ciw",
    "section": "Results",
    "text": "Results\nWe can tabulate the results.\n\nfrom IPython.display import Markdown, display\nimport pandas as pd\nrecords = pd.DataFrame(records)\ndisplay(\n    Markdown(\n        records\n        [['waiting_time', 'service_time', 'queue_size_at_arrival', 'queue_size_at_departure']]\n        .describe()\n        .to_markdown()\n        )\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nwaiting_time\nservice_time\nqueue_size_at_arrival\nqueue_size_at_departure\n\n\n\n\ncount\n546\n546\n546\n546\n\n\nmean\n118.578\n0.668262\n176.855\n510.295\n\n\nstd\n64.0327\n0.813994\n103.554\n289.318\n\n\nmin\n0\n9.40731e-07\n0\n7\n\n\n25%\n66.9117\n0.0991315\n87\n258\n\n\n50%\n119.865\n0.348732\n177.5\n527.5\n\n\n75%\n175.243\n0.901602\n270.75\n763.5\n\n\nmax\n224.18\n4.88812\n352\n1015\n\n\n\n\n\nWe can plot the arrival times against the waiting times.\n\nrecords.plot(x='arrival_date', y='waiting_time')\n\n\n\n\n\n\n\n\nWe can plot the service start times against the service times.\n\nrecords.plot(x='service_start_date', y='service_time')\n\n\n\n\n\n\n\n\nWe can plot the arrival dates against the length of the queue when the customer arrived.\n\nrecords.plot(x='arrival_date', y='queue_size_at_arrival')\n\n\n\n\n\n\n\n\nWe can plot the departure dates against the length of the queue when the customer departed.\n\nrecords.plot(x='exit_date', y='queue_size_at_departure')"
  },
  {
    "objectID": "posts/what-are-ranks/index.html",
    "href": "posts/what-are-ranks/index.html",
    "title": "What are Ranks?",
    "section": "",
    "text": "Introduction\nThis post looks at the notion of ranking data or variables. Broadly, ranks are numbers used to describe something about the order-like properties on the elements of a set. We might think of runners arriving at the end of a marathon at different times and assigning 1st, 2nd, 3rd, etc, place to the runners in order of their arrival accross the finish line.\n\n\nCommon Ranks in Statistics\nA lot of folks (including myself) have often referred to “the” rank transform, but it actually it isn’t a uniquely specified thing. For example, scipy.stats.rankdata implements multiple ranking methods depending on how tied elements are to be handled:\n\nmethod : {‘average’, ‘min’, ‘max’, ‘dense’, ‘ordinal’}, optional\nThe method used to assign ranks to tied elements. The following methods are available (default is ‘average’):\n\n       `‘average’`: The average of the ranks that would have been assigned to all the tied values is assigned to each value.\n       `‘min’`: The minimum of the ranks that would have been assigned to all the tied values is assigned to each value. (This is also referred to as “competition” ranking.)\n       `‘max’`: The maximum of the ranks that would have been assigned to all the tied values is assigned to each value.\n       `‘dense’`: Like `‘min’`, but the rank of the next highest element is assigned the rank immediately after those assigned to the tied elements.\n       `‘ordinal’`: All values are given a distinct rank, corresponding to the order that the values occur in `a`.\n\n\nAnd similarly Wikipedia lists some methods of ranking: - Standard competition ranking - Modified competition ranking - Dense ranking - Ordinal ranking - Fractional ranking\nLet us consider an example. Supposing a random variable \\(X \\sim \\operatorname{binomial} \\left(10, \\frac{1}{2} \\right)\\) which we can sample of size ten from and print a table using the following Python code. The binomial is suitable for this example in order to show how these ranking methods evaluate ties. In contrast, the distribution of the ranks of a continuous random variable are almost surely uniform and would resemble the ordinal method regardless of the ranking method chosen above.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import rankdata\nfrom IPython.display import Markdown, display\n\n\nnp.random.seed(2018)\n\nmethods = (\n  'average',\n  'min',\n  'max',\n  'dense',\n  'ordinal'\n  )\n\nd = {}\nd['x'] = np.random.binomial(10, 0.5, size=10)\nfor method in methods:\n  d[f'{method} method'] = rankdata(d['x'], method=method)\n\ndf = pd.DataFrame(d)\ndf = df.sort_values(by='x')\ndisplay(Markdown(df.to_markdown(index=False)))\n\n\n\n\n\n\n\n\n\n\n\n\nx\naverage method\nmin method\nmax method\ndense method\nordinal method\n\n\n\n\n3\n1.5\n1\n2\n1\n1\n\n\n3\n1.5\n1\n2\n1\n2\n\n\n4\n3\n3\n3\n2\n3\n\n\n5\n4.5\n4\n5\n3\n4\n\n\n5\n4.5\n4\n5\n3\n5\n\n\n6\n6.5\n6\n7\n4\n6\n\n\n6\n6.5\n6\n7\n4\n7\n\n\n7\n9\n8\n10\n5\n8\n\n\n7\n9\n8\n10\n5\n9\n\n\n7\n9\n8\n10\n5\n10\n\n\n\n\n\nYou can see above that most methods return only integers, with the exception of the “average” method. What the average method accomplishes it preserving the total rank even when ties occur. The “average” method will output rational numbers to account for ties whereas the minimum method and maximum method create spacing between the ranks. Another kind of “rank” which is non-integer-valued is the percentile rank which is the percentage of scores in the sample that are less than the given score. Certain percentile ranks can be calculated using scipy.stats.percentileofscore or by passing pct=True into pandas.DataFrame.rank.\nThe ordinal method ignores ties altogether, but does have the advantage of being equal to the indices of a sorted array up to a translation of unity. The ordinal method is called first in the methods available in pandas.DataFrame.rank.\nMany ranking procedures are what I call count ranks because they involve some kind of counting procedure. For example, the dense ranking method can be considered to assign a rank to an element equal to the number of other elements strictly less than it plus unity.\nThe above ranking procedures assume that the ranks are an order-preserving function, but we can also have order-reversing functions that are in a sense a rank. For example, we might have a weight-lifting competition where the person who lifts the most gets first place, thus assigning the largest weight lifted to value of unity. While scipy.stats.rankdata does not do order-reversed ranking per se, applying an order-reversing operation (e.g. \\(f(X) = - X\\)) before applying an order-preserving ranking will acheive the desired result. Some implementations such as pandas.DataFrame.rank provide a Boolean parameter ascending to indicate whether the ranking will be order-preserving or order-reversing.\nI want to make a distinction which is not frequently made explicit, but does specify the mathematics nicely. Namely we should make the distinction between a ranking and a rank which was suggested language in this post by user Sextus Empiricus.\nRank-based statistics are the use of any ranking function to either produce ranks, or a function of ranks, of random variables.\n\n\nAbstract Ranks\nIn this post I gave the following definition of an “abstract ranking” which covers most forms of ranking.\n\nDefinition\nAssume a collection random variables \\(\\{X_1(\\omega), \\ldots, X_n(\\omega) \\}\\) on outcome space \\(\\Omega\\), and \\(\\leq\\) is a partial order.\nAn abstract ranking \\[\\rho: \\prod_{i=1}^n X_i(\\omega) \\mapsto \\mathbb{R}_{\\geq 0}^n\\] is a function such that there exists a non-decreasing function \\(\\kappa:\\mathbb{N} \\mapsto \\mathbb{R}_{\\geq0}\\) that satisfies \\(\\rho(\\vec x)_i \\leq \\kappa(n)\\) for all \\(i\\in \\{1, \\ldots, n\\}\\).\nIt must also hold that \\(\\rho(\\vec x)_i \\leq \\rho(\\vec x)_j \\iff x_i \\leq x_j\\) for all \\(i,j \\in \\{1, \\ldots, n\\}\\) and for all \\(\\omega \\in \\Omega\\) exor \\(\\rho(\\vec x)_i \\geq \\rho(\\vec x)_j \\iff x_i \\leq x_j\\) for all \\(i,j \\in \\{1, \\ldots, n\\}\\) and for all \\(\\omega \\in \\Omega\\).\nAn component of an image element of an abstract ranking is called an abstract rank.\n\nWhile the above definition is quite abstract, it assumes relatively little and provides an umbrella definition for various forms of ranking. I believe it has applications in learning to rank recommendations or detect inequity in addition to some of its familiar uses.\nBut what does this abstract definition mean? Well, in a pinch it means that a ranking is a collection of ranks that respect some order relation. Like the special cases of ranking given above, any abstract ranking takes a collection of instances of random variables and assigns a number that preserves an order within that collection. Not that this monotonicity applies to the ranks within the ranking, but ranks themselves are not monotonic functions of the outcome space due to the comparative and bounded way they are calculated.\n\n\nRanks vs Ratings\nI want to quickly share a distinct which I find valuable for avoiding certain confusions in mathematical modelling. In this post I am explicitly considering and defining ranking and ranks as functions that satisfy an order relation. This should be contrasted to asking someone to assign ordinals (e.g. Likert-like scales) or forced choice preferences to a selection of options because we often observe intransitivity in human preferences. This intransivity violates the assumptions of a partial order that I made above, which even relaxing from a partially ordered set to a weak ordering will not satisfy.\nWhen I am thinking about these mathematical transformations that are guaranteed to preserve/reverse order I use the term ranking, whereas observations of preferences by a human/AI/system I would instead use the term rating.\n\n\nBeyond Ranking Numbers\nI want to share a less obvious facet of ranking as I have defined it above. When (I believe) most people study random variables they are thinking about numerical-valued observations. But consider two facts:\n\nRandom variables can have images that are composed of just about anything.\n\nExamples:\n\nvectors (although there at least we often use the notation \\(\\vec X\\) for a random vector)\nmatrices\ntensors\nsets\ntrees\ngraphs\nhypergraphs\nsimplicial complexes\ngroups\n\n\nPartial orders can likewise be defined on just about anything.\n\nSo we can have just-about-anything-valued random variables and partial orderings on just about anything. Taking these notions together, we can rankings and ranks defined on just about anything as well. We’re not restricted to orders on numbers.\n\n\nConclusions\nRanks are transformations of things, often numbers, to describe something about their order relationships to other things. In the context of statistics we have rankings of random variables, giving rise to the notion of rank-based statistics to help us study order in the presence of uncertainty."
  },
  {
    "objectID": "posts/python-ciw-d-m-1-queue/index.html",
    "href": "posts/python-ciw-d-m-1-queue/index.html",
    "title": "Implementing a D/M/1 Queue in Ciw",
    "section": "",
    "text": "Ciw is a Python package for simulating queueing networks."
  },
  {
    "objectID": "posts/python-ciw-d-m-1-queue/index.html#introduction",
    "href": "posts/python-ciw-d-m-1-queue/index.html#introduction",
    "title": "Implementing a D/M/1 Queue in Ciw",
    "section": "",
    "text": "Ciw is a Python package for simulating queueing networks."
  },
  {
    "objectID": "posts/python-ciw-d-m-1-queue/index.html#simulation",
    "href": "posts/python-ciw-d-m-1-queue/index.html#simulation",
    "title": "Implementing a D/M/1 Queue in Ciw",
    "section": "Simulation",
    "text": "Simulation\nA D/M/1 queue can be implemented and simulated using Ciw in the following way.\n\nimport ciw\n\nciw.seed(2018)\n\nARRIVAL_TIME = 1\nSERVICE_TIME = 1 / 2\nHORIZON = 365\n\nnetwork = ciw.create_network(\n    arrival_distributions = [ciw.dists.Deterministic(ARRIVAL_TIME)],\n    service_distributions = [ciw.dists.Exponential(SERVICE_TIME)],\n    number_of_servers = [1]\n    )\n\nsimulation = ciw.Simulation(network)\nsimulation.simulate_until_max_time(HORIZON)\nrecords = simulation.get_all_records()"
  },
  {
    "objectID": "posts/python-ciw-d-m-1-queue/index.html#results",
    "href": "posts/python-ciw-d-m-1-queue/index.html#results",
    "title": "Implementing a D/M/1 Queue in Ciw",
    "section": "Results",
    "text": "Results\nWe can tabulate the results.\n\nfrom IPython.display import Markdown, display\nimport pandas as pd\nrecords = pd.DataFrame(records)\ndisplay(\n    Markdown(\n        records\n        [['waiting_time', 'service_time', 'queue_size_at_arrival', 'queue_size_at_departure']]\n        .describe()\n        .to_markdown()\n        )\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nwaiting_time\nservice_time\nqueue_size_at_arrival\nqueue_size_at_departure\n\n\n\n\ncount\n183\n183\n183\n183\n\n\nmean\n71.9111\n1.96964\n36.8087\n73.3607\n\n\nstd\n51.243\n1.8815\n25.7219\n51.6083\n\n\nmin\n0\n0.0229044\n0\n0\n\n\n25%\n19.1338\n0.676208\n13.5\n20\n\n\n50%\n75.1552\n1.28472\n34\n76\n\n\n75%\n111.045\n2.53828\n61\n112\n\n\nmax\n175.985\n9.49261\n80\n179\n\n\n\n\n\nWe can plot the arrival times against the waiting times.\n\nrecords.plot(x='arrival_date', y='waiting_time')\n\n\n\n\n\n\n\n\nWe can plot the service start times against the service times.\n\nrecords.plot(x='service_start_date', y='service_time')\n\n\n\n\n\n\n\n\nWe can plot the arrival dates against the length of the queue when the customer arrived.\n\nrecords.plot(x='arrival_date', y='queue_size_at_arrival')\n\n\n\n\n\n\n\n\nWe can plot the departure dates against the length of the queue when the customer departed.\n\nrecords.plot(x='exit_date', y='queue_size_at_departure')"
  },
  {
    "objectID": "posts/pymc-mixtures-of-dirac-delta-distributions/index.html",
    "href": "posts/pymc-mixtures-of-dirac-delta-distributions/index.html",
    "title": "Mixtures of Dirac Delta Distributions in PyMC",
    "section": "",
    "text": "This post just shows what a Dirac delta distribution is and how to make a mixture of Dirac delta distributions in PyMC.\nThe Dirac delta distribution is somewhat unlike most distributions you will come across. Instead of having some spread of non-zero probability density of a measurable set of possibilities, we instead have all of the density concentrated on a single outcome. Usually denoted \\(\\delta\\), it is defined by\n\\[\\delta (x) = \\lim_{\\sigma \\rightarrow 0} \\frac{1}{|\\sigma| \\sqrt{\\pi}}e^{-\\left( \\frac{x}{\\sigma} \\right)^2}\\]\nwhich can be thought of as a normal distibution whose standard deviation limits to zero. The first time I encountered this strange creature of mathematics I thought it was little more than a curiousity, but it turns out to be a highly useful and general tool in statistics, and engineering.\nBut why would you want to have a mixture of Dirac delta distributions? There is no known (to me) use case for having a mixture of “only” Dirac delta distributions because it would be equivalent to simply having a discrete random variable with that many levels. Rather a mixture of Dirac delta distributions is of theoretical importance. You can represent a discrete (i.e. countable) random variable as a mixture of continuous random variables.\nLet us setup an example. Suppose I have\n\\[f(x) = \\sum_{i=0}^{k-1} w_i \\delta(x-i)\\]\nwhere \\(\\vec W\\) follows a Dirichlet distribution and \\(X\\) follows a discrete uniform distribution \\(U(0,k-1)\\). This is an unusual construction. Notice that \\(\\sum_{i=0}^{k-1} W_i = 1\\) giving us our mixture distribution of Dirac delta distributions. The Dirac delta distributions really just force all of the \\(x\\) values to either match with some \\(w_i\\) or not. Thus, indirectly, \\(w_i\\) is our estimate of \\(Pr(x_i)\\).\nHere is some Python code simulating this example. The number of such distributions is controlled by the hyperparameter \\(k\\), which I have taken to be three. The sample size is \\(m=10^3\\).\n\nimport pymc as pm\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample size\nm = 10**3\n\n# Number of distributions in mixture\nk = 3\n\n# Generate data\ndata = np.random.randint(0,k,size=m)\n\n# Model\nbasic_model = pm.Model()\n\n# Mixture of Dirac Delta\nwith basic_model:\n    w = pm.Dirichlet('w', a=np.array([1]*k))\n\n    components = [pm.DiracDelta.dist(c=i) for i in range(k)]\n\n    like = pm.Mixture('like', w=w, comp_dists=components, observed=data)\n    \nwith basic_model:\n    idata = pm.sample(2000)\n    az.plot_trace(idata)\n    plt.tight_layout()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [w]\n\n\n\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 5 seconds.\n\n\n\n\n\n\n\n\n\nNotice that our parameters are roughly centered around \\(\\frac{1}{3}\\)? This is because there are three outcomes for \\(x\\) and they were all equally probable.\nBut there is a use case for including Dirac delta distributions in a mixture with other distributions: inflated values. An inflated value is one in which it is extremely common among other values. A typical example of an inflated value is zero-inflation, where zero is especially common. I’ve seen (what was purportely) phone call durations be zero-inflated. But you can also have inflation of values other than zero. Let’s take our same example from above of mixing three Dirac delta distribution and include a Poisson distribution! That is to say, we will have a mixture of a Poisson distribution with inflation of 0, 1, and 2.\nHere is some Python code to run this simulation.\n\nimport pymc as pm\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample size\nm = 10**3\n\n# Number of distributions in mixture\nk = 3\n\n# Generate data\ndata = np.random.randint(0,k,size=m)\ndata = np.concatenate((data, np.random.poisson(size=m)))\n\n# Model\nmodel = pm.Model()\n\n# Mixture of Dirac Delta and Poisson\nwith model:\n    w = pm.Dirichlet('w', a=np.array([1]*(k+1)))\n    mu = pm.Poisson.dist(1)\n\n    components = [pm.DiracDelta.dist(c=i) for i in range(k)]\n    components += [mu]\n\n    likelihood = pm.Mixture('like', w=w, comp_dists=components, observed=data)\n    \nwith model:\n    idata = pm.sample(2000)\n    az.plot_trace(idata)\n    plt.tight_layout()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [w]\n\n\n\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 7 seconds.\n\n\n\n\n\n\n\n\n\nNotice that the weights are not all around \\(\\frac{1}{3}\\). With the Poisson distribution having a mean parameter of \\(\\lambda = 1\\) it tends to take up more of the probability mass than the inflated values. That is true for this example where the data generating process is constructed to be so. Also notice that \\(w_i\\) is no longer simply the probability of \\(p(x_i)\\) because some values \\(x_i\\) could be generated by either a Dirac delta distribution or by the Poisson distribution.\nDirac delta distributions can represent discrete probability distributions. More pracitcally, we can create mixture distributions that combinate Dirac delta distributions with other distributions."
  },
  {
    "objectID": "posts/unzip-inaturalist-observations-ubuntu/index.html",
    "href": "posts/unzip-inaturalist-observations-ubuntu/index.html",
    "title": "Unzip Your iNaturalist Observations On A Ubuntu System",
    "section": "",
    "text": "Note\n\n\n\nThis post was migrated from my iNaturalist journal to my Jekyll blog on 2023-02-26. It was then migrated to my Quarto blog on 2024-07-27.\n\n\nSuppose - you have just downloaded an export of iNaturalist data, - the data file is a zipped comma-separated value(CSV) text file, - and you are running a Ubuntu system.\nBegin by opening up a BASH environment.\nGo to the path where the file was downloaded to:\ncd /path/to/folder\nThen run the unzip command:\nunzip observations-&lt;ID&gt;.csv.zip\nYou should find that you now have the uncompressed CSV file. You can check with:\nls observations-&lt;ID&gt;.csv"
  },
  {
    "objectID": "posts/neural-networks-simple-functions/index.html",
    "href": "posts/neural-networks-simple-functions/index.html",
    "title": "How Expressive Are Simple Neural Networks?",
    "section": "",
    "text": "It is well-known that neural networks are universal function approximators under mild assumptions. Such theorems proving arbitrary bounds on the empirical risk and similar results tell us that a good approximation exists. They do not tell us how readily neural networks will approximate other functions in practice. The last few decades have shown that neural networks can be hugely successful on complicated domains such as images, audio and text.\nBut still I do not have refined intuitions for just how good/bad neural networks are when they are simple. The common approach of building neural networks that are more complicated than needed but are then regularized to prevent overfitting is obviously useful, but it has done little to understand the boundary where they break down. That is, except for a few classic examples such as learn the XOR function with a single hidden layer in a multi-layer perceptron.\nIn keeping with that theme of tinkering with simple scenarios to gain intuitions about the fundamentals, I decided to train some multilayer perceptrons are some elementary functions.\nThe following script generated noiseless data sets representing systematic samplings of functions such as \\(\\sin\\), \\(\\cos\\), \\(\\tan\\), \\(e^t\\), \\(2t+3\\), and a few polynomials. The data was normalized in the domain (within the model) and image (outside of the model) of the estimand functions to reduce the effects of differing location and scale. That is to say, I was more interested in the learnability of “shape” up-to-but-not-including location and scale. For each data set a sequence of multilayer perceptrons were trained on it with increasing width in the hidden layer. Finally, plots were generated to understand the training behaviour and the visual goodness of fit. For these examples I am only interested in the models’ ability to fit a given pattern, but not whether it generalizes well.\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Utility\n\ndef tfnormalize(x):\n    return (x - tf.reduce_mean(x)) /\\\n           tf.math.reduce_variance(x)\n\n# MLP factory\ndef genmlp(width):\n    '''\n    width: width of internal layer.\n    '''\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.Input(shape=(1,)))\n    model.add(tf.keras.layers.Normalization())\n    model.add(tf.keras.layers.Dense(width, activation='sigmoid'))\n    model.add(tf.keras.layers.Dense(1))\n    return model\n    \n\n# Prepare data\nt = tf.linspace(-10, 10, num=10**4)\nt = tf.reshape(t, shape=(-1,1))\nfunctions = {\n    'sine':tf.sin,\n    'cosine':tf.cos,\n    'tan':tf.tan,\n    'exponential':tf.exp,\n    'linear':lambda x: 2*x+3,\n    'quadratic':lambda x: tf.pow(x,2),\n    'cubic':lambda x: tf.pow(x,3),\n    'quartic':lambda x: tf.pow(x,4),\n    'abs':tf.abs\n    }\nimages = {key:f(t) for key,f in functions.items()}\nimages = {key:tfnormalize(ft) for key,ft in images.items()}\n\n# Train models\nresults = {}\nmodels = {}\nwidths = list(range(1,11))\nfor func in functions:\n    results[func] = {}\n    models[func] = {}\n    y = images[func]\n    for w in widths:\n        print(func, w)\n        models[func][w] = genmlp(w)\n        models[func][w].compile(optimizer='adam',loss='mse')\n        history = models[func][w].fit(x=t, y=y, epochs=100)\n        results[func][w] = history.history['loss']\n\n\n# Plot results\nfor func in results:\n    fig, axis = plt.subplots()\n    axis.set_title(func)\n    for w in results[func]:\n        plt.plot(results[func][w], label=str(w))\n    axis.set_yscale('log')\n    fig.legend()\n    plt.savefig(f'{func}_training_history.png', dpi=300, transparent=True)\n    plt.close()\n\n# Plot model predictions\nfor func in models:\n    fig, axis = plt.subplots()\n    axis.set_title(func)\n    axis.plot(t, images[func], label='True', color='k')\n    for w in models[func]:\n        axis.plot(t, models[func][w].predict(t), label=str(w))\n    fig.legend()\n    plt.savefig(f'{func}_visual_fit.png', dpi=300, transparent=True)\n    plt.close()\nThe following table of figures summarizes the results.\n\n\n\n\n\n\n\n\nFunction\nTraining History\nVisual Fit\n\n\n\n\nsin\n\n\n\n\ncos\n\n\n\n\ntan\n\n\n\n\nexponential\n\n\n\n\nlinear\n\n\n\n\nquadratic\n\n\n\n\ncubic\n\n\n\n\nquartic\n\n\n\n\nabs\n\n\n\n\n\nI see in the training histories that there is noisiness in some of the loss updates, suggesting that some combination of step size or numerical precision could be at play. Probably the former I suspect since the error is not especially close to the machine epsilon around \\(10^{-16}\\). In some cases such as the linear function it may just be that the gradient is more-or-less flat since the visual goodness of fit is excellent even for a single neuron.\nUnsurprisingly the models with only a single hidden neuron did poorly whenever there was a substantial bend in the data, but in many cases they substantially improved with only a handful more neurons. The only function where the networks made little improvement on approximating was the tangent function, which even after normalization still gives really sharp turns in the data that are not easily learned by these simple neural networks. The next-worst were the other sinusoids sine and cosine. While sine and cosine don’t have discontinuities, they do pose a challenge because their curvature periodically changes sign. Sigmoid functions a monotonic, so just to acheive a single pattern of down-up or up-down requires at least two neurons. With periodic functions over the real number line there is a need for an infinite number of such turns… But taking the modulus \\(2 \\pi\\) in some cases could vastly simplify what the model has to learn. Then again, if you know that all you have a superposition of waves on a fixed interval then you might as well use Fourier series. But then again, ‘again’, you could say that for any of these functions. Neural networks will always perform no-better than the ground truth, and that their main selling point is approximating a function when you do not know it.\nNow, it is worth noting that this demonstration doesn’t provide much in the way of highly general inferences. There are many choices to how I setup this experiment that could be tweaked in ways that improve or worsen the fit of the neural networks. But still, I find seeing examples adds an extra level of (hopefully not misleading) intution about what things can look like.\nOn the whole of it, I’m impressed at what even a small number of hidden neurons can accomplish. We often don’t get to see these examples because when the estimand functions are so simple we don’t usually both with neural networks. It was interesting for me to just take a peak this idealized case for intuition."
  },
  {
    "objectID": "posts/zig-run-from-python/index.html",
    "href": "posts/zig-run-from-python/index.html",
    "title": "Build and Run a Zig Project from Quarto Using Python",
    "section": "",
    "text": "In this post I share a way to compile and run a Zig file using Python, which in turn can be used to render the output of the Zig program in Quarto."
  },
  {
    "objectID": "posts/zig-run-from-python/index.html#python-script",
    "href": "posts/zig-run-from-python/index.html#python-script",
    "title": "Build and Run a Zig Project from Quarto Using Python",
    "section": "Python Script",
    "text": "Python Script\nThe Python script I developed is similar to the one I made for Rust, except that it does two phases. While Zig has a very powerful and flexible build system, I will only attempt to compile a single project file.\nimport subprocess\nimport os\nimport stat\n\ndef log_permissions(path):\n    st = os.stat(path)\n    permissions = stat.filemode(st.st_mode)\n    print(f\"Permissions for {path}: {permissions}\")\n\ndef find_executable(bin_dir):\n    \"\"\"Find the executable file in the given directory.\"\"\"\n    for root, dirs, files in os.walk(bin_dir):\n        for file in files:\n            file_path = os.path.join(root, file)\n            if os.access(file_path, os.X_OK):\n                return file_path\n    raise FileNotFoundError(\"No executable found in the bin directory.\")\n\ndef build_and_run_zig(project_dir):\n    project_dir = os.path.abspath(project_dir)\n\n    # Build the Zig project\n    try:\n        build_process = subprocess.run(\n            ['zig', 'build'],\n            cwd=project_dir,\n            check=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n    except subprocess.CalledProcessError as e:\n        print(f\"Build Error: {e.stderr}\")\n        return\n\n    # Find the executable in the zig-out/bin/ directory\n    bin_dir = os.path.join(project_dir, 'zig-out', 'bin')\n    if not os.path.exists(bin_dir):\n        raise FileNotFoundError(\"The bin directory does not exist after the build.\")\n    \n    exe_path = find_executable(bin_dir)\n\n    # Set the executable permissions\n    try:\n        os.chmod(exe_path, stat.S_IRWXU | stat.S_IRGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)\n    except PermissionError as e:\n        print(f\"Error setting permissions: {e}\")\n        return\n\n    # Run the compiled executable and print its output\n    try:\n        run_process = subprocess.run(\n            [exe_path],\n            check=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        # print(run_process.stdout)  # Print the output from the Zig executable\n        # print(run_process.stderr)  # Print any error output\n        if run_process.stdout:\n            return run_process.stdout\n        else:\n            return run_process.stderr\n    except subprocess.CalledProcessError as e:\n        print(f\"Execution Error: {e.stderr}\")\n        return\nLet’s see it in action."
  },
  {
    "objectID": "posts/zig-run-from-python/index.html#single-file-example",
    "href": "posts/zig-run-from-python/index.html#single-file-example",
    "title": "Build and Run a Zig Project from Quarto Using Python",
    "section": "Single File Example",
    "text": "Single File Example\nHere I will define a simple “Hello, World” example. We can do this by creating a project folder hello, and calling zig init within it to create a default project. In the default main.zig I put the following Zig code.\nconst std = @import(\"std\");\n\npub fn main() void {\n    std.debug.print(\"Hello, World!\\n\", .{});\n}\n\n\n\n\n\n\nCaution\n\n\n\nI have used Rust’s syntax highlighting for the above Zig code.\n\n\nNow we can build and run our Zig project by using the earlier Python script.\n\nimport sys\nsys.path.insert(1, '../../scripts')\n\nfrom run_zig import build_and_run_zig\n\nprint(build_and_run_zig('./hello'))\n\nHello, World!"
  },
  {
    "objectID": "posts/zig-run-from-python/index.html#conclusion",
    "href": "posts/zig-run-from-python/index.html#conclusion",
    "title": "Build and Run a Zig Project from Quarto Using Python",
    "section": "Conclusion",
    "text": "Conclusion\nWhile Zig doesn’t really have a REPL (that I’m aware of), you can include its standard output in Quarto blogs using Python’s Subprocess library."
  },
  {
    "objectID": "posts/rust-execution-for-quarto/index.html",
    "href": "posts/rust-execution-for-quarto/index.html",
    "title": "Executable Rust Code in Quarto",
    "section": "",
    "text": "Deprecated Output\n\n\n\nI have turned off the Lua filter discussed in this post. It was running everytime I rendered/published the blog. I have copy-pasted the original output for reference.\n\n\nThe following is a Lua filter which looks through a qmd file for Rust code associated with {rust}, compiles that code using rustc, runs the compiled Rust program and collects its output, and inserts the output to be rendered by pandoc.\nlocal io = require(\"io\")\nlocal os = require(\"os\")\nlocal tempfile = require(\"os\").tmpname\nlocal log_file\n\n-- Function to initialize the log file\nlocal function init_log()\n  log_file = io.open(\"rust_executor_debug.log\", \"w\")\nend\n\n-- Function to log messages to file and stderr\nlocal function log(...)\n  local args = {...}\n  for i = 1, #args do\n    args[i] = tostring(args[i])\n  end\n  local message = table.concat(args, \" \")\n  if log_file then\n    log_file:write(message .. \"\\n\")\n    log_file:flush()\n  end\n  io.stderr:write(message .. \"\\n\")\n  io.stderr:flush()\nend\n\n-- Helper function to execute Rust code and return the output\nlocal function execute_rust_code(code)\n  local temp_file = tempfile() .. \".rs\"\n  log(\"Temporary Rust file:\", temp_file)\n  local source_file, err = io.open(temp_file, \"w\")\n  if not source_file then\n    log(\"Failed to create source file:\", err)\n    error(\"Failed to create source file: \" .. err)\n  end\n\n  source_file:write(code)\n  source_file:close()\n\n  local temp_bin = tempfile()\n  log(\"Temporary binary file:\", temp_bin)\n\n  local compile_command = \"rustc \" .. temp_file .. \" -o \" .. temp_bin .. \" 2&gt;&1\"\n  log(\"Compile Command:\", compile_command)\n  local compile_pipe = io.popen(compile_command)\n  local compile_output = compile_pipe:read(\"*a\")\n  local compile_result = compile_pipe:close()\n\n  if compile_result ~= true then\n    os.remove(temp_file)\n    log(\"Rust compilation failed. Output:\", compile_output)\n    error(\"Rust compilation failed. Output: \" .. compile_output)\n  end\n\n  local exec_command = temp_bin .. \" 2&gt;&1\"\n  log(\"Exec Command:\", exec_command)\n  local exec_pipe = io.popen(exec_command)\n  local output = exec_pipe:read(\"*a\")\n  exec_pipe:close()\n\n  local ok, rm_err = pcall(function()\n    os.remove(temp_file)\n    os.remove(temp_bin)\n  end)\n  if not ok then\n    log(\"Failed to clean up temporary files:\", rm_err)\n    error(\"Failed to clean up temporary files: \" .. rm_err)\n  end\n\n  log(\"Output:\", output)\n  return output\nend\n\nlocal echo_global = true\n\nfunction Meta(meta)\n  if meta.echo ~= nil then\n    echo_global = pandoc.utils.stringify(meta.echo) == \"true\"\n  end\nend\n\n-- Lua filter function\nfunction CodeBlock(elem)\n  if not log_file then\n    init_log()\n  end\n\n  local is_rust_code = elem.attr.classes:includes(\"{rust}\")\n  if is_rust_code then\n    log(\"Processing Rust code block\")\n    local output = execute_rust_code(elem.text)\n    output = output:gsub(\"%s+$\", \"\")\n    local blocks = {}\n\n    if echo_global then\n      -- Render Rust code as a formatted block\n      table.insert(blocks, pandoc.CodeBlock(elem.text, {class=\"rust\"}))\n    end\n\n    -- Always return the output\n    table.insert(blocks, pandoc.Para(pandoc.Str(output)))\n\n    return blocks\n  else\n    log(\"Skipping non-Rust code block\")\n  end\nend\n\n-- Ensure log file is closed properly at the end\nfunction Pandoc(doc)\n  if log_file then\n    log_file:close()\n  end\n  return doc\nend\nLet’s try some examples.\nHere is some Rust code that will be executed and rendered.\nfn main() {\n        println!(\"Galen Seilis is learning Rust!\");\n        println!(\"Time to get Rusty!\");\n}\nGalen Seilis is learning Rust! Time to get Rusty!\nNow let us try some Rust code that will not be executed.\nfn main() {\n    println!(\"Meow\");\n}\nNow let us run a longer example from Rust by Example.\nfn main() {\n    // Integer addition\n    println!(\"1 + 2 = {}\", 1u32 + 2);\n\n    // Integer subtraction\n    println!(\"1 - 2 = {}\", 1i32 - 2);\n    // TODO ^ Try changing `1i32` to `1u32` to see why the type is important\n\n    // Scientific notation\n    println!(\"1e4 is {}, -2.5e-3 is {}\", 1e4, -2.5e-3);\n\n    // Short-circuiting boolean logic\n    println!(\"true AND false is {}\", true && false);\n    println!(\"true OR false is {}\", true || false);\n    println!(\"NOT true is {}\", !true);\n\n    // Bitwise operations\n    println!(\"0011 AND 0101 is {:04b}\", 0b0011u32 & 0b0101);\n    println!(\"0011 OR 0101 is {:04b}\", 0b0011u32 | 0b0101);\n    println!(\"0011 XOR 0101 is {:04b}\\n\\n\\n\", 0b0011u32 ^ 0b0101);\n    println!(\"1 &lt;&lt; 5 is {}\", 1u32 &lt;&lt; 5);\n    println!(\"0x80 &gt;&gt; 2 is 0x{:x}\", 0x80u32 &gt;&gt; 2);\n\n    // Use underscores to improve readability!\n    println!(\"One million is written as {}\", 1_000_000u32);\n}\n1 + 2 = 3 1 - 2 = -1 1e4 is 10000, -2.5e-3 is -0.0025 true AND false is false true OR false is true NOT true is false 0011 AND 0101 is 0001 0011 OR 0101 is 0111 0011 XOR 0101 is 0110 1 &lt;&lt; 5 is 32 0x80 &gt;&gt; 2 is 0x20 One million is written as 1000000\nIn the current state there are a couple of glaring issues I have with this implementation. The first is that Rust code blocks will be run regardless of whether echo: false is used. The second is that all the outputs are being rendered on a single, notwithstanding Quarto’s line wrapping.\nThere is also an enhancement which is desirable, which is to render other types of things from Rust that are not just plaintext. Instead of developing this kind of functionality myself, it would make sense to take a closer look at integrating tools such as the Evcxr Jupyter kernel."
  },
  {
    "objectID": "posts/flask-url-for/index.html",
    "href": "posts/flask-url-for/index.html",
    "title": "Creating URL using url_for in Flask",
    "section": "",
    "text": "In this post I quickly show how the url_for command allows us to generate URLs. It isn’t a fundamental feature, but it can save you some boilerplate.\nimport requests\nimport io\n\nfrom flask import Flask, url_for\n\napp = Flask(__name__)\n\n@app.route('/card/&lt;int:card_id&gt;')\ndef card(card_id):\n    return f'Card({card_id})'\n\nwith app.test_request_context():\n    for i in range(10):\n        print(url_for('card', card_id=str(i)))\n\nif __name__ == '__main__':\n    app.run(debug=True)\nThe function test_request_context is described in its API documentation as follows:\n\nCreate a RequestContext for a WSGI environment created from the given values. This is mostly useful during testing, where you may want to run a function that uses request data without dispatching a full request.\n\nIndeed, if you look at the CLI output from running the application you will see something like this:\n/card/0\n/card/1\n/card/2\n/card/3\n/card/4\n/card/5\n/card/6\n/card/7\n/card/8\n/card/9\nWhile this is a simple example. you can use url_for to create more complicated URLs and content."
  },
  {
    "objectID": "posts/python-naive-simulate-autoregressive-data/index.html",
    "href": "posts/python-naive-simulate-autoregressive-data/index.html",
    "title": "How to Naïvely Generate Autoregressive Time Series Data in Python",
    "section": "",
    "text": "In this post I will show you how to simulate an autoregressive (AR) process using Matplotlib and NumPy.\nFirst, let’s define a function which pseudorandomly generates the data sequence. We’ll set an intercept parameter to set a consistent offset of the series from zero, and a sequence of autoregressive coefficients. The order of the coefficients entails the lag order that they are each applied to, so if you want to skip a lag order you can just set it to zero. Some AR series are noisier than others, and you can set that level of noise using the noise parameter. In this model we assume that the noise is a stationary normal distribution:\n\\[\\epsilon_t \\sim \\mathcal{N}(0,\\sigma)\\]\nfor all \\(t\\).\nThe following implementation is a generalization of the implementation provided in Generate Fake Autoregressive Data in which I have allowed for any number of time lags. I have also allowed a user-provided random number generator to be passed.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom typing import List\n\ndef simulate_ar(\n    intercept: float,\n    coefs: List[float],\n    noise: float = 0.3,\n    warmup: int = 10,\n    steps: int = 200,\n    rng=None\n    ) -&gt; np.ndarray:\n    \"\"\"Simulates an autoregressive (AR) time series.\n\n    Args:\n        intercept (float): The intercept term of the AR process.\n        coefs (List[float]): List of coefficients for the lagged terms.\n        noise (float, optional): The standard deviation of the Gaussian noise. Default is 0.3.\n        warmup (int, optional): Number of initial steps to discard to allow the process to stabilize. Default is 10.\n        steps (int, optional): Number of steps to simulate after the warmup period. Default is 200.\n\n    Returns:\n        np.ndarray: Simulated AR time series data of length `steps`.\n    \"\"\"\n    rng = np.random.default_rng() if rng is None else rng\n    max_lag = len(coefs)\n    draws = np.zeros(warmup + steps)\n    draws[:max_lag] = intercept\n    for step in range(max_lag, warmup + steps):\n        draws[step] = intercept + rng.normal(0, noise)\n        for lag, coef in enumerate(coefs, start=1):\n            draws[step] += coef * draws[step - lag]\n    return draws[warmup:]\n\nLet us set a seed for reproducibility.\n\nRANDOM_SEED = 2018\nrng = np.random.default_rng(RANDOM_SEED)\n\nNow let’s choose some parameters.\n\n# True parameters of the AR process\nintercept = 10\ncoefs = [-0.3, 0.1, -0.3]  # Example coefficients for AR process with 3 lags\n\nNow let us simulate the AR process.\n\nar_data = simulate_ar(intercept, coefs, warmup=2018, steps=200, rng=rng)\n\nFinally, let us plot the simulated data using Matplotlib.\n\nfig, ax = plt.subplots(figsize=(8, 3))\nax.set_title(\"Generated Autoregressive Timeseries\", fontsize=15)\nax.plot(ar_data)\nax.set_xlabel('Time')\nax.set_ylabel('Signal')\nplt.show()\n\n\n\n\n\n\n\n\nThis implementation is not particularly efficient in terms of computing resources. Although performance is influenced by many things, a large factor is the presence of an ordinary for loop. Upon every iteration of the loop the Python interpreter will check that all types are still valid, which is a waste of computing resources when you can assume that they are. Perhaps I will write a more efficient function, but this example should be suitable for examples and tinkering for now.\n\nHere is the output data for the example:\n\nar_data\n\narray([6.941708  , 6.3155875 , 6.79896769, 6.62115125, 6.64690478,\n       6.79654732, 6.32163015, 6.72852581, 6.35004628, 6.89721308,\n       6.92979629, 6.97466588, 6.47919687, 6.24134393, 6.91045418,\n       6.43927381, 6.55083547, 6.53331397, 6.44255904, 6.58915744,\n       6.4923775 , 7.07488836, 6.32039603, 7.21035304, 6.27709664,\n       7.0134047 , 6.68872494, 6.97816857, 6.75025929, 7.00950853,\n       6.39465926, 6.68118717, 6.85268958, 6.46779684, 6.76181945,\n       7.29674199, 6.64142503, 6.58699206, 6.72707155, 5.87192126,\n       6.49363922, 6.28287734, 7.18342675, 6.70507765, 6.94382677,\n       6.39717353, 6.49306093, 7.02621907, 6.8610425 , 6.87523356,\n       6.7267247 , 6.46888024, 6.8280871 , 6.7789368 , 6.77176061,\n       6.58257732, 6.33784174, 6.808397  , 6.62658717, 7.53212642,\n       6.59001865, 7.09541224, 6.4240351 , 6.40443905, 6.08217251,\n       7.13828813, 7.09221513, 7.08928927, 6.21026942, 6.76872715,\n       6.28272682, 6.77029136, 6.46984318, 7.0331383 , 6.85807573,\n       6.64286023, 6.3238929 , 7.03278515, 6.54674528, 6.97528036,\n       7.11517396, 6.22263056, 6.74882539, 6.66139963, 7.0783666 ,\n       6.23980551, 6.67212967, 6.46792713, 6.66990209, 6.77251836,\n       6.94614928, 6.83357381, 6.89816709, 6.15734245, 6.86025094,\n       6.93488203, 6.46304183, 6.91241082, 6.47560317, 6.89390062,\n       7.12772216, 6.41811335, 7.40498293, 6.42844055, 7.2251552 ,\n       6.87492573, 6.15158164, 6.82079686, 6.790811  , 6.90070107,\n       6.37238571, 6.42500936, 6.56438666, 6.35378324, 7.26239073,\n       6.54888149, 6.80957892, 6.20860224, 7.48779131, 6.84955546,\n       6.94570356, 6.72697226, 6.27613628, 6.89544594, 6.75888508,\n       7.07931692, 6.66666166, 6.3180758 , 6.19398727, 7.09355878,\n       6.49304741, 7.55478488, 6.35099042, 6.92931065, 6.00868397,\n       6.86902339, 6.99545602, 6.95783505, 6.64467603, 6.47337945,\n       6.78201172, 6.63753378, 7.48158873, 6.51522663, 7.05250727,\n       6.39296225, 7.3519113 , 6.66425586, 6.48041592, 6.17408598,\n       6.45883478, 6.22486773, 6.54137246, 6.34066406, 6.96759892,\n       6.72473722, 6.98529234, 6.58060658, 6.59542768, 6.54721287,\n       6.88949907, 6.43898143, 7.00231541, 6.1544984 , 6.68328599,\n       6.12233025, 6.16702292, 6.64338878, 7.31158216, 6.90580583,\n       6.62631565, 6.80756359, 6.87177271, 6.57651991, 6.29329644,\n       6.78533065, 6.8948178 , 6.61792592, 7.0182514 , 6.22628874,\n       6.86051622, 6.49121734, 7.16799008, 6.20211378, 7.30671013,\n       6.12323977, 7.34758837, 6.16358606, 6.88596075, 6.71944393,\n       6.36211338, 7.12466492, 6.27294854, 7.01723785, 6.84008439,\n       6.80771285, 5.93900768, 6.51765076, 6.92400731, 6.54072599])"
  },
  {
    "objectID": "posts/bayesian-workflow-diagram-mermaid/index.html",
    "href": "posts/bayesian-workflow-diagram-mermaid/index.html",
    "title": "A Mermaid Diagram of Bayesian Workflow",
    "section": "",
    "text": "In this post I visualize the Bayesian workflow introduced by Gelman et al. (2020).\nI am setting Quato’s echo: true so that you can see the Mermaid instructions for the diagram.\n\nflowchart TD\n1[\"Pick an initial model\"]\n2{\"Prior predictive check\"}\n3[\"Fit the model\"]\n4{\"Validate computation\"}\n5[\"Address computational issues\"]\n6[\"Evaluate and use model\"]\n7[\"Modify the Model\"]\n8[\"Compare Models\"]\n\n1 --&gt; 2\n2 --&gt; |Accept Priors| 3\n2 --&gt; |Reject Priors| 7\n3 --&gt; 4\n4 --&gt; |Invalid| 5\n4 --&gt; |Valid| 6\n5 --&gt; 3\n6 --&gt; |Reject Model| 7\n6 --&gt; |Accept Model| 8\n6 --&gt; |Accept Model| 7\n5 --&gt; |Give Up| 7\n7 --&gt; 2\n\n\n\n\nflowchart TD\n1[\"Pick an initial model\"]\n2{\"Prior predictive check\"}\n3[\"Fit the model\"]\n4{\"Validate computation\"}\n5[\"Address computational issues\"]\n6[\"Evaluate and use model\"]\n7[\"Modify the Model\"]\n8[\"Compare Models\"]\n\n1 --&gt; 2\n2 --&gt; |Accept Priors| 3\n2 --&gt; |Reject Priors| 7\n3 --&gt; 4\n4 --&gt; |Invalid| 5\n4 --&gt; |Valid| 6\n5 --&gt; 3\n6 --&gt; |Reject Model| 7\n6 --&gt; |Accept Model| 8\n6 --&gt; |Accept Model| 7\n5 --&gt; |Give Up| 7\n7 --&gt; 2\n\n\n\n\n\n\nIn the previous diagram I left out details of what to consider in some of the steps. The following diagram expands on some of the steps by showing there are a variety of different approaches depending on the current state of a workflow.\n\nmindmap\n  root((Bayesian Workflow))\n    Validate Computation\n      Convergence Diagnostics\n      Fake data simulation\n      Simulation based calibration\n    Evaluate and use model\n      Posterior predictive check\n      Cross validation\n      Influence of individual data points\n      Influence of prior\n      Prediction\n      Post stratification\n    Modify the model\n      Pick a new starting model\n      Replace model component\n      Enrich/Expand model\n      Use an approximation\n      Add more data\n      Modify priors\n    Compare models\n      Comparing inferences\n      Multiverse analysis\n      Model averaging/stacking\n    Addressing computational issues\n      Simplify the model\n      Implement model components separately\n      Run small number of iterations\n      Run on a subset of the data\n      Stacking individual chains\n      Check for multimodality\n      Reparametrize\n      Plot intermediate quantities\n      Add prior information\n      Add more data\n      Give up\n\n\n\n\nmindmap\n  root((Bayesian Workflow))\n    Validate Computation\n      Convergence Diagnostics\n      Fake data simulation\n      Simulation based calibration\n    Evaluate and use model\n      Posterior predictive check\n      Cross validation\n      Influence of individual data points\n      Influence of prior\n      Prediction\n      Post stratification\n    Modify the model\n      Pick a new starting model\n      Replace model component\n      Enrich/Expand model\n      Use an approximation\n      Add more data\n      Modify priors\n    Compare models\n      Comparing inferences\n      Multiverse analysis\n      Model averaging/stacking\n    Addressing computational issues\n      Simplify the model\n      Implement model components separately\n      Run small number of iterations\n      Run on a subset of the data\n      Stacking individual chains\n      Check for multimodality\n      Reparametrize\n      Plot intermediate quantities\n      Add prior information\n      Add more data\n      Give up\n\n\n\n\n\n\nHappy modelling.\n\n\n\n\nReferences\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. 2020. “Bayesian Workflow.” https://arxiv.org/abs/2011.01808."
  },
  {
    "objectID": "posts/statistics-train-predict-assume-statistical-dependence/index.html",
    "href": "posts/statistics-train-predict-assume-statistical-dependence/index.html",
    "title": "Training Predictive Models Does Not Assume Statistical Dependence",
    "section": "",
    "text": "It was suggested to me recently that to fit a predictive model supposes that the domain variables cause the image variables. This is not the case.\nPredictive models assume nothing about causality per se. You can fit predictive models that are quite successful in making predictions with observational data but perform quite poorly at counterfactual inference. Worse still, the converse doesn’t hold either. If a model is excellent predicting the outcome of an intervention, this does not mean the model will succeed at predicting observational data.\nLet us say we retreat to a position invoking a statistical assumption without causal inference: does training a predictive model assume that the variables are statistically dependent? Again, no. A relevant mathematical fact is that if \\(X\\) and \\(Y\\) are random variables and \\(f,g\\) are Borel-measurable functions that \\(f(X)\\) and \\(g(Y)\\) will also be independent. See these links for guidance in understanding this result:\n\nFunctions of independent Random Variables\nAre functions of independent variables also independent\n\nSo what happens if you train a model \\[Y = f(X; \\theta) + \\epsilon\\] when \\[\\{ X,Y,\\epsilon \\}\\] are independent? I encourage you to try it out. You’ll observe that the sample estimates of \\[\\mathbb{E}[Y \\vert X]\\] and \\[\\mathbb{E}[Y]\\] are equal up to sampling variation. Let’s see an example with coins (i.e. Bernoulli trials):\n\nimport numpy as np\n\nx = np.random.randint(0,2,size=10**4)\ny = np.random.randint(0,2,size=10**4)\n\nprint(np.mean(y[x == 0]), np.mean(y[x == 1]), np.mean(y))\n\n0.4960222752585521 0.5002011263073209 0.4981\n\n\nNot that this computational experiment of computing the conditional expectation didn’t assume anything about a dependence between \\(X\\) and \\(Y\\). And yet this conditional expectation is a choice of \\(Y = f(X; \\theta) + \\epsilon\\) where the parameter \\(\\theta\\) doesn’t come into the picture. Is that cheating that we didn’t even talk about some \\(\\theta\\) in the example? Not really. We could have assigned it anything we like by declaring it theta = 3 for example, but it wouldn’t participate in the computation. Go ahead with trying an example like \\(Y = X + \\beta\\) and you’ll find again that it doesn’t make much of difference, and the tiny bit of difference it makes will not generalize to future samples well. We were also being pretty charitable in assuming that \\(\\epsilon\\) was a degenerate random variable centered at zero because adding noise would just make any correspondence you would have hoped to find come out as even weaker.\nThe intuitive phrasing is that if \\(X\\) and \\(Y\\) are independent than any model that conditions \\(Y\\) on \\(X\\) will not have any effect in theory, and very little if any in practice. When you train a model on variables for which you do not know if they are independent you are not committing any kind of fallacy. The model will fit better or worse. If it consistently predicts test examples then there is likely some statistical dependence. If not, there could statistical independence or the form of statistical dependence is not one in which your choice of model captured very well.\nIn summary, training predictive models does not assume statistical dependence. It just won’t be a better model than taking the average of the predictor variable instead."
  },
  {
    "objectID": "posts/asyncio-hello-world/index.html",
    "href": "posts/asyncio-hello-world/index.html",
    "title": "Hello World, in Asyncio",
    "section": "",
    "text": "We can define a “Hello, World” type of example using Asyncio as follows:\nimport asyncio, time\n\nasync def main():\n    print('Hello,')\n    await asyncio.sleep(0.2018)\n    print('World!')\n\nasyncio.run(main())\nIf you try to run the above Python code directly in a Quarto code block, it will give an error:\nRuntimeError: asyncio.run() cannot be called from a running event loop\nThis error occurs because run expects that it will have control over the main process, but that is not the case if you are running a Jupyter notebook. That is the case if you are running Python in a Quarto block.\nSo similar to what I have done with other languages, we can define a script which we can import into a given blog post in order to call other processes. When we run our asynchronous “Hello, World” script we will want to also capture its standard output in our interactive session.\nWith Python this is pretty straightforward because we don’t need to consider compiling code ourselves, much less linking object files. The following is a good start:\nimport subprocess\n\ndef run_python_file(file_path):\n    try:\n        result = subprocess.run(\n            [\"python\", file_path],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        return result.stdout\n    except subprocess.CalledProcessError as e:\n        return f\"An error occurred while running the file: {e.stderr}\"\nPutting this code in ../../scripts/run_python.py, we can call it like this:\n\nimport sys\nsys.path.insert(1, '../../scripts')\nimport run_python\n\nprint(run_python.run_python_file('example.py'))\n\nHello,\nWorld!\n\n\n\nThis setup is fine for simple and isolated blogging examples. Anyway, that’s about as close as I think we’ll get to a “Hello, World” script with Asyncio. As with similar scripts that I have written for Rust and other languages, Quarto’s automatic freezing feature will only look for if the source of the QMD file has changed. Quarto will ignore any example Python files outside of that."
  },
  {
    "objectID": "posts/rust-simpy-car/index.html",
    "href": "posts/rust-simpy-car/index.html",
    "title": "A Rust Implementation of SimPy’s Simple Car DES",
    "section": "",
    "text": "This post gives an implementation of this example from the SimPy documentation:\nimport simpy\n\ndef car(env):\n    while True:\n        print('Start parking at %d' % env.now)\n        parking_duration = 5\n        yield env.timeout(parking_duration)\n\n        print('Start driving at %d' % env.now)\n        trip_duration = 2\n        yield env.timeout(trip_duration)\n\nenv = simpy.Environment()\nenv.process(car(env))\nenv.run(until=15)"
  },
  {
    "objectID": "posts/rust-simpy-car/index.html#introduction",
    "href": "posts/rust-simpy-car/index.html#introduction",
    "title": "A Rust Implementation of SimPy’s Simple Car DES",
    "section": "",
    "text": "This post gives an implementation of this example from the SimPy documentation:\nimport simpy\n\ndef car(env):\n    while True:\n        print('Start parking at %d' % env.now)\n        parking_duration = 5\n        yield env.timeout(parking_duration)\n\n        print('Start driving at %d' % env.now)\n        trip_duration = 2\n        yield env.timeout(trip_duration)\n\nenv = simpy.Environment()\nenv.process(car(env))\nenv.run(until=15)"
  },
  {
    "objectID": "posts/rust-simpy-car/index.html#rust-implementation",
    "href": "posts/rust-simpy-car/index.html#rust-implementation",
    "title": "A Rust Implementation of SimPy’s Simple Car DES",
    "section": "Rust Implementation",
    "text": "Rust Implementation\nI implemented the above car example in Rust.\n\nKey Concepts\nThe major difference is the introduction of enums for driving events and parking events rather than just having a single type of event. Let’s break down the key concepts anyway.\nFirst, there are events implemented using an Event struct which always have two types of data: time and event_type. The time datum is the time that the event is scheduled to elapse. The event_type essentially provides a flag for how to update the state of the simulated car.\nThe Environment struct manages the simulation, including the event schedule is is implemented as a reverse binary heap (BinaryHeap combined with Reverse).\nLastly, is the use of Rust’s enums. Here they are used to define the two types of events: Parking and Driving.\n\n\nImplementation\nNow that we’ve looked at the key concepts, let’s go through the code in more detail. First, we need to include some crates to help us with the implementation.\nFirst we need to import some crates.\n// car.rs\n\nuse std::cmp::Ordering;\nuse std::collections::BinaryHeap;\nuse std::cmp::Reverse;\nOrdering provides traits that we’ll use for implementing the ordering of events in the event schedule. BinaryHeap and Reverse are useful for implementing the event schedule itself.\nNext I defined different types of events. Maybe it would be more logical to define the existence of events first, but we’re just going in the order of my code event if that is a little weird. I defined different types of events as a EventType enumeration. I also used the derive attribute to attach traits related to debugging and comparison of events.\n#[derive(Debug, PartialEq, PartialOrd)]\nenum EventType {\n    Parking,\n    Driving,\n}\nDefining events themselves as structs works well here because they basically are value objects. They contain two pieces of data: time and event_type. The former is the time that an event is scheduled to occur. The latter is where each instance of Event will store its type.\n#[derive(Debug, PartialEq, PartialOrd)]\nstruct Event {\n    time: f64,\n    event_type: EventType\n}\nI also included implementations for the Eq and Ord, which involves using some predefined methods. We just need to declare them.\nimpl Eq for Event {}\n\nimpl Ord for Event {\n    fn cmp(&self, other: &Self) -&gt; Ordering {\n        self.partial_cmp(other).unwrap_or(Ordering::Equal)\n    }\n}\nThe environment struct is responsible for running the simulation, and has two main components: the event schedule and a clock. The event schedule, which I somewhat weirdly called a “queue” in the code, is actually a reverse binary heap.\nstruct Environment {\n    event_queue: BinaryHeap&lt;Reverse&lt;Event&gt;&gt;,\n    clock: f64,\n}\nNow just because the environment has an event schedule and a clock doesn’t mean it knows what to do with these things, so I provide some implementations so that the environment knows what to do. The new implementation simply instantiates a new (empty) event schedule and begins with a clock set to zero.\nThe schedule_event pushes an event (in reverse ordering in terms of how the data structre operates) to the event schedule. We’re using a predefined data structure so we don’t need to implement actually finding the right location to insert a new value.\nThe run_until implementation is where the magic happens. We continually loop until either a maximum time has been reached or the event schedule is empty, whichever comes first. The logic involves iteratively getting the next event, updating the clock, and then executing the event.\nThe now implementation is an afterthought that maybe if we extended this coded example then we might want to access the time, but it really doesn’t add anything substantial.\nimpl Environment {\n    fn new() -&gt; Self {\n        Self {\n            event_queue: BinaryHeap::new(),\n            clock: 0.0,\n        }\n    }\n\n    fn schedule_event(&mut self, event: Event) {\n        self.event_queue.push(Reverse(event));\n    }\n\n    fn run_until(&mut self, end_time: f64) {\n        while let Some(Reverse(current_event)) = self.event_queue.pop() {\n            if current_event.time &lt; end_time {\n                self.clock = current_event.time;\n                current_event.execute(self);\n            } else {\n                self.clock = end_time;\n                break;\n            }\n        }\n    }\n\n    fn now(&self) -&gt; f64 {\n        self.clock\n    }\n}\nI know this code is weirdly ordered. Next I defined some implementations for Event. Just like Environment, it doesn’t magically know what to do. We have to tell it.\nThe new implementation sets the time and event type.\nThe execute implementation checks which type of event is elapsing, and updates the state of the car depending on the type of event. Notice that the events themselves ask the environment to schedule further events, which is how the system continues to evolve.\nimpl Event {\n    fn new(time: f64, event_type: EventType) -&gt; Self {\n        Self { time, event_type }\n    }\n\n    fn execute(&self, env: &mut Environment) {\n        match self.event_type {\n            EventType::Parking =&gt; {\n                println!(\"Start parking at {}\", self.time);\n                let parking_duration = 5.0;\n                env.schedule_event(Event::new(self.time + parking_duration, EventType::Driving));\n            }\n            EventType::Driving =&gt; {\n                println!(\"Start driving at {}\", self.time);\n                let trip_duration = 2.0;\n                env.schedule_event(Event::new(self.time + trip_duration, EventType::Parking));\n            }\n        }\n    }\n}\nThen we just need to instantiate the environment, schedule the first event which will trigger the cycle of state transitions, and run for the desired amount of time.\nlet mut env = Environment::new();\n\n// Schedule the initial clock event\nenv.schedule_event(Event::new(0.0, EventType::Parking));\n\n// Run the simulation until max time\nenv.run_until(15.0);\n\n\nStart parking at 0\nStart driving at 5\nStart parking at 7\nStart driving at 12\nStart parking at 14\n\n\n\nI think this design is “okay”, but I want to keep exploring different designs for DES in Rust. I like that Environment does not need to know anything about the state of the system being simulated. I would like to weaken the coupling between state transitions and the types of events. Instead of providing impls perhaps all events could have an execute method which just calls a function which it is borrowing from the implementation of the system’s state. That way events could be coupled to state transitions weakly, but not responsible for defining what those state transitions are."
  },
  {
    "objectID": "posts/python-clie-log-trace/index.html",
    "href": "posts/python-clie-log-trace/index.html",
    "title": "A Python CLI Example to Log the Execution Trace",
    "section": "",
    "text": "This is just a short script implementing a logger of the trace of a Python program’s execution.\nimport datetime\nfrom typing import Optional, Callable, Any, Tuple\nimport sys\n\nimport click\n\ndef trace_function(frame: Any, event: str, arg: Any) -&gt; Optional[Callable]:\n    \"\"\"\n    Trace function for monitoring function calls.\n\n    Args:\n        frame (frame): The current frame being executed.\n        event (str): The event type triggering the trace function.\n        arg (Any): The argument associated with the event.\n\n    Returns:\n        Optional[Callable]: The trace function or None to stop tracing.\n    \"\"\"\n    if not hasattr(trace_function, 'log_initialized'):\n        # Initialize log file with column titles if not already done\n        with open('trace_log.txt', 'w') as log_file:\n            log_file.write(\"Timestamp | Event | Function | File | Line | Argument\\n\")\n        trace_function.log_initialized = True\n\n    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    file_name = frame.f_globals.get('__file__', 'unknown')\n    log_entry = f\"{current_time} | {event} | {frame.f_code.co_name} | {file_name} | {frame.f_lineno} | {arg}\\n\"\n    with open('trace_log.txt', 'a') as log_file:\n        log_file.write(log_entry)\n    return trace_function\n\n@click.command()\n@click.argument('target_script', type=click.Path(exists=True))\ndef trace(target_script: str) -&gt; None:\n    \"\"\"\n    Trace function to monitor the execution of a target script.\n\n    Args:\n        target_script (str): Path to the target script to be traced.\n\n    Returns:\n        None\n    \"\"\"\n    # Set the trace function\n    sys.settrace(trace_function)\n\n    # Run the target script\n    with open(target_script, 'r') as script_file:\n        exec(script_file.read(), {})\n\n    # Disable the trace function\n    sys.settrace(None)\n\nif __name__ == '__main__':\n    trace()\nUsing it is straightforward:\n$ Python trace_util.py you_python_script.py"
  },
  {
    "objectID": "posts/flask-http-methods/index.html",
    "href": "posts/flask-http-methods/index.html",
    "title": "HTTP Methods in Flask",
    "section": "",
    "text": "The hypertext transfer protocol (HTTP) is a way for a web application to respond to requests.\nThere are four HTTP methods that you’ll typically find in introductory material:\n\nGET\nPOST\nPUT\nDELETE\n\nIn this post we’ll provide a simple example. We will embed the following video\n\n\nwhen GET is used. If the method is POST, then the page will render \"POST\". Otheriwse the function implicitly returns None, which offhand I don’t think does much of anything.\nimport getpass\n\nfrom flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/automorphisms', methods=['GET', 'POST'])\ndef http_methods_example():\n    if request.method == 'GET':\n        return '&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/vY1UkCPSKH8?si=mRB5eM30UmTUVfhv\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen&gt;&lt;/iframe&gt;'\n    if request.method == 'POST':\n        return 'POST'\n    # We don't need to bother with PUT and DELETE\n\nif __name__ == '__main__':\n    app.run(debug=True)\nOther than taking the parameter methods in app.route, there isn’t much to this application. We’re showing a tag for an embedded video, which for us to render only involves returning a string of the tag."
  },
  {
    "objectID": "posts/fastapi-hello-world/index.html",
    "href": "posts/fastapi-hello-world/index.html",
    "title": "Hello World, in FastAPI",
    "section": "",
    "text": "This is a quick post showing how to create a “Hello, World” example in FastAPI.\nSimilar to the example *Hello World, In Flask, with FastAPI we instantiate an app which contains methods that decorate other functions which will be used to returns responses. Finally, I used uvicorn to actually run the application at a given host and port.\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"Hello, World!\"}\n\nif __name__ == '__main__':\n    import uvcorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\nThat’s all there is for this minimum working example."
  },
  {
    "objectID": "posts/hello-world-flask/index.html",
    "href": "posts/hello-world-flask/index.html",
    "title": "Hello World, In Flask",
    "section": "",
    "text": "Ths post shows a relatively minimal example of using Flask to develop of web application. This application will only run locally, but it is otherwise a bona fide web application.\nfrom flask import Flask # 1\n\napp = Flask(__name__) # 2\n\n@app.route('/') # 3\ndef hello_world(): # 4\n    return 'Hello, World' # 5\n\nif __name__ == '__main__': # 6\n    app.run(debug=True) # 7\n\nImport the Flask class which is the central data structure for defining a Flask application.\nWe pass in __name__, which is the name of the current module, which helps Flask determine the root path for the application. The Flask application knowing what is defined as the root path will allow the application to find resources such as templates and static files.\nThis decorator will bind a URL, which in this case is simply /, root, to a function. When a user visits this URL, the function will be executed.\nDeclare the function we want to have run when a user visits the given URL.\nReturn 'Hello, World'.\nConditional to run the application when the module script is run directly.\nRun the FLask web server. The debug=True parameter assignment enables a debugging mode which provides information about any error messages and automatic restarts. The automatic restarts may occur when changes in the source code of the application are detected.\n\nWhen you run the application you’ll find that it is locally hosted at some port (http://127.0.0.1/&lt;SOME_PORT&gt;)."
  },
  {
    "objectID": "posts/notes-differential-forms/index.html",
    "href": "posts/notes-differential-forms/index.html",
    "title": "Notes on Differential Forms",
    "section": "",
    "text": "Suppose \\(C \\subseteq \\mathbb{R}^2\\) is a curve and \\(p \\in C\\). The tangent space to \\(C\\) at \\(p\\), \\(T_pC\\) is the set of all vectors tangent to \\(C\\) at \\(p\\).\n\n\n\nLet us say we have a curve \\[y = f(x)\\], with a point \\[p = (a, f(a))\\]. A tangent vector at that point can be given by \\[\\vec v = \\langle 1, f^{\\prime}(a) \\rangle\\] where \\[\\langle \\cdot, \\cdot \\rangle\\] is an inner product. Then we can formulate the tangent space\n\\[T_pC = \\text{span} \\{ \\langle 1, f^{\\prime} \\rangle \\} = \\{ \\langle c, c f^{\\prime}(a) | c \\in \\mathbb{R} \\}.\\]\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure()\nax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\nax.spines[['top', 'right']].set_visible(False)\nax.set_xticks([])\nax.set_yticks([])\nax.set_ylim([-30, 15])\n# Plot of function\nx = np.linspace(-5, 5, num=100)\ny = x ** 3 - 10 * x\nax.plot(x, y, color='y')\n# Unit tangent vector\nax.plot([2,3], [-12]*2, '--', color='tab:blue')\nax.plot([3,3], [-11, -12], '--', color='tab:blue')\n# Tangent space\nax.plot(x, 2 * x - 16, color='m')\nax.text(3, -12, r'$T_pC$', color='m')\nax.scatter(2, -12, color='m')\nax.text(2, -14, r'$p$', color='m')\nax.annotate(r'slope$=f^{\\prime}(a)$', xy=(4, -8), xytext=(4, 0),\n             arrowprops=dict(facecolor='black', shrink=0.05), color='grey')\n# Pullbacks\nax.plot([2]*2, [-30,-12], '--', color='m', alpha=0.5)\nax.text(2, -30, r'$a$', color='m')\nax.plot([x.min(), 2], [-12]*2, '--', color='m', alpha=0.5)\nax.text(x.min(), -12, 'f(a)', color='m');\n\n\n\n\n\n\n\n\nHow do we differentiate between points on \\(C \\subseteq \\mathbb{R}^2\\) and vectors in \\(T_p C \\subseteq \\mathbb{R}^2\\)?\nWe can create a coordinate system on \\(C\\)\n\\[(x,y): C \\mapsto \\mathbb{R}^2\\]\n\\[(x,y)(p) = (x(p), y(p))\\]\nand a coordinate system on \\(T_p C\\):\n\\[\\langle dx, dy \\rangle : T_p C \\mapsto \\mathbb{R}^r\\]\n\\[\\langle dx, dy \\rangle = \\langle dx(v), dy(v) \\rangle\\]\nThus \\(x: C \\mapsto \\mathbb{R}\\) and \\(y: C \\mapsto \\mathbb{R}\\) are combined to get a coordinate vector in the ambient space.\nSimilarly, \\(dx: T_pC \\mapsto \\mathbb{R}\\) and \\(dy: T_pC \\mapsto \\mathbb{R}\\) are combined to get a coordinate vector in the tangent space.\n\n\n\nLet \\(y = x^2\\), then the coordinate vector is \\((x,y)(p) = (a, a^2) \\in C\\). Similarly \\(\\langle dx, dy \\rangle (v) = \\langle 1, 2a \\rangle \\in T_pC\\) where \\(v\\) is a tangent vector to the function.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig, ax = plt.subplots()\nax.spines[['top', 'right']].set_visible(False)\nax.set_xticks([])\nax.set_yticks([])\nax.set_ylim([-5, 15])\n# Plot of function\nx = np.linspace(-5, 5, num=100)\ny = x**2\nax.plot(x, y, color='y')\n# Unit tangent vector\nax.plot([2,2], [1,3], '--', color='tab:blue')\nax.plot([1,2], [1, 1], '--', color='tab:blue')\n# Tangent space\nax.plot(x, 2 * x - 1 , color='m')\nax.text(2, 1, r'$T_pC$', color='m')\nax.scatter(1, 1, color='m')\nax.text(1, 0, r'$p$', color='m')\n# Pullbacks\nax.plot([1]*2, [-5,1], '--', color='m', alpha=0.5)\nax.text(1, -5, r'$a$', color='m')\nax.plot([-5, 1], [1]*2, '--', color='m', alpha=0.5)\nax.text(-5, 1, 'f(a)', color='m')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nThe equation \\(\\langle dx, dy \\rangle (v) = \\langle 1, 2a \\rangle\\) implies familiar result that \\(dy = 2a \\cdot 1 = 2 a dx \\implies \\frac{dy}{dx} = 2a = f^{\\prime}(a).\\)\n\n\n\n\nNote that \\[\\mathbb{R}^2 = \\text{span} \\{ (1,0), (0,1) \\} = \\{ (x,y) | x,y \\in \\mathbb{R} \\}.\\]\nThat means that\n\\[T_p \\mathbb{R}^2 = \\text{span} \\{ \\langle 1, 0 \\rangle, \\langle 0, 1 \\rangle \\} = \\{ \\langle dx, dy \\rangle | dx, dy \\in \\mathbb{R} \\}.\\]\n\n\n\n\n\n\nTip\n\n\n\nIf you want to keep track of the tangent space at different points, you can denote \\(\\langle dx, dy \\rangle_p\\) for the tangent space at point \\(p\\).\n\n\n\n\n\nSuppose we have \\(\\langle 1, 2 \\rangle_{(1,1)}\\), we can think of the differentials as another set of axes as visualized below.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig, ax = plt.subplots()\nax.spines[['top', 'right']].set_visible(False)\nax.set_xticks([])\nax.set_yticks([])\nax.set_xlim([0,3])\nax.set_ylim([0,3])\nax.annotate(\"\", xy=(2,2), xytext=(1, 1),\n            arrowprops=dict(color='tab:blue'))\nplt.text(2, 2, r'$\\langle c,d \\rangle_{(a,b)}$', color='gray')\nax.annotate(\"\", xy=(1,2.5), xytext=(1, 0.9),\n            arrowprops=dict(color='m'))\nax.annotate(\"\", xy=(2.5,1), xytext=(0.9, 1),\n            arrowprops=dict(color='m'))\nax.text(2.5, 1, 'dx', color='grey')\nax.text(1, 2.5, 'dy', color='grey')\nax.text(2, 1, 'c', color='grey')\nax.text(1, 2, 'd', color='grey')\nax.text(2, 0, 'a', color='grey')\nax.text(0, 2, 'b', color='grey')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/notes-differential-forms/index.html#definition",
    "href": "posts/notes-differential-forms/index.html#definition",
    "title": "Notes on Differential Forms",
    "section": "",
    "text": "Suppose \\(C \\subseteq \\mathbb{R}^2\\) is a curve and \\(p \\in C\\). The tangent space to \\(C\\) at \\(p\\), \\(T_pC\\) is the set of all vectors tangent to \\(C\\) at \\(p\\)."
  },
  {
    "objectID": "posts/notes-differential-forms/index.html#example",
    "href": "posts/notes-differential-forms/index.html#example",
    "title": "Notes on Differential Forms",
    "section": "",
    "text": "Let us say we have a curve \\[y = f(x)\\], with a point \\[p = (a, f(a))\\]. A tangent vector at that point can be given by \\[\\vec v = \\langle 1, f^{\\prime}(a) \\rangle\\] where \\[\\langle \\cdot, \\cdot \\rangle\\] is an inner product. Then we can formulate the tangent space\n\\[T_pC = \\text{span} \\{ \\langle 1, f^{\\prime} \\rangle \\} = \\{ \\langle c, c f^{\\prime}(a) | c \\in \\mathbb{R} \\}.\\]\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure()\nax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\nax.spines[['top', 'right']].set_visible(False)\nax.set_xticks([])\nax.set_yticks([])\nax.set_ylim([-30, 15])\n# Plot of function\nx = np.linspace(-5, 5, num=100)\ny = x ** 3 - 10 * x\nax.plot(x, y, color='y')\n# Unit tangent vector\nax.plot([2,3], [-12]*2, '--', color='tab:blue')\nax.plot([3,3], [-11, -12], '--', color='tab:blue')\n# Tangent space\nax.plot(x, 2 * x - 16, color='m')\nax.text(3, -12, r'$T_pC$', color='m')\nax.scatter(2, -12, color='m')\nax.text(2, -14, r'$p$', color='m')\nax.annotate(r'slope$=f^{\\prime}(a)$', xy=(4, -8), xytext=(4, 0),\n             arrowprops=dict(facecolor='black', shrink=0.05), color='grey')\n# Pullbacks\nax.plot([2]*2, [-30,-12], '--', color='m', alpha=0.5)\nax.text(2, -30, r'$a$', color='m')\nax.plot([x.min(), 2], [-12]*2, '--', color='m', alpha=0.5)\nax.text(x.min(), -12, 'f(a)', color='m');\n\n\n\n\n\n\n\n\nHow do we differentiate between points on \\(C \\subseteq \\mathbb{R}^2\\) and vectors in \\(T_p C \\subseteq \\mathbb{R}^2\\)?\nWe can create a coordinate system on \\(C\\)\n\\[(x,y): C \\mapsto \\mathbb{R}^2\\]\n\\[(x,y)(p) = (x(p), y(p))\\]\nand a coordinate system on \\(T_p C\\):\n\\[\\langle dx, dy \\rangle : T_p C \\mapsto \\mathbb{R}^r\\]\n\\[\\langle dx, dy \\rangle = \\langle dx(v), dy(v) \\rangle\\]\nThus \\(x: C \\mapsto \\mathbb{R}\\) and \\(y: C \\mapsto \\mathbb{R}\\) are combined to get a coordinate vector in the ambient space.\nSimilarly, \\(dx: T_pC \\mapsto \\mathbb{R}\\) and \\(dy: T_pC \\mapsto \\mathbb{R}\\) are combined to get a coordinate vector in the tangent space."
  },
  {
    "objectID": "posts/notes-differential-forms/index.html#example-1",
    "href": "posts/notes-differential-forms/index.html#example-1",
    "title": "Notes on Differential Forms",
    "section": "",
    "text": "Let \\(y = x^2\\), then the coordinate vector is \\((x,y)(p) = (a, a^2) \\in C\\). Similarly \\(\\langle dx, dy \\rangle (v) = \\langle 1, 2a \\rangle \\in T_pC\\) where \\(v\\) is a tangent vector to the function.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig, ax = plt.subplots()\nax.spines[['top', 'right']].set_visible(False)\nax.set_xticks([])\nax.set_yticks([])\nax.set_ylim([-5, 15])\n# Plot of function\nx = np.linspace(-5, 5, num=100)\ny = x**2\nax.plot(x, y, color='y')\n# Unit tangent vector\nax.plot([2,2], [1,3], '--', color='tab:blue')\nax.plot([1,2], [1, 1], '--', color='tab:blue')\n# Tangent space\nax.plot(x, 2 * x - 1 , color='m')\nax.text(2, 1, r'$T_pC$', color='m')\nax.scatter(1, 1, color='m')\nax.text(1, 0, r'$p$', color='m')\n# Pullbacks\nax.plot([1]*2, [-5,1], '--', color='m', alpha=0.5)\nax.text(1, -5, r'$a$', color='m')\nax.plot([-5, 1], [1]*2, '--', color='m', alpha=0.5)\nax.text(-5, 1, 'f(a)', color='m')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nThe equation \\(\\langle dx, dy \\rangle (v) = \\langle 1, 2a \\rangle\\) implies familiar result that \\(dy = 2a \\cdot 1 = 2 a dx \\implies \\frac{dy}{dx} = 2a = f^{\\prime}(a).\\)"
  },
  {
    "objectID": "posts/notes-differential-forms/index.html#example-2",
    "href": "posts/notes-differential-forms/index.html#example-2",
    "title": "Notes on Differential Forms",
    "section": "",
    "text": "Note that \\[\\mathbb{R}^2 = \\text{span} \\{ (1,0), (0,1) \\} = \\{ (x,y) | x,y \\in \\mathbb{R} \\}.\\]\nThat means that\n\\[T_p \\mathbb{R}^2 = \\text{span} \\{ \\langle 1, 0 \\rangle, \\langle 0, 1 \\rangle \\} = \\{ \\langle dx, dy \\rangle | dx, dy \\in \\mathbb{R} \\}.\\]\n\n\n\n\n\n\nTip\n\n\n\nIf you want to keep track of the tangent space at different points, you can denote \\(\\langle dx, dy \\rangle_p\\) for the tangent space at point \\(p\\)."
  },
  {
    "objectID": "posts/notes-differential-forms/index.html#example-3",
    "href": "posts/notes-differential-forms/index.html#example-3",
    "title": "Notes on Differential Forms",
    "section": "",
    "text": "Suppose we have \\(\\langle 1, 2 \\rangle_{(1,1)}\\), we can think of the differentials as another set of axes as visualized below.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig, ax = plt.subplots()\nax.spines[['top', 'right']].set_visible(False)\nax.set_xticks([])\nax.set_yticks([])\nax.set_xlim([0,3])\nax.set_ylim([0,3])\nax.annotate(\"\", xy=(2,2), xytext=(1, 1),\n            arrowprops=dict(color='tab:blue'))\nplt.text(2, 2, r'$\\langle c,d \\rangle_{(a,b)}$', color='gray')\nax.annotate(\"\", xy=(1,2.5), xytext=(1, 0.9),\n            arrowprops=dict(color='m'))\nax.annotate(\"\", xy=(2.5,1), xytext=(0.9, 1),\n            arrowprops=dict(color='m'))\nax.text(2.5, 1, 'dx', color='grey')\nax.text(1, 2.5, 'dy', color='grey')\nax.text(2, 1, 'c', color='grey')\nax.text(1, 2, 'd', color='grey')\nax.text(2, 0, 'a', color='grey')\nax.text(0, 2, 'b', color='grey')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/notes-differential-forms/index.html#definition-1",
    "href": "posts/notes-differential-forms/index.html#definition-1",
    "title": "Notes on Differential Forms",
    "section": "Definition",
    "text": "Definition\nA 1-form is a linear function \\(\\omega : T_p \\mathbb{R}^n \\mapsto \\mathbb{R}\\)."
  },
  {
    "objectID": "posts/notes-differential-forms/index.html#proposition",
    "href": "posts/notes-differential-forms/index.html#proposition",
    "title": "Notes on Differential Forms",
    "section": "Proposition",
    "text": "Proposition\nFor a 1-form \\(\\omega : T_p \\mathbb{R}^n \\mapsto \\mathbb{R}\\) it holds that \\(\\omega \\in (T_p \\mathbb{R})^{*}\\) where \\((T_p \\mathbb{R})^{*}\\) is the dual space of the tangent space \\(T_p \\mathbb{R}\\)."
  },
  {
    "objectID": "posts/notes-differential-forms/index.html#example-4",
    "href": "posts/notes-differential-forms/index.html#example-4",
    "title": "Notes on Differential Forms",
    "section": "Example",
    "text": "Example\nGiven \\(\\mathbb{R}^2\\) and \\(T_p \\mathbb{R}^2\\) and \\(\\omega : T_p \\mathbb{R}^2 \\mapsto \\mathbb{R}\\) be linear:\n\\[\\implies \\omega (\\langle dx, dy \\rangle) = adx + bdx = \\langle a, b \\rangle \\cdot \\langle dx, dy \\rangle\\]\nwhich also equals\n\\[\\vert| \\langle a, b \\rangle \\vert| \\operatorname{scalar\\_projection}_{\\langle a,b \\rangle} \\langle dx, dy \\rangle .\\]\nThis leads to an intuition that a 1-form is a multiple of the scalar projection on to the same line."
  },
  {
    "objectID": "posts/notes-differential-forms/index.html#example-5",
    "href": "posts/notes-differential-forms/index.html#example-5",
    "title": "Notes on Differential Forms",
    "section": "Example",
    "text": "Example\nLet \\(\\omega : T_p \\mathbb{R}^n \\mapsto \\mathbb{R}\\) then\n\\[\\omega ( \\langle dx_1, \\ldots, dx_n \\rangle ) = \\sum_{i=1}^n a_i dx_i\\]"
  },
  {
    "objectID": "posts/notes-differential-forms/index.html#example-6",
    "href": "posts/notes-differential-forms/index.html#example-6",
    "title": "Notes on Differential Forms",
    "section": "Example",
    "text": "Example\nDefine \\(\\omega (\\langle dx, dy \\rangle) = 3dx + 2dy.\\) What line does \\(\\omega\\) project vectors onto?\n\nThe line is in the direction \\(\\langle 3, 2 \\rangle\\) b/c \\(\\omega ( \\langle dx, dy \\rangle ) = \\langle 3,2 \\rangle \\cdot \\langle dx, dy \\rangle\\)\n\nNotice that \\(\\langle 3,2 \\rangle\\) is parallel to \\(\\langle 1, \\frac{2}{3} \\rangle\\) which entails that \\(dy = \\frac{2}{3} x.\\)"
  },
  {
    "objectID": "posts/notes-differential-forms/index.html#example-7",
    "href": "posts/notes-differential-forms/index.html#example-7",
    "title": "Notes on Differential Forms",
    "section": "Example",
    "text": "Example\nSuppose \\(\\omega\\) scalar projects onto the line \\(dy = 2dx\\) with length of 3. Find \\(\\omega .\\)\nWe know that \\(\\omega (\\langle dx, dy \\rangle) = \\langle a,b \\rangle \\cdot \\langle dx, dy \\rangle .\\) We need \\(\\langle a, b \\rangle\\) to be parallel to \\(\\langle 1, 2 \\rangle\\) because \\(\\langle 1, 2 \\rangle\\) is the vector in the direction of the line \\(dy = 2dx\\).\nSo \\(\\langle a, b \\rangle = \\langle a, 2a \\rangle\\) which needs that have \\[\\vert| \\langle a, 2a \\rangle \\vert| = 3 \\] .\nSo \\[\\vert| \\langle a, 2a \\rangle \\vert| = \\sqrt{a^2 + 4 a^2} = 3 \\implies a \\sqrt{5} = 3 \\implies a = \\frac{3}{\\sqrt{5}}\\] . So \\(b=\\frac{6}{\\sqrt{5}}\\)\n\\[\\implies \\omega (\\langle dx, dy \\rangle) = \\frac{3}{\\sqrt{5}} dx + \\frac{6}{\\sqrt{5}} dy\\]"
  },
  {
    "objectID": "posts/notes-differential-forms/index.html#example-8",
    "href": "posts/notes-differential-forms/index.html#example-8",
    "title": "Notes on Differential Forms",
    "section": "Example",
    "text": "Example\nSupposing \\(n=2\\) and \\(\\omega_1 \\wedge \\omega_2 : T_p \\mathbb{R}^2 \\times T_p \\mathbb{R}^2 \\mapsto \\mathbb{R}\\) then what is \\(\\omega_1 \\wedge \\omega_2 (v_1, v_2)\\) where \\(v_1, v_2 \\in \\mathbb{R}^2\\) ?"
  },
  {
    "objectID": "posts/flask-basic-error-handling/index.html",
    "href": "posts/flask-basic-error-handling/index.html",
    "title": "Basic Error Handling In Flask",
    "section": "",
    "text": "Flask provides support for error handling, including defining custom errors. When an error occurs which is not otherwise defined, the default error code is 500 (internal server error).\nIn this post I will show a minimal example of using a predefined code for error handling in a Flask application.\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.errorhandler(404)\ndef page_not_found(error):\n    return 'This page does not exist (DNE)', 404\n\nif __name__ == '__main__':\n    app.run(debug=True)\nIn the above code we picked the classic 404 error, which is reserved for when the page cannot be reached. XKCD has an appropriate entry on this topic."
  },
  {
    "objectID": "posts/se-lib-fault-tree/index.html",
    "href": "posts/se-lib-fault-tree/index.html",
    "title": "The Statistical Assumptions of Quantitative Fault Trees in Se-Lib",
    "section": "",
    "text": "The se-lib (Systems Engineering Library) offers a variety of diagrams that simply take in tuples of strings and numbers. In this post I want to dicsuss their quantitative fault tree diagrams.\nSince I work in healthcare, I put together a healthcare-related example. The overall points of discussion are about statistics, however.\nimport selib as se\n\n# Healthcare computed fault tree given probabilities for basic events\nhealthcare_fault_tree = [\n    (\"Patient Adverse Event\", \"or\", '', [\"Medication Error\", \"Surgical Error\", \"Infection\"]),\n    ('Medication Error', 'and', '', ['Wrong Dosage', 'Wrong Medication']),\n    ('Surgical Error', 'and', '', [\"Equipment Failure\", \"Human Error\"]),\n    ('Infection', 'or', '', [\"Post-operative Infection\", \"Hospital Acquired Infection\"]),\n    ('Wrong Dosage', 'basic', .01),\n    ('Wrong Medication', 'basic', .005),\n    ('Equipment Failure', 'basic', .02),\n    ('Human Error', 'basic', .015),\n    ('Post-operative Infection', 'basic', .01),\n    ('Hospital Acquired Infection', 'basic', .03),\n]\n\ndot = se.draw_fault_tree_diagram_quantitative(healthcare_fault_tree, filename=\"healthcare_quantitative_fault_tree\", format=\"svg\")\n\n\n\n\n\n\nWarning\n\n\n\nWhile the above code will run and produced the figure below as an SVG, it is strangely fragmented into a combination of dot and HTML references to logic gate SVG files. I tried a variety of options including rendering with Graphviz, converting to PNG and pulling it into Jupyter, and other stuff. In all cases the gates were missing, which is annoying. Below I have simply referenced a screenshot of the SVG.\n\n\n\nOnce you get over the misalignment of the rendered logic gates, we can start to interpret the above diagram. Each rectangular node is an event from an event space. These nodes have inputs which first go through a logic gate. There are two types of logic gates in this diagram.\n\n\n\n\n\n\n\nLogic Gate\nSymbol\n\n\n\n\nAND\n\n\n\n(inclusive) OR\n\n\n\n\nFor the purposes of discussion, let us also assign labels to the events (which we will assume are sets), and also note their marginal probabilities.\n\n\n\nSymbol\nDescription\nMarginal Probability\n\n\n\n\n\\(A\\)\nPatient Adverse Event\n0.04035\n\n\n\\(M\\)\nMedication Error\n\\(5.0 \\times 10^{-5}\\)\n\n\n\\(S\\)\nSurgical Error\n\\(3.0 \\times 10^{-3}\\)\n\n\n\\(I\\)\nInfection\n\\(0.04\\)\n\n\n\\(D\\)\nWrong Dosage\n0.01\n\n\n\\(W\\)\nWrong Medication\n0.005\n\n\n\\(E\\)\nEquipment Failure\n0.02\n\n\n\\(H\\)\nHuman Error\n\\(0.015\\)\n\n\n\\(O\\)\nPost-Operative Infection\n0.01\n\n\n\\(Q\\)\nHospital-Aquired Infection\n0.03\n\n\n\nI want to now discuss how this table of probabilities relates to certain assumptions about probabilities."
  },
  {
    "objectID": "posts/se-lib-fault-tree/index.html#introduction",
    "href": "posts/se-lib-fault-tree/index.html#introduction",
    "title": "The Statistical Assumptions of Quantitative Fault Trees in Se-Lib",
    "section": "",
    "text": "The se-lib (Systems Engineering Library) offers a variety of diagrams that simply take in tuples of strings and numbers. In this post I want to dicsuss their quantitative fault tree diagrams.\nSince I work in healthcare, I put together a healthcare-related example. The overall points of discussion are about statistics, however.\nimport selib as se\n\n# Healthcare computed fault tree given probabilities for basic events\nhealthcare_fault_tree = [\n    (\"Patient Adverse Event\", \"or\", '', [\"Medication Error\", \"Surgical Error\", \"Infection\"]),\n    ('Medication Error', 'and', '', ['Wrong Dosage', 'Wrong Medication']),\n    ('Surgical Error', 'and', '', [\"Equipment Failure\", \"Human Error\"]),\n    ('Infection', 'or', '', [\"Post-operative Infection\", \"Hospital Acquired Infection\"]),\n    ('Wrong Dosage', 'basic', .01),\n    ('Wrong Medication', 'basic', .005),\n    ('Equipment Failure', 'basic', .02),\n    ('Human Error', 'basic', .015),\n    ('Post-operative Infection', 'basic', .01),\n    ('Hospital Acquired Infection', 'basic', .03),\n]\n\ndot = se.draw_fault_tree_diagram_quantitative(healthcare_fault_tree, filename=\"healthcare_quantitative_fault_tree\", format=\"svg\")\n\n\n\n\n\n\nWarning\n\n\n\nWhile the above code will run and produced the figure below as an SVG, it is strangely fragmented into a combination of dot and HTML references to logic gate SVG files. I tried a variety of options including rendering with Graphviz, converting to PNG and pulling it into Jupyter, and other stuff. In all cases the gates were missing, which is annoying. Below I have simply referenced a screenshot of the SVG.\n\n\n\nOnce you get over the misalignment of the rendered logic gates, we can start to interpret the above diagram. Each rectangular node is an event from an event space. These nodes have inputs which first go through a logic gate. There are two types of logic gates in this diagram.\n\n\n\n\n\n\n\nLogic Gate\nSymbol\n\n\n\n\nAND\n\n\n\n(inclusive) OR\n\n\n\n\nFor the purposes of discussion, let us also assign labels to the events (which we will assume are sets), and also note their marginal probabilities.\n\n\n\nSymbol\nDescription\nMarginal Probability\n\n\n\n\n\\(A\\)\nPatient Adverse Event\n0.04035\n\n\n\\(M\\)\nMedication Error\n\\(5.0 \\times 10^{-5}\\)\n\n\n\\(S\\)\nSurgical Error\n\\(3.0 \\times 10^{-3}\\)\n\n\n\\(I\\)\nInfection\n\\(0.04\\)\n\n\n\\(D\\)\nWrong Dosage\n0.01\n\n\n\\(W\\)\nWrong Medication\n0.005\n\n\n\\(E\\)\nEquipment Failure\n0.02\n\n\n\\(H\\)\nHuman Error\n\\(0.015\\)\n\n\n\\(O\\)\nPost-Operative Infection\n0.01\n\n\n\\(Q\\)\nHospital-Aquired Infection\n0.03\n\n\n\nI want to now discuss how this table of probabilities relates to certain assumptions about probabilities."
  },
  {
    "objectID": "posts/se-lib-fault-tree/index.html#statistical-independence",
    "href": "posts/se-lib-fault-tree/index.html#statistical-independence",
    "title": "The Statistical Assumptions of Quantitative Fault Trees in Se-Lib",
    "section": "Statistical Independence",
    "text": "Statistical Independence\nThe first assumption is statistical independence, which for a collection of events \\(\\{ E_i \\}_{i=1}^n\\) means that\n\\[\\Pr \\left[ \\bigcap_{i=1}^n E_i \\right] = \\prod_{i=1}^n \\Pr [E_i]\\]\nwhich is not an assumption we can always allow. Where it is used in the above quantitative fault tree pertains to the AND gates. There are two AND gates in the diagram.\nThe first AND gate (going from left-to-right) is the one which produces the probabiliy for a medication error. You’ll note that the propositions\n\\[M = D \\cap W\\]\nand\n\\[\\Pr [M] = \\Pr [D] \\Pr [W]\\]\nare assumed to hold in that the proposition\n\\[5 \\times 10^{-5} = 0.01 \\times 0.005.\\]\nThe second AND gate feeds into the surgical error node. The probability of a surgical error is similarly assumed to satisfy\n\\[S = E \\cap H\\]\nand\n\\[\\Pr [S] = \\Pr [E] \\Pr [H]\\]\ndue to observing that\n\\[3.0 \\times 10^{-3} = 0.02 \\times 0.015.\\]\nSometimes statistical independence is a great assumption, but sometimes it is an assumption that we definitely should not make. When we cannot make the assumption of statistical independence of events we should consider a different approach."
  },
  {
    "objectID": "posts/se-lib-fault-tree/index.html#almost-sure-pairwise-disjoint-events",
    "href": "posts/se-lib-fault-tree/index.html#almost-sure-pairwise-disjoint-events",
    "title": "The Statistical Assumptions of Quantitative Fault Trees in Se-Lib",
    "section": "Almost-Sure Pairwise Disjoint Events",
    "text": "Almost-Sure Pairwise Disjoint Events\nTwo events \\(E_1\\) and \\(E_2\\) are considered disjoint if-and-only-if\n\\[E_1 \\cap E_2 = \\emptyset\\]\nmeaning that they have no elements in common. It could be said in this case that the events are mutually exclusive.\nTwo events \\(E_1\\) and \\(E_2\\) are considered almost-surely disjoint if-and-only-if\n\\[\\mu \\left( E_1 \\cap E_2 \\right) = 0.\\]\nfor a choice of measure \\(\\mu\\). This might look very similar to the disjoint case that we just covered. However, they are not the same. See Seilis (2022) for some examples, noting that for our purposes we would take \\(\\mu\\) to be a probability measure.\nIf two sets of disjoint, then they are also almost-surely disjoint:\n\\[E_1 \\cap E_2 = \\emptyset \\implies \\mu \\left( E_1 \\cap E_2 \\right) = 0\\]\nSwitching to considering unions, related to the OR gates, it holds for two almost-surely disjoint events that\n\\[\\Pr [E_1 \\cup E_2] = \\Pr [E_1] + \\Pr[E_2].\\]\nYou can verify that this countable additivity is used at the OR gates in the example diagram. Further, this type of result will hold for any countable collection of events (i.e. not just for two events).\nJust like statistical independence, almost-sure disjointedness is not always an assumption we can make. It depends on the problem, which motivates us to want to find a more general mathematical tool."
  },
  {
    "objectID": "posts/se-lib-fault-tree/index.html#conclusions",
    "href": "posts/se-lib-fault-tree/index.html#conclusions",
    "title": "The Statistical Assumptions of Quantitative Fault Trees in Se-Lib",
    "section": "Conclusions",
    "text": "Conclusions\nse-lib’s quantitative fault trees assume statistical independence and almost-sure pairwise disjointedness for the collection of events being considered. In general these assumptions will not hold, but it depends on the problem. Other approaches such as Judea Pearl’s causal graphs combined with Bayesian networks (also developed by Pearl) could replace everything that I think is essential to these quantitative fault trees without loss of generality."
  },
  {
    "objectID": "posts/python-ciw-simpy-car/index.html",
    "href": "posts/python-ciw-simpy-car/index.html",
    "title": "A Ciw Implementation of SimPy’s Car Example",
    "section": "",
    "text": "In the SimPy documentation they describe a car process which involves a car which transitions between parking and driving. From being parked it takes 5 units of time to change to being in a state of driving. Similarly, it takes 2 units of time to go from being in a state of driving to a state of parkerd. Here is a state diagram representing this system:\n\n\n\n\n\nstateDiagram\n    [*] --&gt; Parked\n    Parked --&gt; Driving: 5\n    Driving --&gt; Parked: 2\n    Parked --&gt; [*]\n    Driving --&gt; [*]\n\n\n\n\n\n\nFor something this simple we don’t really need discrete event simulation software, but it can be helpful for the sake of illustration to do it anyway. Let’s use idiomatic Ciw to make an equivalent simulation.\nLet our queueing network have two nodes, each representing either of the two states “Parked” or “Driving”. The car will go between these two nodes. When the car arrives at a node that will repesent that it has begun the whatever process is involved, and when the car finishes service at that node we will interpret that to mean it is now changing state.\nSince there will be only one car in the whole network, we do not need to give any consideration to service discipline. If the car is on the queue, then serve it. Otherwise, don’t.\nWe do need to pay attention to arrival times however. We need the car to arrive at the first node at the start of the simulation and for no additional cars to arrive thereafter. This can be achieved with a sequential distribution available in Ciw:\n\nimport ciw\nciw.dists.Sequential([0, float('inf')])\n\nSequential([0, inf])\n\n\nThe class ciw.dists.Sequential takes an iterable (usually a list) representing the sequence of arrival times to be done. When it has gotten through the list it will start again at the start of the list. But here’s the catch: we have set the second time to \\(\\infty\\), which it will never reach. So there will be a single arrival at \\(t=0\\) and non thereafter. We need to specify that no arrivals occur at the second node, which we can do using None:\n\narrival_dists = [ciw.dists.Sequential([0, float('inf')]), None]\n\nFor service times we know that the car will start driving after 5 units of time, then be parked for 2 units of time, and repeat. These are constant random variables which can be specified in Ciw using ciw.dists.Deterministic:\n\nservice_dists = [ciw.dists.Deterministic(5), ciw.dists.Deterministic(2)]\n\nWe don’t need to, but let’s add some fluff so that we get a similar printout. To do that let’s write a wrapper distribution.\n\nfrom typing import NoReturn\n\nimport ciw\n\nclass PrintDistWrapper(ciw.dists.Distribution):\n    \"\"\"\n    A wrapper class for a probability distribution that adds a print statement\n    before sampling from the underlying distribution.\n\n    Parameters:\n    - dist (ciw.dists.Distribution): The underlying probability distribution to be wrapped.\n    - message (str): A custom message to be printed before sampling.\n\n    Note: This class inherits from ciw.dists.Distribution.\n\n    Example:\n    ```\n\n    underlying_dist = SomeDistributionClass(parameters)\n    wrapper_dist = PrintDistWrapper(underlying_dist, \"Sampling from the distribution:\")\n    sample_result = wrapper_dist.sample()\n    ```\n    \"\"\"\n\n    def __init__(self, dist: ciw.dists.Distribution, message: str) -&gt; NoReturn:\n        \"\"\"\n        Initialize the PrintDistWrapper instance.\n\n        Parameters:\n        - dist (ciw.dists.Distribution): The underlying probability distribution to be wrapped.\n        - message (str): A custom message to be printed before sampling.\n        \"\"\"\n        super().__init__()\n        self.dist = dist\n        self.message = message\n\n    def sample(self, t: float = None, ind: ciw.Individual = None) -&gt; float:\n        \"\"\"\n        Generate a sample from the underlying distribution, printing a custom message.\n\n        Parameters:\n        - t: Optional parameter (if applicable to the underlying distribution).\n        - ind: Optional parameter (if applicable to the underlying distribution).\n\n        Returns:\n        - The sampled value from the underlying distribution.\n\n        Note: This method overrides the sample method of ciw.dists.Distribution.\n        \"\"\"\n        print(self.message, t)\n        return self.dist.sample(t, ind)\n\nThe PrintDistWrapper class allows us to print whatever message we like when the distribution is sampled from. Now we can just wrap our service distributions like this:\n\nservice_dists = [\n  PrintDistWrapper(\n    ciw.dists.Deterministic(5),\n    'Start parking at'),\n  PrintDistWrapper(\n    ciw.dists.Deterministic(2),\n    'Start driving at')\n    ]\n\nWe only need one server at each node since there could only ever be one car either there, or not there, requiring service. So we can just set those number of servers in a list like this:\n\nnum_servers = [1, 1]\n\nSince we have two nodes, we need to specifying the routing behaviour. It is simple in this case, and stationary. Whenever the car is leaving node one, go to node to. Whenever the car is leaving node two, go to node one. We can specify this as the following routing matrix:\n\\[\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\\]\nIn Ciw we can take our routing matrix to be:\n\nR = [[0.0, 1.0], [1.0, 0.0]]\n\nWe can now put together our simulation object:\n\nnetwork = ciw.create_network(\n    arrival_distributions = arrival_dists,\n    service_distributions = service_dists,\n    number_of_servers = num_servers,\n    routing=R\n    )\n\nAnd put together out simulation object:\n\nsimulation = ciw.Simulation(network)\n\nAnd simulate (for 15 units of time)!\n\nsimulation.simulate_until_max_time(15)\n\nStart parking at 0\nStart driving at 5\nStart parking at 7\nStart driving at 12\nStart parking at 14\n\n\nHappy simulating! :)"
  },
  {
    "objectID": "posts/cpp-execution-for-quarto/index.html",
    "href": "posts/cpp-execution-for-quarto/index.html",
    "title": "Executable C++ Code in Quarto",
    "section": "",
    "text": "Deprecated Output\n\n\n\nI have turned off the Lua filter discussed in this post. It was running everytime I rendered/published the blog. I have copy-pasted the original output for reference.\n\n\nIn Executable Rust Code in Quarto I made a rough implementation of having Rust code compiled and its output rendered.\nWith some small adjustments we can do the same for other languages, including C++.\nHere is the Lua filter:\nlocal io = require(\"io\")\nlocal os = require(\"os\")\nlocal tempfile = require(\"os\").tmpname\nlocal log_file\n\n-- Function to initialize the log file\nlocal function init_log()\n  log_file = io.open(\"cpp_executor_debug.log\", \"w\")\nend\n\n-- Function to log messages to file and stderr\nlocal function log(...)\n  local args = {...}\n  for i = 1, #args do\n    args[i] = tostring(args[i])\n  end\n  local message = table.concat(args, \" \")\n  if log_file then\n    log_file:write(message .. \"\\n\")\n    log_file:flush()\n  end\n  io.stderr:write(message .. \"\\n\")\n  io.stderr:flush()\nend\n\n-- Helper function to execute C++ code and return the output\nlocal function execute_cpp_code(code)\n  local temp_file = tempfile() .. \".cpp\"\n  log(\"Temporary C++ file:\", temp_file)\n  local source_file, err = io.open(temp_file, \"w\")\n  if not source_file then\n    log(\"Failed to create source file:\", err)\n    error(\"Failed to create source file: \" .. err)\n  end\n\n  source_file:write(code)\n  source_file:close()\n\n  local temp_bin = tempfile()\n  log(\"Temporary binary file:\", temp_bin)\n\n  local compile_command = \"g++ \" .. temp_file .. \" -o \" .. temp_bin .. \" 2&gt;&1\"\n  log(\"Compile Command:\", compile_command)\n  local compile_pipe = io.popen(compile_command)\n  local compile_output = compile_pipe:read(\"*a\")\n  local compile_result = compile_pipe:close()\n\n  if compile_result ~= true then\n    os.remove(temp_file)\n    log(\"C++ compilation failed. Output:\", compile_output)\n    error(\"C++ compilation failed. Output: \" .. compile_output)\n  end\n\n  local exec_command = temp_bin .. \" 2&gt;&1\"\n  log(\"Exec Command:\", exec_command)\n  local exec_pipe = io.popen(exec_command)\n  local output = exec_pipe:read(\"*a\")\n  exec_pipe:close()\n\n  local ok, rm_err = pcall(function()\n    os.remove(temp_file)\n    os.remove(temp_bin)\n  end)\n  if not ok then\n    log(\"Failed to clean up temporary files:\", rm_err)\n    error(\"Failed to clean up temporary files: \" .. rm_err)\n  end\n\n  log(\"Output:\", output)\n  return output\nend\n\nlocal echo_global = true\n\nfunction Meta(meta)\n  if meta.echo ~= nil then\n    echo_global = pandoc.utils.stringify(meta.echo) == \"true\"\n  end\nend\n\n-- Lua filter function\nfunction CodeBlock(elem)\n  if not log_file then\n    init_log()\n  end\n\n  local is_cpp_code = elem.attr.classes:includes(\"{cpp}\")\n  if is_cpp_code then\n    log(\"Processing C++ code block\")\n    local output = execute_cpp_code(elem.text)\n    output = output:gsub(\"%s+$\", \"\")\n    local blocks = {}\n\n    if echo_global then\n      -- Render C++ code as a formatted block\n      table.insert(blocks, pandoc.CodeBlock(elem.text, {class=\"cpp\"}))\n    end\n\n    -- Always return the output\n    table.insert(blocks, pandoc.Para(pandoc.Str(output)))\n\n    return blocks\n  else\n    log(\"Skipping non-C++ code block\")\n  end\nend\n\n-- Ensure log file is closed properly at the end\nfunction Pandoc(doc)\n  if log_file then\n    log_file:close()\n  end\n  return doc\nend\nHere is a “Hello, World” example.\n#include &lt;iostream&gt;\n\nint main() {\n    std::cout &lt;&lt; \"Hello, World!\" &lt;&lt; std::endl;\n    return 0;\n}\nHello, World!\nSimilar to the lessons learned from the Rust implementation, there is plausibly a better implementation with an entirely different starting point."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Seilis, Galen. The Importance of Partial Pooling with Northern Health Data. Research and Knowledge Translation Newsletter\n\n\nSeilis, Galen. Project: Advanced Analytics. Research and Knowledge Translation Newsletter."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Running A Kedro Project With A HTTP Request Via FastAPI\n\n\n\n\n\n\nPython\n\n\nFastAPI\n\n\nKedro\n\n\nHTTP\n\n\nHTTP Requests\n\n\n\n\n\n\n\n\n\nAug 21, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nHello World, in FastAPI\n\n\n\n\n\n\nPython\n\n\nFastAPI\n\n\n\n\n\n\n\n\n\nAug 21, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nA State Diagram of a Simple Healthcare Service Using Mermaid\n\n\n\n\n\n\nMermaid\n\n\nHealthcare\n\n\nQueueing Systems\n\n\n\n\n\n\n\n\n\nAug 10, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nBuild and Run a Zig Project from Quarto Using Python\n\n\n\n\n\n\nZig\n\n\nQuarto\n\n\nPython\n\n\nsubprocess\n\n\n\n\n\n\n\n\n\nAug 9, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Greatest Common Divisor in Rust\n\n\n\n\n\n\nRust\n\n\nMathematics\n\n\nGreatest Common Divisor\n\n\n\n\n\n\n\n\n\nAug 9, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nA Rust Implementation of the Poisson Binomial Probability Distribution\n\n\n\n\n\n\nRust\n\n\nStatistics\n\n\nPoisson Binomial Distribution\n\n\nProbability Mass Function\n\n\nCumulative Distribution Function\n\n\n\n\n\n\n\n\n\nAug 8, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nevcxr_jupyter provides Rust execution in Quarto\n\n\n\n\n\n\nQuarto\n\n\nRust\n\n\nevcxr\n\n\nevcxr_jupyter\n\n\nJupyter\n\n\nJupyter Notebooks\n\n\n\n\n\n\n\n\n\nAug 8, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nRendering a Dot Representation of a Graph in Quarto Using Petgraph and Petgraph-evcxr\n\n\n\n\n\n\nRust\n\n\nPetgraph\n\n\nPetgraph-evcxr\n\n\nDot\n\n\n\n\n\n\n\n\n\nAug 8, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Naïvely Generate Autoregressive Time Series Data in Python\n\n\n\n\n\n\nPython\n\n\nStatistics\n\n\nStochastic Processes\n\n\nAutoregressive\n\n\nTime Series\n\n\nMatplotlib\n\n\nNumPy\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nBuild and Run a C Project from Quarto Using Python\n\n\n\n\n\n\nC\n\n\nQuarto\n\n\nPython\n\n\nsubprocess\n\n\n\n\n\n\n\n\n\nJul 30, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nThe Statistical Assumptions of Quantitative Fault Trees in Se-Lib\n\n\n\n\n\n\nPython\n\n\nSystems Engineering\n\n\nSe-Lib\n\n\nFault Tree\n\n\nProbability\n\n\nAssumptions\n\n\nStatistical Independence\n\n\nDisjoint Sets\n\n\nPairwise Disjoint Sets\n\n\nAlmost-Sure Pairwise Disjoint Sets\n\n\n\n\n\n\n\n\n\nJul 30, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nHello World, in Asyncio\n\n\n\n\n\n\nPython\n\n\nAsyncio\n\n\nsubprocess\n\n\nAsynchronous Programming\n\n\nConcurrent Programming\n\n\n\n\n\n\n\n\n\nJul 30, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nA Mermaid Diagram of Bayesian Workflow\n\n\n\n\n\n\nMermaid\n\n\nBayesian Workflow\n\n\nBayesian Statistics\n\n\nBayesian Inference\n\n\nBayesian Probability\n\n\nBayesian Modelling\n\n\nPrior Probability\n\n\nPosterior Probability\n\n\nModelling\n\n\nModel Development\n\n\n\n\n\n\n\n\n\nJul 30, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Error Handling In Flask\n\n\n\n\n\n\nPython\n\n\nFlask\n\n\nWeb Applications\n\n\nError Handling\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nHello World, In Flask\n\n\n\n\n\n\nPython\n\n\nFlask\n\n\nWeb Applications\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nHTTP Methods in Flask\n\n\n\n\n\n\nPython\n\n\nFlask\n\n\nWeb Applications\n\n\nURL\n\n\nHTTP\n\n\nHTTP Methods\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nHTML Templates in Flask\n\n\n\n\n\n\nPython\n\n\nFlask\n\n\nWeb Applications\n\n\nURL\n\n\nHTML Templates\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nFetching from a URL and Displaying an Image Using Flask\n\n\n\n\n\n\nPython\n\n\nFlask\n\n\nWeb Applications\n\n\nURL\n\n\nRequests\n\n\nImages\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic URLs in Flask\n\n\n\n\n\n\nPython\n\n\nFlask\n\n\nWeb Applications\n\n\nURL\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nCreating URL using url_for in Flask\n\n\n\n\n\n\nPython\n\n\nFlask\n\n\nWeb Applications\n\n\nURL\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nBuild and Run a Rust Project from Quarto Using Python\n\n\n\n\n\n\nRust\n\n\nQuarto\n\n\nPython\n\n\nCargo\n\n\nrustc\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nExecutable C++ Code in Quarto\n\n\n\n\n\n\nQuarto\n\n\nC++\n\n\nLua\n\n\nPandoc\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nExecutable C Code in Quarto\n\n\n\n\n\n\nQuarto\n\n\nC\n\n\nLua\n\n\nPandoc\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nExecutable Rust Code in Quarto\n\n\n\n\n\n\nQuarto\n\n\nRust\n\n\nLua\n\n\nPandoc\n\n\n\n\n\n\n\n\n\nJul 26, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Compile CPython on Debian-Based Linux\n\n\n\n\n\n\nPython\n\n\nCPython\n\n\nC\n\n\ncompilation\n\n\nmake\n\n\nDebian\n\n\nLinux\n\n\n\n\n\n\n\n\n\nJul 25, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nMy Quarto Blog\n\n\n\n\n\n\nblog\n\n\nQuarto\n\n\nJekyll\n\n\nMermaid\n\n\nGraphviz\n\n\ndot\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nCombining Kedro with Rye\n\n\n\n\n\n\nKedro\n\n\nRye\n\n\nPython\n\n\nProject Management\n\n\nPackage Management\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nThe Probability of the Minimum of X Being Greater than the Maximum of Y\n\n\n\n\n\n\nStatistics\n\n\nProbability Distributions\n\n\nDerivation\n\n\nChange of Variables\n\n\nTransformations\n\n\nStochastic Orders\n\n\nIdentical and Independently Distributed\n\n\nRandom Variables\n\n\nStatistical Independence\n\n\nSmooth Functions\n\n\nMinimum\n\n\nMaximum\n\n\nProbability\n\n\nOrder Statistics\n\n\nConvolution\n\n\nConvolution Theorem\n\n\nFourier Transform\n\n\n\n\n\n\n\n\n\nMar 2, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nA Python CLI Example to Log the Execution Trace\n\n\n\n\n\n\nComputer Programming\n\n\nPython\n\n\nTracing\n\n\nScripting\n\n\nCLI\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nA Ciw Implementation of SimPy’s Car Example\n\n\n\n\n\n\nPython\n\n\nSimulation\n\n\nQueueing System\n\n\nQueueing Network\n\n\nQueueing Theory\n\n\nRouting\n\n\nProcess-Based Simulation\n\n\nRandom Variables\n\n\nConstant Random Variables\n\n\nArrival Distributions\n\n\nService Distributions\n\n\nTiming\n\n\nInitialization\n\n\nDiscrete Event Simulation\n\n\nCiw\n\n\nSimPy\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nA Rust Implementation of SimPy’s Simple Car DES\n\n\n\n\n\n\nRust\n\n\nDiscrete Event Simulation\n\n\nPython\n\n\nSimPy\n\n\nSimulation\n\n\nSoftware Design\n\n\nPrototyping\n\n\nCPython\n\n\nPyPy\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a D/M/1 Queue in Ciw\n\n\n\n\n\n\nPython\n\n\nDiscrete Event Simulation\n\n\nCiw\n\n\nQueueing Network\n\n\nQueueing System\n\n\nQueueing Theory\n\n\nExponential Distribution\n\n\nStatistics\n\n\nOperations Research\n\n\nRandom Variables\n\n\nInter-Arrival Times\n\n\nService Times\n\n\nRandom Number Generation\n\n\nServers\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a G/G/1 Queue in Ciw\n\n\n\n\n\n\nPython\n\n\nDiscrete Event Simulation\n\n\nCiw\n\n\nSimulation\n\n\nQueueing Systems\n\n\nQueueing Theory\n\n\nHyperexponential Distribution\n\n\nGamma Distribution\n\n\nStatistics\n\n\nOperations Research\n\n\nRandom Variables\n\n\nInter-Arrival Times\n\n\nService Times\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nA Ciw Implementation of SimPy’s Clock Example\n\n\n\n\n\n\nPython\n\n\nDiscrete Event Simulation\n\n\nCiw\n\n\nSimPy\n\n\n\n\n\n\n\n\n\nDec 19, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nBandit On Bandit\n\n\n\n\n\n\nPython\n\n\nSecurity\n\n\nInformation Security\n\n\nBandit\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Predictive Models Does Not Assume Statistical Dependence\n\n\n\n\n\n\nStatistics\n\n\nStatistical Independence\n\n\nPredictive Modelling\n\n\nAssumptions\n\n\nConditional Expectation\n\n\nPython\n\n\nNumPy\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nCrayon 1\n\n\n\n\n\n\nArt\n\n\nCrayon\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nHow Expressive Are Simple Neural Networks?\n\n\n\n\n\n\nMachine Learning\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nAutomorphism Orbits of Graphlets\n\n\n\n\n\n\nMath\n\n\nGraph Theory\n\n\nGraphlets\n\n\nAutomorphisms\n\n\nAutomorphism Orbits\n\n\nAutomorphism Orbits of Graphlets\n\n\nIsomorphisms\n\n\nGraphs\n\n\nRelations\n\n\nBinary Relations\n\n\nSets\n\n\nVertices\n\n\nEdges\n\n\nSubsets\n\n\nCartesian Products\n\n\nNetworks\n\n\ndirected Graphs\n\n\nDigraphs\n\n\nSigned Graphs\n\n\nWeighted Graphs\n\n\nSimple Graphs\n\n\nFunctions\n\n\nInjective Functions\n\n\nSurjective Functions\n\n\nBijective Functions\n\n\nGraph Isomorphism\n\n\nGraph Automorphisms\n\n\nEquivalence Relations\n\n\nReflexive Relations\n\n\nSymmetric Relations\n\n\nTransitive Relations\n\n\nEquivalence Classes\n\n\nVertex Orbit Automorphisms\n\n\nPermutation Groups\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nPainting 7\n\n\n\n\n\n\nArt\n\n\nPainting\n\n\nWet-On-Wet Technique\n\n\nOil Painting\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nMixtures of Dirac Delta Distributions in PyMC\n\n\n\n\n\n\nStatistics\n\n\nDirac Delta Distribution\n\n\nPython\n\n\nPyMC\n\n\nArviz\n\n\nMatplotlib\n\n\nNumPy\n\n\nMixture Distributions\n\n\nPoisson Distribution\n\n\nDirichlet Distribution\n\n\nBayesian Inference\n\n\nBayesian Modelling\n\n\nBayesian Probability\n\n\nBayesian Statistics\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are Ranks?\n\n\n\n\n\n\nStatistics\n\n\nRanks\n\n\nRank-Based Statistics\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Differential Forms\n\n\n\n\n\n\nMathematics\n\n\nDifferential Forms\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Order Theory\n\n\n\n\n\n\nMathematics\n\n\nOrder Theory\n\n\nPartial Orders\n\n\nReflexive Relations\n\n\nTransitive Relations\n\n\nAntisymmetric Relations\n\n\nAsymmetric Relations\n\n\nCartesian Products\n\n\nBinary Relations\n\n\nRelations\n\n\nPython\n\n\nLaTeX\n\n\nDirected Acyclic Graphs\n\n\nStrict Partial Orders\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Plot a Basic Pie Chart Of Taxa At A Given Taxonomic Rank Using Pandas And Matplotlib\n\n\n\n\n\n\nPython\n\n\nVolunteer\n\n\niNaturalist\n\n\nData\n\n\nPlotting\n\n\nMatplotlib\n\n\nPandas\n\n\nCitizen Science\n\n\n\n\n\n\n\n\n\nMar 16, 2022\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nUnzip Your iNaturalist Observations On A Ubuntu System\n\n\n\n\n\n\nBASH\n\n\nVolunteer\n\n\niNaturalist\n\n\nData\n\n\nCitizen Science\n\n\nLinux\n\n\nUbuntu\n\n\n\n\n\n\n\n\n\nMar 16, 2022\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nQuantEcon 1 Numba\n\n\n\n\n\n\nPython\n\n\nScipy\n\n\nNumPy\n\n\ntimeit\n\n\noptimization\n\n\nNumba\n\n\n\n\n\n\n\n\n\nApr 9, 2019\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nQuantEcon 1 Scipy Submodules\n\n\n\n\n\n\nPython\n\n\nScipy\n\n\nNumPy\n\n\nscipy.optimize\n\n\nNewton Raphson method\n\n\nBrents method\n\n\ntimeit\n\n\nbisect method\n\n\noptimization\n\n\n\n\n\n\n\n\n\nMar 28, 2019\n\n\nGalen Seilis\n\n\n\n\n\n\n\n\n\n\n\n\nStarting Data Science\n\n\n\n\n\n\nData Science\n\n\n\n\n\n\n\n\n\nMar 23, 2019\n\n\nGalen Seilis\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/c-run-from-python/index.html",
    "href": "posts/c-run-from-python/index.html",
    "title": "Build and Run a C Project from Quarto Using Python",
    "section": "",
    "text": "In this post I share a way to compile and run a C file using Python, which in turn can be used to render the output of the C program in Quarto."
  },
  {
    "objectID": "posts/c-run-from-python/index.html#python-script",
    "href": "posts/c-run-from-python/index.html#python-script",
    "title": "Build and Run a C Project from Quarto Using Python",
    "section": "Python Script",
    "text": "Python Script\nThe Python script I developed is similar to the one I made for Rust, except that it does two phases. First gcc -c file.c is run on each file in a target path. Then gcc -o program main.o file1.o ... is run to put together the final program with appropriate linking.\nimport subprocess\nimport os\nimport glob\nimport stat\n\ndef log_permissions(path):\n    st = os.stat(path)\n    permissions = stat.filemode(st.st_mode)\n    print(f\"Permissions for {path}: {permissions}\")\n\ndef compile_and_run_c(project_dir):\n    project_dir = os.path.abspath(project_dir)\n\n    # Find all .c files in the project directory\n    c_files = glob.glob(os.path.join(project_dir, '*.c'))\n    if not c_files:\n        raise FileNotFoundError(\"No C source files found in the project directory.\")\n\n    object_files = []\n\n    # Compile each .c file into an object file\n    for c_file in c_files:\n        obj_file = os.path.splitext(c_file)[0] + '.o'\n        try:\n            compile_process = subprocess.run(\n                ['gcc', '-c', c_file, '-o', obj_file],\n                cwd=project_dir,\n                check=True,\n                capture_output=True,\n                text=True\n            )\n            object_files.append(obj_file)\n        except subprocess.CalledProcessError as e:\n            print(f\"Compilation Error for {c_file}: {e.stderr}\")\n            return\n\n    # Determine the name of the executable (assuming the file with main is called main.c)\n    exe_name = 'program'\n    main_file = os.path.join(project_dir, 'main.c')\n    if os.path.exists(main_file):\n        exe_name = os.path.splitext(os.path.basename(main_file))[0]\n\n    # Link all object files into a single executable\n    try:\n        link_process = subprocess.run(\n            ['gcc', '-o', exe_name] + object_files,\n            cwd=project_dir,\n            check=True,\n            capture_output=True,\n            text=True\n        )\n    except subprocess.CalledProcessError as e:\n        print(f\"Linking Error: {e.stderr}\")\n        return\n\n    # Find the compiled executable\n    target_exe = os.path.join(project_dir, exe_name)\n\n    if not os.path.exists(target_exe):\n        raise FileNotFoundError(\"Compiled executable not found.\")\n\n    # Set the executable permissions\n    try:\n        os.chmod(target_exe, stat.S_IRWXU | stat.S_IRGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)\n    except PermissionError as e:\n        print(f\"Error setting permissions: {e}\")\n        return\n\n    # Run the compiled executable and capture its output\n    try:\n        run_process = subprocess.run(\n            [target_exe],\n            check=True,\n            capture_output=True,\n            text=True\n        )\n        output = run_process.stdout\n        return output\n    except subprocess.CalledProcessError as e:\n        print(f\"Execution Error: {e.stderr}\")\n        return\n\n# Example usage\nif __name__ == \"__main__\":\n    output = compile_and_run_c('../posts/c-run-from-python/hello')\n    print(output)\nLet’s see it in action."
  },
  {
    "objectID": "posts/c-run-from-python/index.html#single-file-example",
    "href": "posts/c-run-from-python/index.html#single-file-example",
    "title": "Build and Run a C Project from Quarto Using Python",
    "section": "Single File Example",
    "text": "Single File Example\n\nimport sys\nsys.path.insert(1, '../../scripts')\n\nimport run_c\n\nprint(run_c.compile_and_run_c('./hello/'))\n\n1 Hello, world!\n2 Hello, world!\n3 Hello, world!\n4 Hello, world!\n5 Hello, world!\n6 Hello, world!\n7 Hello, world!\n8 Hello, world!\n9 Hello, world!\n10 Hello, world!\n11 Hello, world!\n12 Hello, world!\n\n\n\nIn the above example I am only compiling and running a single C file, but it is certainly possible to compile multiple files with a further change. For each .c file in the project path, I could run gcc -c file.c on each file. Then I could run gcc -o program main.o file1.o file2.o. This way I can link the object files together."
  },
  {
    "objectID": "posts/c-run-from-python/index.html#linked-files-example",
    "href": "posts/c-run-from-python/index.html#linked-files-example",
    "title": "Build and Run a C Project from Quarto Using Python",
    "section": "Linked Files Example",
    "text": "Linked Files Example\nIn this example I define a main.c source file, and a couple of helper&lt;#&gt;.c source files along with their header files.\n// main.c\n#include &lt;stdio.h&gt;\n#include \"helper1.h\"\n#include \"helper2.h\"\n\nint main() {\n    printf(\"Starting program...\\n\");\n    helper1();\n    helper2();\n    printf(\"Program finished.\\n\");\n    return 0;\n}\n// helper1.c\n#include &lt;stdio.h&gt;\n#include \"helper1.h\"\n\nvoid helper1() {\n    printf(\"Hello from helper1!\\n\");\n}\n// helper1.h\n#ifndef HELPER1_H\n#define HELPER1_H\n\nvoid helper1();\n\n#endif\n// helper2.c\n#include &lt;stdio.h&gt;\n#include \"helper2.h\"\n\nvoid helper2() {\n    printf(\"Hello from helper2!\\n\");\n}\n// helper2.h\n#ifndef HELPER2_H\n#define HELPER2_H\n\nvoid helper2();\n\n#endif\nNow we can similarly point run_c.compile_and_rune_c pointed at the path where these files exist.\n\nimport sys\nsys.path.insert(1, '../../scripts')\n\nimport run_c\n\nprint(run_c.compile_and_run_c('./hello2/'))\n\nStarting program...\nHello from helper1!\nHello from helper2!\nProgram finished."
  },
  {
    "objectID": "posts/c-run-from-python/index.html#conclusions",
    "href": "posts/c-run-from-python/index.html#conclusions",
    "title": "Build and Run a C Project from Quarto Using Python",
    "section": "Conclusions",
    "text": "Conclusions\nAs long as the linking and compiled options are kept simple, this script allows you to compile simple C langauge programs. This may be suitable for ensuring that C code examples for blogging actually work. Expanding into autotools and make files is the way to go for more complicated builds.\nThe same limitation as the corresponding script to run Rust code applies: if your qmd file does not change while Quarto’s setting is freeze: auto, the page will not be rerendered if the C code changes even if the Quarto document is not changed."
  },
  {
    "objectID": "posts/pie-chart-inaturalist/index.html",
    "href": "posts/pie-chart-inaturalist/index.html",
    "title": "How to Plot a Basic Pie Chart Of Taxa At A Given Taxonomic Rank Using Pandas And Matplotlib",
    "section": "",
    "text": "Note\n\n\n\nThis post was migrated from my iNaturalist journal to my Jekyll blog on 2023-02-26. It was then migrated to my Quarto blog on 2024-07-27.\n\n\nSuppose you have an exported CSV of iNaturalist observations, observations-&lt;ID&gt;.csv.\nTo follow this tutorial you will have to have Python installed, and the Matplotlib and Pandas packages installed. Pandas is not necessary, but it makes things convenient enough that I recommend using it here. In other contexts you may wish to plot pie charts without Pandas.\nAssuming you have PIP installed, you can install Pandas and Matplotlib as follows:\npip install matplotlib pandas\nAlthough, since Pandas actually uses Matplotlib as a dependency for plotting, it might suffice to simply use:\npip install pandas\nNext, you must create a script file with the *.py extension. We can do fancier things with paths, but let us create the file pie_taxa.py using BASH.\ntouch pie_taxa.py\nNow let us write some lines of code in pie_taxa.py. First we need to import the required packages.\nimport matplotlib.pyplot as plt\nimport pandas as pd\nNext we can load our data using the pd.read_csv function, which assumes a CSV format by default. It has many other parameters, including changing the delimiter (see the docs), but we are fine with the defaults here.\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(\"observations-&lt;ID&gt;.csv\")\nNow, as if by magic (but actually the hard work of software developers), we can create the plot in a single line of code. Let us do it for the kingdom level, which will require us knowing that this is represented by the taxon_kingdom_name column in our data file.\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(\"observations-&lt;ID&gt;.csv\")\ndf['taxon_kingdom_name'].value_counts().plot.pie()\nThere are a few things going on in the previous line of code. First is that df['taxon_kingdom_name'] has selected only the taxon_kingdom_name column. This is next passed to the value_counts() which counts the occurrences of each kingdom in that column and returns a Pandas series object with this information, and then we finally call the plot.pie method on this series object which… well… makes the pie chart.\nIf you run the code at this point you may be surprised to not actually see a plot appear anywhere. If you ran the code from the command line you might have seen something like &lt;AxesSubplot:ylabel='taxon_kingdom_name'&gt;. This is because creating the instructions of what goes on the drawing canvas is different from graphically rendering it. In order to do that, we can call plt.show().\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(\"observations-&lt;ID&gt;.csv\")\ndf['taxon_kingdom_name'].value_counts().plot.pie()\nplt.show()\nRunning the above, you should see a window pop up. It has various settings for resizing, reshaping, zooming, and saving your figure.\nWhat if you didn’t want to look at Kingoms, but rather orders, or families, etc? You would simply use a different column instead of taxon_kingdom_name. Here is a table of these similar columns:\n\n\n\nTaxonomic Rank\n\n\n\n\ntaxon_kingdom_name\n\n\ntaxon_phylum_name\n\n\ntaxon_subphylum_name\n\n\ntaxon_superclass_name\n\n\ntaxon_class_name\n\n\ntaxon_subclass_name\n\n\ntaxon_superorder_name\n\n\ntaxon_order_name\n\n\ntaxon_suborder_name\n\n\ntaxon_superfamily_name\n\n\ntaxon_family_name\n\n\ntaxon_subfamily_name\n\n\ntaxon_supertribe_name\n\n\ntaxon_tribe_name\n\n\ntaxon_subtribe_name\n\n\ntaxon_genus_name\n\n\ntaxon_genushybrid_name\n\n\ntaxon_species_name\n\n\ntaxon_hybrid_name\n\n\ntaxon_subspecies_name\n\n\ntaxon_variety_name\n\n\ntaxon_form_name\n\n\n\nHappy plotting."
  },
  {
    "objectID": "posts/fastapi-kedro-params/index.html",
    "href": "posts/fastapi-kedro-params/index.html",
    "title": "Running A Kedro Project With A HTTP Request Via FastAPI",
    "section": "",
    "text": "This post gives a minimal (but not production-ready) example of using FastAPI to run Kedro projects with parameters specified by HTTP requests. Although in this example I will have the web applications address set to home, in principle this lets you send remote instructions to run a Kedro project with modifiable behaviour."
  },
  {
    "objectID": "posts/fastapi-kedro-params/index.html#introduction",
    "href": "posts/fastapi-kedro-params/index.html#introduction",
    "title": "Running A Kedro Project With A HTTP Request Via FastAPI",
    "section": "",
    "text": "This post gives a minimal (but not production-ready) example of using FastAPI to run Kedro projects with parameters specified by HTTP requests. Although in this example I will have the web applications address set to home, in principle this lets you send remote instructions to run a Kedro project with modifiable behaviour."
  },
  {
    "objectID": "posts/fastapi-kedro-params/index.html#explanation",
    "href": "posts/fastapi-kedro-params/index.html#explanation",
    "title": "Running A Kedro Project With A HTTP Request Via FastAPI",
    "section": "Explanation",
    "text": "Explanation\nWe can define a data model which contains both the project name and the parameters we want to override in the Kedro project.\nclass KedroParams(BaseModel):\n    project_name: str\n        params: Dict[str, str]\nThe FastAPI application needs to know about a given Kedro project’s path in order to be able to run it. So in a file projects.yml I put the following:\nprojects:\n    spaceflights-pandas: spaceflights-pandas\nWe also need to expose a command to process incoming requests. That’s where run_kedro comes in, which will expose our run-kedro command to HTTP POST requests. It loads the projects configuration, and finds the relevant Kedro project path based on the project name provided in the kedroParam object obtained from the request. If the project name in the request doesn’t match the project name in the projects configuration file then an HTTP 404 status will be sent back. Lastly, we run the Kedro project and send back the logged output.\n@app.post(\"/run-kedro/\")\ndef run_kedro(params: KedroParams):\n    # Load the project configuration\n    projects = load_project_config()\n\n    # Get the project path from the config\n    project_path = projects.get(params.project_name)\n\n    if not project_path:\n        raise HTTPException(status_code=404, detail=\"Project not found.\")\n\n    # Run the Kedro command with the provided parameters\n    output = run_kedro_command(project_path, params.params)\n    return {\"output\": output}\nWe’ll also need to be able to load this project configuration. The project configuration comes from load_project_config which is defined as follows:\ndef load_project_config(config_file: str = \"projects.yaml\") -&gt; Dict[str, str]:\n    if not os.path.exists(config_file):\n        raise FileNotFoundError(\"Config file not found.\")\n    with open(config_file, 'r') as file:\n        config = yaml.safe_load(file)\n    return config.get(\"projects\", {})\nThe final piece is to define how run_kedro_command works. It takes the project path and the parameter values to be overloaded when the project runs. First it prepares the parameters from its dictionary data structure into a string which Kedro’s run command accepts. Then it prepares a list which can be used in a subprocess.call, which is attempted within a try-except just in case something goes wrong in running the project and we don’t want the application to crash. If the project successfully runs then we will grab the standard output to be sent back. Otherwise a server error response will be given.\ndef run_kedro_command(project_path: str, params: Dict[str, str]) -&gt; str:\n    # Convert params dictionary to a Kedro CLI formatted string\n    params_str = \",\".join([f\"{key}={value}\" for key, value in params.items()])\n    command = [\"kedro\", \"run\", f\"--params={params_str}\"]\n\n    try:\n        # Running the command\n        result = subprocess.run(command, cwd=project_path, capture_output=True, text=True, check=True)\n        return result.stdout\n    except subprocess.CalledProcessError as e:\n        raise HTTPException(status_code=500, detail=f\"Error executing Kedro command: {e.stderr}\")"
  },
  {
    "objectID": "posts/starting-data-science/index.html",
    "href": "posts/starting-data-science/index.html",
    "title": "Starting Data Science",
    "section": "",
    "text": "I’ve been thinking about Data Science lately, and I recently watched a YouTube video describing how to get started learning prerequisite knowledge for this field. While I am skeptical of the use of buzzwords, I think data science does reflect a loose collection of ideas and tools that are interesting and useful. Some of these subtopics include statistics, algorithms, databases, machine learning, and other miscellaneous topics within computer science and mathematics. I’m not partial to the term, but I am to the associated skillset.\nIn the interest of learning this subject in a structured way, I am going to go through the curricula suggested by Giles McMullen-Klein. I’ll modify it as it suites my needs or interests, but it serves as a simple template to get started with."
  },
  {
    "objectID": "posts/quantecon-2-numba/index.html",
    "href": "posts/quantecon-2-numba/index.html",
    "title": "QuantEcon 1 Numba",
    "section": "",
    "text": "Introduction\nNumba is a Python library that provides an open source just-in-time compiler that allows a coder to mark selected parts of their code to be compiled for faster execution. As someone interested in computation at any scale, from calculating \\(13 \\times 19\\) (mental arithmetic is not my forte) to analyzing the behaviour of tens of thousands of genes or hundreds of thousands of IP addresses. I am not one to squeeze every inch of performance out of something small that was only meant to run once as a proof of concept, but it can be worth speeding up tasks that are either huge or will be repeated.\nLet’s get into how to use Numba – hang on! Why not just use compiled languages like C, C++ or FORTRAN? Well, herein lies one of meta-problems of development that requires some optimization. Coding in Python is useful for quickly coding up proofs of concept, but properties like its dynamic typing slow it down compared to memory-managed code in C. Coding in C will give a faster execution for the same code, but will often require more time and degugging to get ready for deployment. Using Python with Numba is an attempt to get the best of both worlds, and in practice is not much slower than software compiled from well-written low-level languages.\n\n\nJIT\nThe first way we can use Numba to speed up our code is by compiling a function so that future executions can use the compiled version, removing the need to compile at runtime. Let’s take a function that gives us the first n Fibonnacci numbers, and see how it performs.\n\nimport numpy as np\nfrom timeit import timeit\n\ndef fib(n):\n    '''\n    Adjusted from:\n    https://lectures.quantecon.org/py/numba.html\n    https://en.wikipedia.org/wiki/Fibonacci_number\n    https://www.geeksforgeeks.org/program-for-nth-fibonacci-number/\n    '''\n\n    if n == 1:\n        return np.ones(1)\n    elif n &gt; 1:\n        x = np.empty(n)\n        x[0] = 1\n        x[1] = 1\n        for i in range(2,n):\n            x[i] =  x[i-1] + x[i-2]\n        return x\n    else:\n        print('WARNING: Check validity of input.')\n\nprint(timeit('fib(10)', globals={'fib':fib}))\n\n2.0311123809988203\n\n\nRunning the above code on my laptop gives around 2 seconds to run the function \\(1000000\\) times according to timeit, which is reasonable for small \\(n\\) but let’s see if we can speed this up with the Numba’s jit.\n\nfrom numba import jit\nimport numpy as np\nfrom timeit import timeit\n\ndef fib(n):\n    '''\n    ARGUMENTS:\n    n: Max index to calculate Fibonacci numbers up to (int)\n    RETURNS\n    x: Array of Fibonnacci numbers (numpy.ndarray)\n\n    NOTES:\n    Adjusted from:\n    https://lectures.quantecon.org/py/numba.html\n    https://en.wikipedia.org/wiki/Fibonacci_number\n    https://www.geeksforgeeks.org/program-for-nth-fibonacci-number/\n    '''\n\n    if n == 1:\n        return np.ones(1)\n    elif n &gt; 1:\n        x = np.empty(n)\n        x[0] = 1\n        x[1] = 1\n        for i in range(2,n):\n            x[i] =  x[i-1] + x[i-2]\n        return x\n    else:\n        print('WARNING: Check validity of input.')\n\nfib = jit(fib)\n\nprint(timeit('fib(10)', globals={'fib':fib}))\n\n0.6980576210007712\n\n\nRunning the above code with jit brought the execution time down to about 0.7 seconds, which is faster than the original function.\n\n\nVectorizing\nAnother approach to speeding up code is vectorizing, where multiple operations are applied to each entry directly instead of producing multiple intermediate arrays as operations are applied. Originally I had wanted to use our fib function, but I quickly learned that it is not vectorizable because it cannot be made into a universal function. For a function to be universal, it is necessary that they map scalars into scalers, and map arrays into arrays. With that in mind, we’ll vectorize a suitable function to show how the time performance is improved. Let’s start by timing the unvectorized function.\nimport numpy as np\nimport quantecon as qe\n\ndef f(x, y):\n    return np.exp(np.abs(x -y**3)) *  np.cos(x**2 + y**2)\n\n# generate data\ngrid = np.linspace(-3, 3, 1000)\nx, y = np.meshgrid(grid, grid)\n\nstart = qe.util.tic()\nf(x, y)\nend = qe.util.toc()\nOn my machine the execution time was about \\(0.05\\) seconds, which isn’t half-bad by itself. Not let’s run the same code in vectorized form.\nfrom numba import vectorize\nimport numpy as np\nimport quantecon as qe\n\n@vectorize\ndef f(x, y):\n    return np.exp(np.abs(x -y**3)) *  np.cos(x**2 + y**2)\n\n# generate data\ngrid = np.linspace(-3, 3, 1000)\nx, y = np.meshgrid(grid, grid)\n\nf(x,y) # precompile\n\nstart = qe.util.tic()\nf(x, y)\nend = qe.util.toc()\nThis vectorized form took about \\(0.0042\\) seconds to execute, which is about \\(12\\) times faster! This is a clear demonstration that vectorizing functions is worthwhile as scalability becomes an issue.\nBecause the vectorization of this function means that each element of the array is calculated independently, we can further attempt to speed this calculation up by calculating elements in parallel! We do that by telling the decorator the element types (we’ll use float64), and that the target is function should be done in parallel.\nfrom numba import vectorize\nimport numpy as np\nimport quantecon as qe\n\n@vectorize('float64(float64, float64)', target='parallel')\ndef f(x, y):\n    return np.exp(np.abs(x -y**3)) *  np.cos(x**2 + y**2)\n\n# generate data\ngrid = np.linspace(-3, 3, 1000)\nx, y = np.meshgrid(grid, grid)\n\nf(x,y) # precompile\n\nstart = qe.util.tic()\nf(x, y)\nend = qe.util.toc()\nThis last acceleration to make the calculations parallel squeezed the execution time down to \\(0.0031\\) seconds. This is only \\(0.0011\\) seconds faster than without the parallel execution, but still a worthwhile addition to the toolkit for doing independent calculations.\n\n\nConclusion\nUsing Numba allows us an easy way to increase the performance of functions in Python without going to a lower-abstraction language such as C or FORTRAN. Some functions will be more suitable to @jit than @vectorize based on the type of operations and whether the function is universal (or can be made into a universal function). These accelerations in performance becomes increasingly valuable as the amount of data being processed becomes large!"
  },
  {
    "objectID": "posts/math-distribution-independent-max-greater-than-min/index.html",
    "href": "posts/math-distribution-independent-max-greater-than-min/index.html",
    "title": "The Probability of the Minimum of X Being Greater than the Maximum of Y",
    "section": "",
    "text": "Suppose we have a collection of IID random variables \\(\\{ X_1, \\ldots, X_n \\}\\), and we also have a second collection of IID random variables \\(\\{ Y_1, \\ldots, Y_m \\}\\). Each \\(X_i \\sim F_X\\) and \\(Y_i \\sim F_Y\\) and we will assume that all these variables are statistically independent. Let us also assume that that \\(F_X\\) and \\(F_Y\\) are in the \\(\\mathcal{C}^1\\) smoothness class.\nSuppose we would like to find \\(Pr \\left[ \\min (X_1, \\ldots, X_n) &gt; \\max (Y_1, \\ldots, Y_n) \\right]\\), which is equal to \\(Pr \\left[ \\min (X_1, \\ldots, X_n) - \\max (Y_1, \\ldots, Y_n) &gt; 0 \\right]\\). The relevance of this observation is that \\(\\min (X_1, \\ldots, X_n) - \\max (Y_1, \\ldots, Y_n)\\) is an expression for which we can derive the distribution\nFor the minimum of the collection of \\(X\\) variables we can use order statistics to obtain:\n\\[F_{X_{(1)}}(x) = Pr \\left[ \\min \\{X_1, \\ldots, X_n \\} \\leq x \\right] =  1 - \\left[1 - F_X(x) \\right]^n.\\]\nLikewise, the maximum of the \\(Y\\) variables comes from order statistics:\n\\[\\max (Y_1, \\ldots, Y_m) \\sim \\left[ F_Y \\right]^m\\]\nWe would like to put our problem into the form of adding two independent random variables \\(U + V\\) because then we can convolve them to obtain the distribution of the sum. Taking \\(U = X_{(1)}\\) as our minimum of the \\(X\\) variables, and \\(V = - Y_{(m)}\\) of the \\(Y\\) variables, we can next consider the distribution of \\(V\\) to be a reflection of \\(Y_{(m)}\\). The smooth change in variables works out to be\n\\[f_V(v) = m f_Y(-y) \\left[ F_Y(-y)\\right]^{m-1}.\\]\nTo compute the convolution of the densities \\(f_U \\star f_V\\) we need the density \\(f_U\\):\n\\[f_U(u) = \\frac{d}{dx} F_X(x) = n \\left[ 1 - F_X(x) \\right]^{n-1}f_X(x)\\]\nWe can use the convolution theorem to obtain the result via the Fourier transform \\(\\mathcal{F}\\) and its inverse \\(\\mathcal{F}^{-1}\\).\n\\[f_{X_{(1)} - Y_{(m)}} = \\mathcal{F}^{-1} \\left\\{ \\mathcal{F} \\left\\{ n \\left[ 1 - F_X(x) \\right]^{n-1} f_X(x) \\right\\} \\mathcal{F} \\left\\{ m f_Y(-y) \\left[ F_Y(-y) \\right]^{m-1} \\right\\}  \\right\\}\\]\nFinally, we can obtain the cumulative distribution by integrating:\n\\[F_{X_{(1)} - Y_{(m)}}(x,y) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f_{X_{(1)} - Y_{(m)}}(x,y) dx dy\\]"
  },
  {
    "objectID": "posts/c-execution-for-quarto/index.html",
    "href": "posts/c-execution-for-quarto/index.html",
    "title": "Executable C Code in Quarto",
    "section": "",
    "text": "Deprecated Output\n\n\n\nI have turned off the Lua filter discussed in this post. It was running everytime I rendered/published the blog. I have copy-pasted the original output for reference.\n\n\nIn Executable Rust Code in Quarto I made a rough implementation of having Rust code compiled and its output rendered.\nWith some small adjustments we can do the same for other languages, including C.\nHere is the Lua filter:\nlocal io = require(\"io\")\nlocal os = require(\"os\")\nlocal tempfile = require(\"os\").tmpname\nlocal log_file\n\n-- Function to initialize the log file\nlocal function init_log()\n  log_file = io.open(\"c_executor_debug.log\", \"w\")\nend\n\n-- Function to log messages to file and stderr\nlocal function log(...)\n  local args = {...}\n  for i = 1, #args do\n    args[i] = tostring(args[i])\n  end\n  local message = table.concat(args, \" \")\n  if log_file then\n    log_file:write(message .. \"\\n\")\n    log_file:flush()\n  end\n  io.stderr:write(message .. \"\\n\")\n  io.stderr:flush()\nend\n\n-- Helper function to execute C code and return the output\nlocal function execute_c_code(code)\n  local temp_file = tempfile() .. \".c\"\n  log(\"Temporary C file:\", temp_file)\n  local source_file, err = io.open(temp_file, \"w\")\n  if not source_file then\n    log(\"Failed to create source file:\", err)\n    error(\"Failed to create source file: \" .. err)\n  end\n\n  source_file:write(code)\n  source_file:close()\n\n  local temp_bin = tempfile()\n  log(\"Temporary binary file:\", temp_bin)\n\n  local compile_command = \"gcc \" .. temp_file .. \" -o \" .. temp_bin .. \" 2&gt;&1\"\n  log(\"Compile Command:\", compile_command)\n  local compile_pipe = io.popen(compile_command)\n  local compile_output = compile_pipe:read(\"*a\")\n  local compile_result = compile_pipe:close()\n\n  if compile_result ~= true then\n    os.remove(temp_file)\n    log(\"C compilation failed. Output:\", compile_output)\n    error(\"C compilation failed. Output: \" .. compile_output)\n  end\n\n  local exec_command = temp_bin .. \" 2&gt;&1\"\n  log(\"Exec Command:\", exec_command)\n  local exec_pipe = io.popen(exec_command)\n  local output = exec_pipe:read(\"*a\")\n  exec_pipe:close()\n\n  local ok, rm_err = pcall(function()\n    os.remove(temp_file)\n    os.remove(temp_bin)\n  end)\n  if not ok then\n    log(\"Failed to clean up temporary files:\", rm_err)\n    error(\"Failed to clean up temporary files: \" .. rm_err)\n  end\n\n  log(\"Output:\", output)\n  return output\nend\n\nlocal echo_global = true\n\nfunction Meta(meta)\n  if meta.echo ~= nil then\n    echo_global = pandoc.utils.stringify(meta.echo) == \"true\"\n  end\nend\n\n-- Lua filter function\nfunction CodeBlock(elem)\n  if not log_file then\n    init_log()\n  end\n\n  local is_c_code = elem.attr.classes:includes(\"{c}\")\n  if is_c_code then\n    log(\"Processing C code block\")\n    local output = execute_c_code(elem.text)\n    output = output:gsub(\"%s+$\", \"\")\n    local blocks = {}\n\n    if echo_global then\n      -- Render C code as a formatted block\n      table.insert(blocks, pandoc.CodeBlock(elem.text, {class=\"c\"}))\n    end\n\n    -- Always return the output\n    table.insert(blocks, pandoc.Para(pandoc.Str(output)))\n\n    return blocks\n  else\n    log(\"Skipping non-C code block\")\n  end\nend\n\n-- Ensure log file is closed properly at the end\nfunction Pandoc(doc)\n  if log_file then\n    log_file:close()\n  end\n  return doc\nend\nHere is a “Hello, World” example.\n#include &lt;stdio.h&gt;\n\nint main() {\n    printf(\"Hello, World!\\n\");\n    return 0;\n}\nHello, World!\nSimilar to the lessons learned from the Rust implementation, there is plausibly a better implementation with an entirely different starting point."
  },
  {
    "objectID": "posts/mermaid-healthcare-service-diagram/index.html",
    "href": "posts/mermaid-healthcare-service-diagram/index.html",
    "title": "A State Diagram of a Simple Healthcare Service Using Mermaid",
    "section": "",
    "text": "Here is a simple diagram representing a healthcare service. This diagram can be used for communicating with clients about the basic structure of queueing systems.\n\nstateDiagram\n    direction LR\n    [*] --&gt; 🚑Arrivals\n    🚑Arrivals --&gt; ⏳Queue\n    state ⏳Queue {\n      direction LR\n      🤒 --&gt; 😷\n      😷 --&gt; 🤕\n    }\n\n    state 🏥Servers {\n      direction LR\n      👨🏾‍⚕️🧠\n      👩🏾‍⚕️🩺\n    }\n\n    ⏳Queue --&gt; ⚕️Service_Discipline\n    ⚕️Service_Discipline --&gt; 🏥Servers\n    🏥Servers --&gt; 🚘Departures\n    🚘Departures --&gt; [*]\n\n\n\n\nstateDiagram\n    direction LR\n    [*] --&gt; 🚑Arrivals\n    🚑Arrivals --&gt; ⏳Queue\n    state ⏳Queue {\n      direction LR\n      🤒 --&gt; 😷\n      😷 --&gt; 🤕\n    }\n\n    state 🏥Servers {\n      direction LR\n      👨🏾‍⚕️🧠\n      👩🏾‍⚕️🩺\n    }\n\n    ⏳Queue --&gt; ⚕️Service_Discipline\n    ⚕️Service_Discipline --&gt; 🏥Servers\n    🏥Servers --&gt; 🚘Departures\n    🚘Departures --&gt; [*]\n\n\n\n\n\n\nFeel free to adapt this to your own slides and projects."
  },
  {
    "objectID": "posts/flask-html-templates/index.html",
    "href": "posts/flask-html-templates/index.html",
    "title": "HTML Templates in Flask",
    "section": "",
    "text": "In this post we will not only have dynamic URLs, but start loading HTML templates. In the last post I wrote on Flask I used the factorial function. In this post we’ll take a number and GET/POST different templates depending on whether the input number is a prime number.\nTo check that a number is prime we can first check whether the candidate is less than 2. For natural numbers the number 2 is the smallest prime.\n\n\n\n\n\n\nNote\n\n\n\nIn the complex numbers, \\(\\mathbb{C}\\), the number 2 is not a “Gaussian prime”. It can be factored:\n\\[2 = (1 + i) (1 - i)\\]\n\n\nThe next criterion is that we only need to check whether there exists a natural number \\(k\\) such that \\(k^2 &lt; n\\) and \\(k | n\\). If such a number \\(k\\) does not exist, then \\(n\\) is a prime number.\nIn this example I prepared two templates:\n\ntemplates/example_is_prime.html\ntemplates/example_is_not_prime.html\n\nThe directory templates is in the same directory that the Flask application will be run. In example_is_prime.html I put\n&lt;h1&gt;The number {{ number }} is a prime!&lt;/h1&gt;\nand in example_is_not_prime.html I put\n&lt;h1&gt;The number {{ number }} is not a prime!&lt;/h1&gt;\nNote that neither of these files have the usual preamble that you’d find in an index.html file. That’s taken care of behind the scenes. You can also see { number } which specifies a variable name that is expected to come from render_template.\nfrom flask import Flask, render_template\n\ndef is_prime(n):\n    if n &lt; 2:\n        return False\n    i = 2\n    while i*i &lt;= n:\n        if n % i == 0:\n            return False\n        i += 1\n    return True\n\n\napp = Flask(__name__)\n\n@app.route('/primality/&lt;int:number&gt;')\ndef display_primarily(number):\n    if is_prime(number):\n        return render_template('example_is_prime.html', number=number)\n    else:\n        return render_template('example_is_not_prime.html', number=number)\n\nif __name__ == '__main__':\n    app.run(debug=True)\nWhen you run this application you’ll be able to enter various numbers in, and what will be displayed is a sentence telling you whether the number you provided in the URL is a prime number of note."
  },
  {
    "objectID": "posts/flask-display-image/index.html",
    "href": "posts/flask-display-image/index.html",
    "title": "Fetching from a URL and Displaying an Image Using Flask",
    "section": "",
    "text": "In this post I am going to show a Flask application which takes an integer and searches the Magic the Gathering database for an image of a corresponding card whose “multiverse ID” is the same as the given number.\nI used the requests package to get the image content. Supposing you get the content back, which will result in a response status code of 200, the image will still need to be converted into a memory binary stream. That’s where io.BytesIO comes in handy. Once the stream is prepared, the flask.send_file function can be used to prepare the rendered page content containing the file.\nfrom flask import Flask, send_file\nimport requests\nfrom io import BytesIO\n\napp = Flask(__name__)\n\n@app.route('/card/&lt;int:card_id&gt;')\ndef display_image(card_id):\n    # The URL of the image you want to display\n    image_url = f'https://gatherer.wizards.com/Handlers/Image.ashx?multiverseid={card_id}&type=card'\n\n    # Fetch the image from the URL\n    response = requests.get(image_url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Create an in-memory binary stream of the image\n        image_stream = BytesIO(response.content)\n\n        # Send the image as a response\n        return send_file(image_stream, mimetype='image/png')\n    else:\n        # Handle the error case\n        return \"Failed to retrieve image\", response.status_code\n\nif __name__ == '__main__':\n    app.run(debug=True)\nThis app should allow you to input different numbers into the URL which then will render an Magic the Gathering card image, or will give an error."
  },
  {
    "objectID": "posts/crayon-drawing-1/index.html",
    "href": "posts/crayon-drawing-1/index.html",
    "title": "Crayon 1",
    "section": "",
    "text": "I tried googling “what is a picture made with crayon called” and I consistently got suggestions for crayon etching, which is a more specific usage of crayons. I asked ChatGPT the following question\n\nWhat is a picture called when it is made with crayon? I don’t mean a crayon etching, but just when crayons are used to make a picture.\n\nChatGPT responded by labelling the chat session “Crayon Drawing” and responded:\n\nA picture made with crayons is commonly referred to as a crayon drawing or a crayon artwork. It is a type of drawing or art made using wax crayons, which are sticks of colored wax that can be used to create various colors and textures on paper or other surfaces. Other terms that may be used to describe a picture made with crayons include crayon painting or crayon illustration, depending on the style and technique used.\n\nThat’s good enough for me, so I’ve named the title of this post “Crayon Drawing 1”. Maybe at some point I’ll stop numbering my pictures and start naming them.\nAnyway, before trying painting I picked up some crayons and tried to make a picture. Naturally, whenever I pick up a complex skill and “just try it” I make a ton of mistakes. But strangely I rather like this picture. It along with my other attempts at art represents my courage in accepting imperfection and lack of expertise when I creatively explore."
  },
  {
    "objectID": "posts/python-bandit-on-bandit/index.html",
    "href": "posts/python-bandit-on-bandit/index.html",
    "title": "Bandit On Bandit",
    "section": "",
    "text": "Bandit is a Python package for checking for security issues in Python code. I have been thinking about using Bandit to check for security risks in my own sofware as well as 3rd party packages that I use. I couldn’t help but venture to the potentially ironic situation of Bandit evaluating that Bandit itself is insecure.\nWe can find the version of Bandit we are using like this:\n\nimport bandit\nprint(bandit.__version__)\n\n1.7.9\n\n\nThe path that the __init__.py file for Bandit is located on your system can be found in bandit.__file__. We can call Bandit from the command line on itself.\n\nfrom pathlib import Path\nimport subprocess\n\ninit_path = Path(bandit.__file__).parent\n\nprint(subprocess.run(['bandit', '-r', init_path], capture_output=True, text=True).stdout)\n\n\nWorking... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0% -:--:--\nWorking... ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  14% -:--:--\nWorking... ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  14% -:--:--\nWorking... ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━  29% 0:00:01\nWorking... ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━  48% 0:00:01\nWorking... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00\nWorking... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00\nWorking... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00\nRun started:2024-08-11 05:16:57.106814\n\nTest results:\n    No issues identified.\n\nCode scanned:\n    Total lines of code: 8340\n    Total lines skipped (#nosec): 0\n    Total potential issues skipped due to specifically being disabled (e.g., #nosec BXXX): 10\n\nRun metrics:\n    Total issues (by severity):\n        Undefined: 0\n        Low: 0\n        Medium: 0\n        High: 0\n    Total issues (by confidence):\n        Undefined: 0\n        Low: 0\n        Medium: 0\n        High: 0\nFiles skipped (0):\n\n\n\nAt a quick glance you might misread the above output as saying that there are no issues. There actually are, but the authors of Bandit have disabled them using the #nosec comment. This is indicated by the Total potential issues skipped due to specifically being disabled (e.g., #nosec BXXX): 10. Why the output also says Total lines skipped (#nosec): 0 I am not sure.\nWe can find the locations of these lines.\n\nprint(subprocess.run(['grep', '-rin', '\\\\#\\\\snosec',  init_path], capture_output=True, text=True).stdout)\n\n/home/galen/.rye/py/cpython@3.12.3/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_tmp.py:62:        return {\"tmp_dirs\": [\"/tmp\", \"/var/tmp\", \"/dev/shm\"]}  # nosec: B108\n/home/galen/.rye/py/cpython@3.12.3/lib/python3.12/site-packages/bandit/plugins/general_hardcoded_tmp.py:72:        tmp_dirs = [\"/tmp\", \"/var/tmp\", \"/dev/shm\"]  # nosec: B108\n/home/galen/.rye/py/cpython@3.12.3/lib/python3.12/site-packages/bandit/plugins/general_bind_all_interfaces.py:46:    if context.string_val == \"0.0.0.0\":  # nosec: B104\n/home/galen/.rye/py/cpython@3.12.3/lib/python3.12/site-packages/bandit/cli/baseline.py:18:import subprocess  # nosec: B404\n/home/galen/.rye/py/cpython@3.12.3/lib/python3.12/site-packages/bandit/cli/baseline.py:107:                output = subprocess.check_output(bandit_command)  # nosec: B603\n/home/galen/.rye/py/cpython@3.12.3/lib/python3.12/site-packages/bandit/cli/main.py:332:        help=\"do not skip lines with # nosec comments\",\n/home/galen/.rye/py/cpython@3.12.3/lib/python3.12/site-packages/bandit/cli/main.py:586:            \"do not skip lines with # nosec\",\n/home/galen/.rye/py/cpython@3.12.3/lib/python3.12/site-packages/bandit/formatters/xml.py:38:from xml.etree import ElementTree as ET  # nosec: B405\n/home/galen/.rye/py/cpython@3.12.3/lib/python3.12/site-packages/bandit/core/manager.py:308:            # nosec_lines is a dict of line number -&gt; set of tests to ignore\n/home/galen/.rye/py/cpython@3.12.3/lib/python3.12/site-packages/bandit/core/utils.py:367:    except AttributeError:  # nosec(tkelsey): catching expected exception\n\n\n\nThis is always something important to keep in mind about Bandit. It can tell you about potential security issues, but it also reports when it has been told to ignore some line of code."
  },
  {
    "objectID": "posts/automorphism-orbits-of-graphlets/index.html",
    "href": "posts/automorphism-orbits-of-graphlets/index.html",
    "title": "Automorphism Orbits of Graphlets",
    "section": "",
    "text": "Back in May 2020 I released a video on YouTube for the HackSeq event:\n\n\nBut I figure I could give some written description as well, which is what the rest of this blog post covers.\nA graph is a 2-tuple containing a set of edges and a set of vertices, and the set of edges is a subset of the cartesian product of the set of vertices with itself. We can think the vertices as ‘things’ and the set of edges as a binary relation between them.\nSometimes graphs are called “networks” when either the vertices (often called “nodes” when discussing networks) or edges have additional attributes, however this usage is not universally accepted. Three common properties of graph edges are whether they are directed, signed, or weighted. In the case of directed edges, some sort of ‘directionality’ is associated with the edges, and we’d say that (u,v) and (v,u) are considered distinct for two vertices u and v from the graph. A graph with directed edges is called a directed graph or a digraph. In the case of signed edges, each edge is assigned a ‘positive’ or ‘negative’ value, and we call such a graph a signed graph. In the case of weighted edges, each edge is assigned a numerical value, and we call such a graph a weighted graph. If a graph doesn’t have directed, signed, or weighted edges and doesn’t have multiple edges for any given pair of vertices, then it is called a simple graph.\nA graphlet is a specific type of graph, inheriting all the properties of graphs but also being a weakly-connected induced subgraph. For a graph to be subgraph, there exists another graph such that the subgraph’s vertex set is a subset of the other graph’s vertex set and the subgraph’s edge set is a subset of the other graph’s edge set. A subgraph being induced can be thought of procedurally, by first selecting any subset of the vertices and then also selecting all edges that connect those selected vertices. The property of weakly-connected can be considered for any graph, and means that there exists a path between any pair of vertices in the graph.\nA function is injective if it satisfies that f(x)=f(y) implies that x=y.\n\n\n\nhttps://mathworld.wolfram.com/images/eps-gif/Injection_1000.gif\n\n\nA function is surjective if it satisfies for any element b in the image that there exists an element a of the domain for which b=f(a).\n\n\n\nhttps://mathworld.wolfram.com/images/eps-gif/Surjection_1000.gif\n\n\nA bijection is a function that is both injective (one-to-one) and surjective (onto) from one set to another (these two sets can be the same set).\n\n\n\nhttps://mathworld.wolfram.com/images/eps-gif/Bijection_1000.gif\n\n\nA graph isomorphism is a bijection f between two graphs (which can be the same graph in the case of graph automorphisms) such that any two vertices u and v in the first graph are adjacent if-and-only-if f(u) and f(v) are adjacent.\n\n\n\nhttps://upload.wikimedia.org/wikipedia/commons/9/9a/Graph_isomorphism_a.svg\n\n\n\n\n\nhttps://upload.wikimedia.org/wikipedia/commons/8/84/Graph_isomorphism_b.svg\n\n\nA graph automorphism is a graph isomorphism where the domain graph and the image graph are the same graph.\n\n\n\nhttps://mathworld.wolfram.com/images/eps-gif/GraphAutomorphismGridGraph_1000.gif\n\n\nAn equivalence relation is a binary relation that is reflexive, symmetric, and transitive. An equivalence class is a subset of a set such that all members of the subset adhere to an equivalence relation. A (vertex) orbit automorphism is an equivalence class from the vertex set of a graph under the action of a graph automorphism. Multiple graph automorphisms are possible, and the set of all automorphisms with composition of permutations of the vertex set is called a permutation group.\nSince graphlets are graphs, and orbit automorphisms can be found within graphs, we can find orbit automorphisms within graphlets. The idea behind enumeration of orbit automorphisms of graphlets is to count the number of times each vertex of a graph participates in each orbit automorphism of each graphlet from a set of graphlets. While each finite graph has a finite number of distinct (i.e. mutually non-isomorphic) graphlets, considering every conceivable graphlet would be computationally infeasible. Instead of considering all graphlets of a graph, a constraint is often imposed where graphlets containing only 2-3 vertices are considered.\nEnumeration of orbit automorphisms of graphlets has been used to characterize correlation networks of coexpression of genes, and characterize the role of traders in the world trade network."
  },
  {
    "objectID": "posts/rust-poisson-binomial-pmf-implementation/index.html",
    "href": "posts/rust-poisson-binomial-pmf-implementation/index.html",
    "title": "A Rust Implementation of the Poisson Binomial Probability Distribution",
    "section": "",
    "text": "The probability mass function of the Poisson binomial distribution is given by\n\\[\\sum_{A \\in F_k} \\prod_{i \\in A} p_i \\prod_{j \\in A^c} (1 - p_j)\\]\nwhere \\(F_k\\) is the set of all subsets of \\(k\\) integers that can be selected from the set \\(\\{ 1, \\ldots, n \\}\\). This expression does not by itself suggest one algorithm over another due to the cummutativty and associativity of the operators involved.\nWikipedia gives pseudocode for computing the probability mass function for the Poisson binomial distribution via what it terms the “direct convolution algorithm”.\n\nfn dc(p: &[f64]) -&gt; Vec&lt;f64&gt; {\n    let n = p.len();\n    let mut pmf: Vec&lt;f64&gt; = vec![1.0]; // PMF array with size 1, initialized to 1\n\n    for i in 0..n {\n        // Create a new next_pmf array with size i + 2\n        let mut next_pmf: Vec&lt;f64&gt; = vec![0.0; i + 2]; \n\n        // Calculate the first element of next_pmf\n        next_pmf[0] = (1.0 - p[i]) * pmf[0];\n\n        // Calculate the last element of next_pmf if within bounds\n        if i &lt; pmf.len() {\n            next_pmf[i + 1] = p[i] * pmf[i];\n        }\n\n        // Update the rest of next_pmf\n        for k in 1..=i {\n            next_pmf[k] = p[i] * pmf[k - 1] + (1.0 - p[i]) * pmf[k];\n        }\n\n        // Update pmf for the next iteration\n        pmf = next_pmf;\n    }\n\n    pmf\n}\n\nLet’s try to use the algorithm.\n\n// Example usage\nlet p: Vec&lt;f64&gt; = vec![0.1, 0.2, 0.3, 0.4];\nlet pmf: Vec&lt;f64&gt; = dc(&p);\nprintln!(\"{:?}\", pmf);\n\n[0.3024, 0.4404, 0.2144, 0.0404, 0.0024000000000000007]\n\n\nWe can also double check that the result resembles a probability by checking if the sum of the probabilities is in fact equal to unity.\n\nlet sum_check: f64 = pmf.iter().sum(); // Calculate the sum of the PMF values\nprintln!(\"Sum of PMF: {}\", sum_check);\n\nSum of PMF: 1\n\n\nWe can also define the cumulative distribution function (CDF) as the cumulative sum of the PMF.\n\nfn compute_cdf(pmf: &[f64]) -&gt; Vec&lt;f64&gt; {\n    let mut cdf: Vec&lt;f64&gt; = Vec::with_capacity(pmf.len());\n    let mut sum = 0.0;\n\n    for &prob in pmf {\n        sum += prob;\n        cdf.push(sum);\n    }\n\n    cdf\n}\n\nHere is an example of using the CDF.\n\nlet cdf: Vec&lt;f64&gt; = compute_cdf(&pmf);\nprintln!(\"CDF: {:?}\", cdf);\n\nCDF: [0.3024, 0.7428, 0.9572, 0.9976, 1.0]\n\n\nOften we will want to be able to sample from such a probability distribution. Here is a function which implicitly relies on the inverse transform theorem.\n\n:dep rand\nuse rand::Rng;\n\nfn sample_from_distribution(cdf: &[f64]) -&gt; usize {\n    let mut rng = rand::thread_rng();\n    let random_value: f64 = rng.gen(); // Generate a uniform random value between 0 and 1\n\n    cdf.iter()\n        .position(|&x| random_value &lt;= x)\n        .unwrap_or(cdf.len() - 1) // If not found, return the last index\n}\n\nNow let us take some samples.\n\nlet num_samples = 10000;\n\nlet samples: Vec&lt;usize&gt; = (0..num_samples)\n        .map(|_| sample_from_distribution(&cdf))\n        .collect();\n\n\nprintln!(\"Samples: {:?}\", samples);\n\nSamples: [2, 2, 2, 1, 0, 1, 1, 2, 1, 0, 1, 0, 0, 2, 0, 1, 1, 0, 1, 0, 2, 2, 0, 1, 0, 2, 0, 2, 2, 2, 0, 1, 1, 0, 2, 1, 1, 2, 3, 1, 0, 0, 0, 2, 2, 0, 1, 0, 1, 3, 1, 2, 1, 1, 2, 2, 0, 2, 1, 0, 2, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 2, 0, 3, 1, 1, 1, 2, 0, 0, 1, 2, 0, 1, 1, 0, 1, 2, 1, 0, 1, 2, 2, 1, 3, 0, 3, 1, 2, 2, 2, 1, 0, 0, 1, 2, 1, 3, 0, 0, 1, 1, 1, 1, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 2, 0, 1, 0, 1, 1, 1, 0, 0, 1, 2, 2, 0, 2, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 2, 0, 2, 0, 2, 1, 1, 3, 1, 2, 0, 0, 1, 1, 2, 2, 3, 2, 1, 1, 0, 1, 1, 1, 2, 1, 1, 2, 1, 0, 1, 0, 2, 1, 0, 0, 2, 1, 0, 1, 1, 2, 2, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 3, 1, 0, 1, 2, 4, 1, 4, 0, 1, 1, 2, 1, 0, 1, 1, 1, 1, 0, 2, 2, 1, 1, 0, 1, 2, 1, 2, 0, 1, 0, 1, 1, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 1, 1, 1, 0, 2, 2, 1, 0, 0, 0, 0, 2, 0, 2, 2, 1, 1, 1, 0, 1, 2, 4, 2, 2, 1, 1, 1, 2, 1, 2, 1, 0, 4, 0, 0, 1, 0, 1, 1, 1, 1, 2, 3, 0, 1, 2, 1, 0, 2, 0, 1, 2, 1, 2, 3, 0, 1, 1, 1, 2, 1, 0, 0, 0, 2, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 2, 1, 0, 1, 2, 2, 2, 2, 2, 2, 0, 2, 1, 0, 0, 2, 0, 2, 2, 0, 1, 1, 1, 1, 2, 2, 1, 0, 1, 1, 1, 3, 2, 2, 0, 0, 1, 2, 2, 1, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 2, 1, 1, 2, 0, 1, 0, 0, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 1, 3, 0, 2, 2, 2, 1, 2, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 0, 2, 0, 1, 0, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 2, 0, 1, 2, 1, 0, 0, 0, 0, 1, 1, 1, 0, 2, 2, 2, 1, 1, 1, 0, 2, 2, 2, 1, 1, 1, 1, 0, 2, 0, 1, 1, 1, 0, 2, 1, 1, 0, 1, 1, 2, 1, 1, 1, 2, 2, 1, 0, 2, 1, 0, 1, 1, 0, 2, 0, 2, 2, 1, 1, 1, 0, 0, 0, 1, 2, 2, 2, 1, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 1, 0, 0, 1, 1, 2, 2, 1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 3, 0, 1, 0, 0, 0, 1, 1, 0, 2, 1, 1, 0, 1, 2, 1, 0, 2, 0, 0, 2, 1, 2, 1, 1, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 1, 2, 3, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 3, 0, 0, 1, 1, 3, 1, 2, 1, 0, 1, 0, 0, 1, 1, 3, 2, 2, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 2, 1, 0, 1, 2, 1, 0, 1, 0, 0, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 0, 2, 0, 0, 4, 1, 0, 1, 2, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 2, 0, 1, 2, 0, 1, 2, 0, 2, 2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 0, 0, 1, 2, 1, 1, 1, 1, 1, 2, 1, 0, 0, 3, 0, 2, 1, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 2, 0, 0, 1, 2, 2, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 2, 2, 0, 0, 3, 1, 2, 2, 0, 1, 2, 2, 2, 1, 1, 0, 1, 3, 0, 0, 3, 1, 1, 0, 3, 1, 1, 2, 0, 1, 1, 1, 1, 3, 1, 2, 1, 1, 2, 2, 2, 0, 0, 1, 1, 0, 3, 1, 0, 2, 1, 0, 1, 1, 1, 0, 0, 2, 2, 1, 2, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 3, 1, 1, 1, 1, 1, 0, 1, 1, 1, 3, 0, 2, 0, 2, 1, 0, 2, 1, 0, 0, 0, 1, 1, 2, 0, 1, 0, 2, 1, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 1, 1, 2, 0, 1, 2, 1, 2, 1, 2, 1, 1, 1, 3, 0, 1, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 1, 1, 2, 2, 1, 3, 1, 1, 0, 0, 2, 2, 0, 1, 0, 0, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 0, 2, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 0, 0, 2, 0, 1, 0, 1, 2, 1, 1, 2, 2, 2, 0, 0, 1, 0, 1, 1, 1, 0, 2, 1, 2, 1, 1, 1, 2, 1, 0, 1, 0, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 1, 0, 2, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 3, 1, 1, 0, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 1, 1, 1, 0, 2, 0, 2, 0, 2, 2, 3, 1, 2, 0, 1, 1, 1, 2, 0, 0, 1, 1, 1, 0, 1, 2, 3, 1, 1, 2, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 3, 1, 2, 1, 1, 2, 1, 1, 0, 1, 2, 0, 1, 1, 2, 2, 0, 0, 0, 1, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1, 3, 2, 0, 0, 1, 2, 2, 2, 1, 2, 0, 2, 0, 0, 2, 0, 1, 1, 0, 1, 1, 2, 1, 0, 2, 1, 2, 2, 1, 1, 1, 0, 1, 1, 2, 0, 1, 2, 0, 1, 3, 0, 1, 1, 1, 0, 0, 1, 0, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 3, 3, 0, 2, 2, 0, 2, 0, 1, 0, 0, 0, 1, 2, 0, 0, 2, 3, 0, 1, 2, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 0, 0, 2, 2, 1, 1, 1, 1, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 1, 0, 3, 2, 0, 0, 1, 2, 2, 0, 2, 0, 1, 0, 1, 1, 0, 0, 0, 1, 2, 1, 0, 1, 2, 1, 3, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 0, 1, 1, 2, 0, 3, 2, 1, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 0, 1, 1, 2, 1, 0, 2, 4, 1, 0, 2, 0, 0, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 1, 1, 2, 3, 0, 1, 2, 0, 1, 0, 1, 1, 0, 3, 1, 1, 2, 1, 1, 2, 0, 2, 1, 0, 1, 2, 2, 0, 1, 1, 1, 0, 0, 2, 1, 0, 2, 0, 0, 2, 0, 1, 1, 1, 1, 2, 0, 0, 0, 2, 2, 0, 0, 3, 1, 2, 0, 0, 2, 0, 3, 0, 0, 2, 1, 2, 0, 1, 0, 1, 1, 2, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 0, 0, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 2, 2, 1, 0, 1, 0, 3, 2, 1, 0, 0, 2, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 3, 0, 1, 1, 2, 3, 0, 2, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 2, 0, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 0, 1, 1, 0, 0, 2, 2, 1, 2, 2, 0, 1, 0, 1, 2, 1, 1, 1, 1, 3, 1, 0, 0, 1, 0, 1, 0, 0, 2, 2, 1, 1, 1, 0, 0, 1, 1, 1, 2, 2, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 0, 2, 2, 2, 2, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 3, 2, 1, 0, 3, 2, 2, 0, 2, 0, 0, 2, 1, 3, 1, 1, 1, 1, 1, 3, 3, 2, 2, 1, 0, 1, 1, 3, 1, 1, 0, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 0, 1, 0, 2, 0, 1, 1, 2, 1, 0, 2, 0, 0, 0, 1, 2, 1, 2, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 2, 2, 4, 2, 1, 2, 1, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0, 2, 3, 2, 1, 0, 1, 0, 2, 0, 0, 1, 3, 1, 0, 1, 0, 0, 1, 3, 2, 2, 1, 0, 1, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 2, 3, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 0, 1, 3, 2, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 0, 0, 2, 2, 0, 1, 0, 1, 0, 0, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 0, 0, 1, 2, 0, 1, 1, 1, 1, 0, 2, 1, 0, 0, 1, 3, 0, 2, 2, 1, 1, 1, 3, 3, 4, 0, 0, 1, 1, 1, 0, 1, 0, 0, 3, 2, 1, 0, 1, 0, 0, 0, 0, 0, 3, 2, 1, 0, 3, 0, 3, 1, 1, 1, 1, 1, 0, 2, 0, 2, 0, 2, 2, 1, 3, 2, 1, 1, 0, 2, 2, 2, 1, 3, 0, 0, 1, 2, 3, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 2, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 2, 2, 0, 3, 1, 1, 2, 0, 2, 1, 2, 2, 1, 1, 2, 1, 0, 3, 3, 2, 0, 1, 0, 2, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 2, 0, 1, 3, 1, 1, 2, 1, 0, 1, 0, 0, 2, 0, 1, 2, 2, 1, 1, 1, 0, 0, 3, 2, 2, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 0, 2, 2, 1, 0, 1, 3, 3, 1, 1, 0, 3, 0, 1, 1, 2, 0, 1, 2, 0, 0, 1, 1, 1, 1, 2, 1, 2, 3, 0, 1, 0, 0, 1, 1, 3, 1, 3, 1, 1, 0, 3, 1, 2, 0, 0, 1, 0, 1, 0, 2, 0, 0, 2, 0, 2, 1, 1, 0, 0, 0, 1, 1, 1, 1, 2, 0, 1, 2, 1, 2, 1, 2, 0, 1, 3, 1, 2, 0, 1, 1, 0, 0, 1, 1, 2, 2, 1, 1, 3, 2, 0, 2, 3, 1, 0, 2, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 2, 1, 0, 2, 1, 1, 2, 1, 1, 0, 1, 2, 2, 1, 1, 3, 3, 0, 0, 1, 0, 2, 0, 3, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 2, 1, 0, 0, 2, 1, 1, 2, 1, 2, 2, 0, 1, 1, 1, 2, 0, 2, 2, 0, 1, 0, 2, 0, 2, 1, 1, 1, 0, 2, 0, 0, 2, 1, 0, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 3, 0, 0, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 0, 1, 0, 2, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 1, 0, 2, 2, 2, 0, 0, 2, 2, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 2, 1, 2, 0, 0, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 3, 1, 0, 0, 1, 1, 2, 0, 1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 2, 1, 2, 1, 1, 2, 2, 1, 1, 0, 1, 0, 3, 1, 1, 2, 0, 1, 0, 2, 1, 2, 1, 1, 3, 0, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 0, 1, 0, 2, 1, 2, 1, 1, 1, 2, 2, 0, 0, 1, 3, 3, 2, 1, 2, 0, 2, 1, 0, 0, 1, 2, 2, 1, 0, 2, 1, 1, 0, 3, 2, 2, 1, 0, 1, 2, 1, 0, 3, 2, 1, 2, 1, 1, 0, 2, 1, 3, 1, 2, 0, 1, 2, 3, 1, 1, 1, 1, 2, 0, 0, 0, 1, 0, 1, 1, 2, 2, 2, 0, 0, 1, 1, 1, 1, 1, 1, 0, 3, 2, 1, 0, 1, 1, 1, 0, 2, 0, 1, 2, 1, 2, 0, 0, 1, 2, 0, 1, 2, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 0, 1, 2, 0, 2, 2, 1, 1, 0, 2, 1, 2, 1, 1, 3, 0, 0, 1, 1, 2, 2, 3, 1, 1, 1, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 3, 3, 0, 1, 1, 0, 1, 1, 1, 3, 1, 1, 1, 0, 1, 2, 1, 0, 2, 1, 1, 1, 2, 1, 3, 0, 0, 3, 1, 1, 0, 1, 1, 0, 2, 3, 2, 1, 2, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 2, 0, 1, 2, 1, 3, 0, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 0, 0, 1, 2, 1, 2, 2, 1, 0, 1, 2, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 0, 0, 1, 1, 0, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 0, 1, 3, 1, 1, 3, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 2, 1, 2, 2, 2, 1, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 2, 1, 2, 1, 3, 1, 2, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 2, 0, 1, 2, 1, 2, 3, 0, 0, 2, 1, 1, 0, 0, 2, 1, 1, 1, 2, 1, 0, 2, 1, 3, 3, 2, 1, 0, 0, 0, 1, 1, 1, 0, 2, 1, 0, 1, 1, 2, 1, 1, 2, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 2, 0, 1, 0, 2, 2, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 1, 0, 2, 2, 1, 1, 0, 3, 0, 1, 1, 1, 0, 2, 2, 1, 2, 1, 1, 0, 3, 2, 1, 1, 2, 3, 2, 3, 1, 2, 0, 3, 1, 1, 2, 2, 0, 0, 2, 0, 2, 1, 0, 2, 1, 1, 1, 0, 2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 3, 0, 1, 2, 0, 2, 0, 0, 0, 1, 2, 2, 2, 1, 2, 1, 1, 0, 0, 0, 1, 1, 0, 2, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 3, 2, 1, 1, 2, 0, 2, 4, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 2, 2, 1, 1, 1, 3, 0, 3, 1, 2, 2, 2, 0, 1, 0, 2, 1, 1, 1, 2, 2, 0, 2, 3, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 2, 1, 0, 1, 0, 1, 1, 2, 0, 1, 0, 1, 2, 1, 3, 0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 2, 2, 0, 1, 1, 1, 1, 2, 1, 0, 0, 1, 0, 3, 0, 2, 1, 0, 1, 1, 0, 2, 1, 0, 3, 1, 0, 0, 2, 0, 0, 2, 3, 1, 0, 0, 0, 0, 1, 0, 1, 2, 2, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2, 2, 2, 1, 1, 0, 2, 2, 2, 0, 1, 1, 2, 2, 0, 0, 0, 2, 1, 0, 1, 0, 1, 1, 2, 1, 1, 2, 2, 1, 2, 0, 3, 0, 1, 0, 2, 1, 0, 1, 1, 1, 3, 1, 1, 0, 1, 2, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 3, 0, 0, 0, 2, 2, 2, 1, 0, 0, 0, 3, 1, 0, 0, 1, 0, 1, 2, 3, 1, 1, 0, 2, 1, 1, 1, 1, 3, 0, 1, 1, 1, 0, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 1, 1, 1, 1, 2, 0, 1, 0, 1, 2, 2, 0, 3, 1, 1, 3, 3, 2, 2, 0, 2, 0, 1, 0, 2, 2, 2, 1, 1, 0, 1, 2, 1, 1, 1, 3, 1, 1, 1, 2, 0, 1, 0, 1, 0, 2, 2, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 3, 1, 0, 0, 3, 1, 2, 0, 2, 3, 2, 1, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 3, 1, 3, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 1, 2, 2, 0, 1, 1, 3, 0, 2, 0, 1, 1, 1, 1, 1, 1, 2, 1, 0, 2, 1, 1, 2, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 2, 2, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 3, 1, 3, 2, 2, 2, 1, 1, 2, 2, 2, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 0, 1, 0, 1, 2, 0, 1, 1, 0, 1, 0, 2, 1, 1, 2, 3, 2, 1, 1, 2, 1, 1, 0, 0, 0, 1, 1, 2, 2, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 2, 3, 1, 0, 1, 2, 1, 1, 1, 1, 2, 3, 0, 1, 1, 1, 2, 0, 1, 0, 1, 2, 0, 2, 1, 2, 0, 0, 1, 3, 0, 1, 1, 1, 2, 3, 1, 1, 2, 0, 1, 0, 2, 0, 2, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 3, 1, 0, 3, 0, 0, 0, 1, 2, 1, 0, 2, 1, 0, 1, 2, 1, 1, 1, 0, 1, 0, 1, 0, 3, 1, 1, 0, 1, 1, 2, 0, 2, 0, 2, 2, 1, 2, 1, 1, 0, 2, 1, 0, 0, 1, 0, 1, 2, 0, 1, 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 3, 0, 1, 0, 0, 2, 1, 2, 0, 1, 1, 2, 1, 0, 3, 1, 1, 1, 3, 1, 2, 0, 1, 1, 2, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 2, 1, 3, 2, 1, 0, 1, 3, 2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 0, 1, 2, 3, 2, 0, 1, 1, 1, 1, 0, 2, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 2, 0, 1, 1, 3, 2, 1, 0, 2, 1, 1, 2, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 2, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 1, 0, 1, 2, 1, 2, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0, 1, 0, 2, 1, 2, 0, 2, 1, 1, 0, 2, 2, 3, 2, 0, 2, 0, 1, 0, 1, 0, 2, 0, 1, 2, 1, 2, 2, 0, 1, 2, 0, 1, 2, 0, 2, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 0, 0, 3, 0, 1, 0, 2, 1, 0, 1, 1, 3, 3, 1, 0, 0, 0, 3, 1, 4, 1, 0, 0, 0, 1, 1, 0, 1, 1, 2, 0, 1, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 3, 2, 1, 1, 1, 0, 2, 1, 0, 0, 0, 0, 1, 0, 1, 0, 2, 1, 0, 1, 0, 3, 1, 0, 1, 1, 0, 2, 2, 1, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 3, 1, 2, 1, 0, 3, 0, 2, 0, 1, 1, 1, 2, 1, 2, 1, 2, 1, 0, 0, 2, 0, 1, 3, 2, 0, 2, 1, 1, 1, 0, 0, 3, 1, 2, 0, 1, 1, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 0, 2, 1, 2, 2, 0, 2, 1, 1, 1, 3, 1, 0, 1, 0, 1, 1, 1, 1, 4, 1, 1, 0, 2, 2, 2, 1, 0, 1, 1, 1, 0, 2, 1, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 0, 2, 1, 1, 0, 3, 0, 0, 0, 1, 2, 2, 1, 0, 1, 1, 2, 2, 1, 1, 2, 1, 0, 2, 1, 1, 0, 3, 4, 0, 1, 1, 0, 1, 2, 1, 2, 2, 2, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1, 1, 0, 1, 1, 1, 4, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 0, 0, 1, 3, 2, 2, 3, 0, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 1, 1, 2, 0, 1, 2, 2, 1, 1, 3, 0, 0, 1, 1, 2, 2, 1, 2, 2, 2, 0, 3, 0, 2, 2, 0, 1, 0, 1, 0, 1, 1, 1, 2, 0, 1, 2, 2, 0, 0, 0, 1, 1, 2, 3, 3, 0, 1, 3, 0, 1, 2, 1, 1, 2, 1, 0, 1, 2, 1, 0, 0, 0, 1, 2, 1, 1, 1, 0, 1, 3, 2, 0, 0, 2, 0, 1, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 0, 0, 3, 0, 0, 2, 1, 2, 0, 0, 0, 3, 0, 2, 1, 3, 2, 1, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 2, 1, 0, 1, 1, 1, 3, 0, 0, 0, 1, 1, 0, 3, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 0, 3, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 2, 2, 1, 1, 0, 2, 2, 0, 2, 0, 2, 1, 2, 2, 2, 0, 0, 1, 0, 1, 2, 1, 2, 1, 1, 0, 2, 0, 0, 1, 2, 1, 0, 0, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 0, 0, 2, 2, 2, 2, 1, 1, 0, 2, 0, 1, 2, 0, 2, 1, 2, 2, 0, 0, 1, 1, 1, 2, 1, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 1, 1, 0, 1, 1, 2, 1, 0, 3, 1, 2, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 2, 0, 1, 1, 2, 3, 1, 1, 1, 0, 0, 1, 0, 1, 2, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 2, 0, 2, 1, 2, 2, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 3, 3, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 2, 1, 0, 1, 0, 0, 0, 0, 2, 1, 1, 1, 1, 2, 1, 0, 2, 1, 0, 0, 1, 1, 1, 1, 0, 2, 0, 0, 0, 0, 1, 1, 2, 2, 0, 0, 1, 2, 0, 1, 0, 2, 1, 3, 1, 0, 1, 1, 0, 1, 0, 2, 2, 2, 0, 1, 0, 2, 1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 1, 2, 3, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 2, 0, 1, 1, 1, 2, 1, 0, 0, 1, 1, 2, 1, 2, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 0, 1, 2, 1, 1, 0, 1, 2, 1, 0, 1, 2, 1, 1, 0, 0, 2, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 2, 1, 0, 1, 0, 3, 2, 1, 2, 0, 2, 2, 2, 0, 0, 1, 0, 1, 1, 1, 1, 1, 3, 2, 0, 1, 0, 1, 3, 1, 2, 0, 2, 0, 1, 2, 2, 0, 1, 3, 1, 2, 0, 2, 0, 1, 2, 1, 2, 1, 1, 1, 1, 1, 0, 2, 0, 1, 1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 2, 1, 1, 1, 2, 0, 0, 3, 1, 1, 3, 1, 1, 1, 2, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 2, 0, 0, 1, 0, 1, 1, 2, 0, 1, 1, 2, 1, 2, 0, 0, 1, 0, 1, 0, 0, 2, 1, 2, 0, 1, 0, 0, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 2, 2, 2, 0, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 0, 2, 1, 1, 1, 0, 1, 0, 1, 3, 0, 1, 1, 0, 1, 2, 2, 1, 3, 2, 1, 0, 0, 0, 1, 0, 3, 1, 1, 3, 1, 2, 2, 1, 0, 1, 1, 2, 3, 1, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 0, 0, 0, 0, 1, 3, 0, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 3, 2, 0, 3, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0, 3, 2, 2, 2, 1, 0, 1, 1, 1, 2, 0, 2, 1, 2, 2, 1, 1, 2, 0, 1, 0, 3, 0, 1, 2, 0, 1, 2, 1, 0, 0, 0, 1, 2, 1, 1, 0, 1, 1, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2, 1, 1, 2, 0, 3, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 3, 0, 2, 0, 0, 1, 1, 0, 2, 0, 3, 2, 1, 2, 0, 1, 2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 0, 1, 0, 1, 1, 0, 1, 1, 2, 1, 1, 0, 2, 0, 1, 1, 1, 2, 1, 0, 0, 2, 0, 0, 1, 1, 1, 2, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 3, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 2, 0, 1, 2, 0, 2, 1, 1, 1, 1, 3, 0, 0, 1, 2, 1, 2, 1, 1, 0, 1, 0, 1, 0, 2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 1, 1, 0, 0, 1, 2, 0, 1, 2, 0, 1, 0, 0, 2, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1, 0, 2, 1, 1, 0, 3, 1, 0, 1, 1, 0, 0, 1, 0, 2, 1, 0, 1, 1, 1, 1, 1, 0, 3, 1, 0, 2, 2, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 2, 0, 1, 0, 1, 1, 0, 1, 0, 2, 0, 1, 1, 0, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1, 2, 0, 2, 0, 1, 2, 1, 0, 2, 1, 1, 1, 1, 2, 2, 0, 1, 2, 1, 1, 1, 0, 2, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 2, 2, 2, 0, 1, 0, 2, 1, 0, 2, 3, 1, 1, 2, 0, 2, 1, 2, 1, 0, 1, 2, 1, 0, 0, 1, 1, 2, 2, 1, 1, 0, 1, 0, 3, 2, 1, 2, 1, 1, 2, 0, 1, 1, 0, 0, 2, 0, 2, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 2, 1, 0, 1, 0, 1, 3, 1, 0, 2, 1, 1, 0, 0, 2, 2, 1, 1, 1, 0, 0, 1, 2, 0, 2, 0, 2, 0, 0, 1, 0, 2, 2, 2, 1, 2, 0, 0, 1, 1, 2, 0, 1, 2, 1, 2, 1, 3, 1, 0, 1, 1, 2, 1, 2, 0, 1, 2, 2, 1, 1, 1, 0, 2, 2, 2, 1, 0, 1, 2, 2, 1, 2, 3, 2, 2, 1, 1, 0, 3, 2, 0, 1, 1, 0, 1, 2, 2, 1, 2, 0, 1, 1, 0, 0, 2, 2, 2, 1, 2, 0, 0, 1, 2, 1, 1, 1, 0, 2, 0, 1, 1, 1, 2, 0, 1, 1, 1, 1, 0, 3, 1, 3, 2, 0, 2, 2, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 2, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 2, 2, 0, 2, 4, 1, 2, 0, 1, 0, 1, 1, 1, 0, 2, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 2, 0, 3, 0, 1, 1, 2, 1, 1, 1, 1, 2, 0, 0, 1, 1, 2, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 2, 0, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 2, 2, 1, 1, 2, 0, 0, 1, 2, 0, 0, 2, 2, 1, 1, 2, 0, 0, 2, 2, 1, 0, 1, 1, 1, 1, 0, 2, 2, 0, 1, 1, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 0, 2, 0, 2, 2, 1, 2, 1, 2, 1, 1, 1, 3, 2, 0, 0, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0, 2, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 2, 1, 3, 1, 0, 0, 0, 1, 2, 2, 0, 0, 2, 1, 4, 2, 0, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 0, 0, 0, 0, 1, 0, 3, 0, 1, 3, 3, 0, 2, 1, 0, 2, 1, 2, 0, 1, 1, 1, 2, 1, 0, 1, 0, 1, 3, 1, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 0, 1, 3, 1, 2, 2, 0, 0, 2, 1, 1, 2, 2, 1, 2, 1, 1, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 2, 0, 0, 0, 2, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 2, 2, 2, 1, 0, 0, 1, 2, 0, 1, 1, 2, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 2, 1, 1, 2, 2, 3, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 3, 1, 0, 2, 1, 1, 1, 3, 3, 2, 0, 0, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1, 0, 0, 1, 2, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 2, 1, 2, 1, 2, 0, 2, 1, 0, 2, 0, 1, 1, 1, 1, 2, 3, 2, 0, 1, 1, 0, 0, 2, 2, 1, 1, 1, 1, 2, 3, 0, 2, 0, 1, 1, 2, 0, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 2, 3, 0, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 0, 0, 0, 2, 1, 0, 2, 0, 0, 2, 0, 1, 1, 1, 2, 2, 2, 2, 2, 1, 0, 3, 2, 0, 0, 1, 3, 1, 1, 0, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 2, 1, 0, 2, 0, 2, 2, 1, 0, 0, 2, 0, 0, 1, 1, 1, 1, 3, 0, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 2, 2, 1, 0, 1, 1, 2, 2, 3, 2, 0, 0, 4, 1, 0, 2, 0, 2, 1, 2, 0, 0, 2, 1, 1, 1, 1, 1, 3, 0, 1, 0, 2, 2, 2, 1, 3, 1, 0, 2, 3, 1, 0, 0, 0, 3, 0, 2, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 2, 0, 1, 1, 0, 2, 1, 0, 1, 1, 0, 1, 0, 1, 2, 1, 0, 1, 1, 0, 3, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 2, 0, 2, 1, 1, 1, 0, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 2, 0, 1, 1, 2, 1, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 3, 2, 0, 1, 1, 0, 1, 0, 1, 1, 0, 2, 1, 0, 2, 1, 1, 1, 0, 1, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 2, 1, 1, 0, 1, 0, 1, 3, 1, 1, 0, 1, 1, 0, 1, 1, 3, 0, 0, 1, 3, 2, 1, 2, 1, 0, 2, 0, 1, 1, 0, 2, 2, 1, 1, 3, 2, 1, 1, 0, 2, 2, 2, 1, 1, 1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 1, 1, 1, 2, 1, 0, 2, 1, 1, 1, 0, 0, 0, 1, 2, 0, 2, 0, 0, 2, 2, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 2, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 1, 0, 2, 2, 1, 0, 0, 2, 0, 4, 1, 3, 0, 0, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 0, 1, 0, 0, 2, 0, 0, 2, 1, 1, 2, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 3, 1, 3, 1, 0, 1, 1, 2, 2, 2, 0, 0, 2, 0, 1, 1, 2, 1, 1, 0, 1, 2, 0, 1, 0, 1, 0, 0, 1, 2, 2, 1, 1, 2, 0, 1, 0, 2, 2, 2, 2, 0, 0, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 0, 0, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 3, 1, 0, 1, 0, 1, 0, 2, 1, 0, 0, 0, 3, 2, 1, 0, 3, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 2, 1, 0, 0, 1, 0, 1, 0, 3, 1, 0, 0, 0, 1, 1, 0, 2, 1, 1, 1, 0, 1, 2, 0, 2, 1, 2, 1, 2, 0, 0, 1, 0, 2, 1, 2, 2, 2, 2, 0, 1, 1, 2, 0, 2, 0, 1, 1, 0, 0, 1, 1, 1, 2, 3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 0, 2, 1, 2, 2, 1, 0, 0, 2, 1, 1, 0, 0, 0, 0, 1, 0, 3, 1, 1, 1, 2, 2, 1, 0, 0, 0, 1, 1, 1, 3, 2, 1, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 3, 0, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 0, 2, 0, 1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 2, 1, 2, 1, 1, 2, 0, 1, 3, 0, 0, 1, 1, 1, 1, 0, 0, 0, 2, 2, 1, 1, 1, 3, 1, 2, 0, 2, 1, 1, 0, 3, 1, 1, 1, 0, 0, 1, 0, 2, 1, 2, 1, 1, 0, 1, 2, 1, 1, 2, 4, 1, 2, 2, 1, 1, 1, 2, 2, 1, 3, 0, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 3, 1, 2, 0, 1, 2, 1, 0, 0, 3, 1, 0, 0, 1, 0, 0, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 0, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 2, 0, 2, 2, 2, 1, 2, 1, 1, 0, 2, 1, 0, 1, 1, 0, 1, 1, 2, 2, 2, 2, 1, 1, 3, 2, 0, 0, 1, 1, 0, 3, 1, 1, 0, 1, 3, 1, 1, 1, 2, 0, 2, 2, 2, 1, 2, 0, 1, 2, 0, 1, 2, 0, 0, 0, 1, 0, 1, 3, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 2, 1, 2, 1, 3, 0, 2, 1, 0, 3, 1, 2, 3, 2, 1, 1, 1, 0, 1, 2, 1, 1, 0, 3, 2, 0, 1, 2, 0, 2, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 0, 3, 1, 2, 0, 0, 3, 1, 0, 0, 1, 0, 0, 1, 0, 3, 0, 0, 2, 3, 1, 1, 0, 2, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 0, 1, 0, 2, 0, 2, 1, 1, 1, 3, 1, 0, 1, 1, 1, 0, 1, 0, 1, 3, 1, 1, 2, 0, 2, 2, 2, 2, 0, 0, 1, 1, 2, 0, 1, 0, 2, 2, 2, 1, 0, 0, 0, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 0, 3, 0, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 2, 0, 0, 2, 2, 2, 1, 0, 3, 3, 1, 1, 1, 2, 0, 1, 2, 1, 0, 0, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 2, 0, 2, 1, 2, 0, 0, 1, 1, 1, 1, 2, 1, 0, 0, 0, 2, 2, 2, 3, 2, 1, 2, 2, 1, 3, 0, 0, 3, 2, 1, 0, 1, 1, 0, 3, 1, 0, 2, 2, 2, 1, 1, 1, 1, 1, 1, 3, 0, 1, 2, 0, 1, 2, 1, 1, 2, 2, 1, 1, 1, 0, 2, 1, 2, 1, 0, 3, 0, 1, 1, 1, 1, 2, 2, 2, 1, 1, 3, 1, 1, 3, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 3, 2, 1, 1, 0, 0, 3, 1, 0, 2, 1, 0, 1, 1, 0, 0, 1, 3, 2, 2, 2, 1, 0, 0, 0, 3, 0, 1, 1, 2, 0, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 3, 1, 2, 0, 1, 0, 0, 3, 1, 1, 0, 1, 1, 2, 0, 1, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 2, 2, 1, 2, 1, 0, 3, 2, 1, 1, 0, 0, 1, 4, 1, 0, 1, 2, 0, 1, 3, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 2, 0, 0, 3, 0, 1, 2, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 2, 1, 0, 1, 2, 0, 0, 0, 2, 2, 0, 1, 1, 1, 1, 1, 1, 2, 0, 2, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 0, 1, 3, 1, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 2, 3, 2, 1, 1, 3, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 2, 0, 1, 1, 0, 0, 1, 0, 1, 3, 1, 0, 0, 1, 0, 1, 1, 1, 2, 2, 2, 1, 0, 2, 3, 3, 1, 1, 1, 0, 2, 2, 1, 0, 1, 1, 1, 1, 0, 2, 3, 0, 2, 0, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 0, 3, 0, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 0, 2, 2, 1, 1, 0, 0, 1, 0, 0, 0, 1, 2, 2, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 3, 1, 0, 3, 0, 1, 2, 0, 2, 1, 0, 0, 0, 3, 0, 2, 2, 0, 0, 2, 1, 0, 2, 2, 2, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 3, 0, 2, 2, 1, 0, 0, 0, 0, 2, 3, 1, 1, 1, 1, 1, 0, 0, 2, 3, 0, 0, 0, 1, 2, 0, 1, 1, 2, 1, 4, 3, 0, 2, 1, 1, 0, 1, 2, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 1, 3, 2, 1, 3, 1, 1, 1, 1, 1, 2, 0, 0, 0, 2, 0, 1, 1, 0, 1, 2, 2, 1, 3, 0, 1, 2, 0, 0, 1, 1, 1, 0, 2, 3, 1, 2, 1, 0, 3, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 3, 1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 2, 1, 2, 0, 0, 1, 2, 2, 1, 1, 0, 2, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 3, 1, 0, 0, 1, 1, 1, 0, 0, 0, 3, 0, 1, 1, 3, 2, 1, 2, 1, 1, 0, 2, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 2, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 3, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 2, 3, 1, 0, 1, 2, 1, 1, 1, 0, 0, 0, 2, 1, 3, 1, 0, 0, 1, 1, 2, 1, 1, 2, 0, 1, 0, 1, 0, 0, 2, 0, 1, 2, 0, 3, 2, 0, 0, 1, 2, 1, 2, 0, 2, 1, 0, 3, 0, 1, 0, 2, 1, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 3, 2, 1, 1, 2, 2, 2, 0, 0, 1, 1, 1, 2, 0, 0, 0, 2, 1, 0, 1, 1, 2, 2, 0, 2, 0, 2, 0, 2, 0, 0, 1, 1, 3, 1, 0, 0, 0, 0, 2, 0, 2, 1, 0, 2, 1, 0, 3, 1, 0, 1, 0, 1, 1, 0, 3, 2, 1, 0, 1, 1, 3, 2, 2, 2, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 2, 0, 2, 1, 0, 1, 2, 1, 0, 1, 0, 2, 2, 0, 1, 1, 2, 3, 0, 2, 1, 1, 0, 0, 1, 1, 2, 1, 1, 2, 0, 2, 2, 2, 0, 0, 1, 0, 1, 1, 2, 2, 1, 2, 1, 1, 1, 0, 1, 1, 2, 3, 0, 3, 1, 1, 0, 1, 1, 2, 1, 1, 0, 0, 0, 2, 2, 1, 1, 0, 1, 2, 1, 1, 0, 0, 1, 2, 1, 1, 0, 1, 1, 0, 0, 4, 1, 2, 0, 0, 3, 2, 1, 2, 0, 1, 1, 2, 1, 1, 1, 2, 0, 2, 1, 0, 2, 0, 0, 1, 3, 3, 0, 0, 1, 1, 1, 1, 2, 0, 1, 0, 1, 0, 1, 1, 3, 0, 2, 1, 1, 0, 0, 2, 2, 2, 0, 0, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 1, 1, 2, 0, 1, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 2, 1, 3, 0, 1, 1, 3, 3, 1, 2, 3, 2, 1, 0, 2, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 3, 0, 1, 1, 1, 0, 1, 1, 2, 1, 0, 0, 0, 1, 1, 1, 2, 2, 0, 2, 0, 1, 2, 2, 0, 2, 1, 2, 0, 2, 1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 1, 0, 0, 0, 0, 2, 2, 0, 0, 1, 3, 1, 1, 0, 0, 2, 0, 1, 1, 2, 0, 2, 1, 0, 1, 3, 0, 2, 1, 0, 1, 1, 0, 1, 1, 3, 0, 2, 1, 1, 1, 0, 2, 2, 3, 1, 2, 1, 0, 1, 1, 1, 2, 2, 1, 2, 1, 0, 0, 1, 2, 2, 2, 0, 0, 1, 1, 0, 2, 1, 1, 1, 2, 0, 3, 1, 0, 2, 2, 2, 1, 2, 1, 1, 0, 2, 1, 2, 0, 2, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 1, 2, 0, 0, 0, 1, 1, 0, 0, 2, 2, 1, 2, 2, 1, 0, 2, 0, 0, 0, 1, 0, 2, 1, 0, 1, 1, 1, 1, 3, 2, 0, 0, 1, 2, 2, 2, 4, 2, 0, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 0, 3, 0, 2, 0, 3, 2, 1, 3, 2, 0, 3, 2, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0, 1, 1, 0, 3, 2, 0, 1, 1, 2, 1, 2, 0, 2, 1, 1, 0, 1, 1, 1, 2, 1, 0, 0, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 3, 0, 1, 0, 2, 1, 0, 1, 2, 2, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 1, 0, 1, 0, 2, 2, 1, 1, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 0, 1, 1, 2, 2, 0, 0, 0, 2, 2, 0, 1, 1, 1, 0, 1, 1, 1, 3, 1, 1, 2, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 2, 0, 1, 2, 0, 2, 1, 2, 2, 2, 0, 0, 1, 1, 4, 0, 1, 1, 2, 3, 0, 0, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 0, 1, 0, 2, 2, 3, 3, 0, 2, 0, 0, 1, 0, 2, 1, 1, 2, 1, 2, 0, 0, 1, 1, 1, 1, 4, 2, 2, 2, 0, 2, 1, 1, 1, 0, 2, 2, 1, 1, 1, 2, 0, 1, 1, 0, 2, 0, 1, 1, 0, 1, 1, 2, 1, 1, 2, 0, 3, 1, 2, 1, 2, 0, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 2, 3, 0, 3, 0, 0, 1, 1, 2, 1, 0, 1, 2, 1, 1, 0, 3, 3, 1, 1, 1, 1, 0, 1, 2, 2, 1, 0, 1, 0, 0, 1, 2, 2, 1, 0, 1, 2, 1, 2, 0, 2, 1, 1, 0, 1, 0, 0, 3, 1, 1, 1, 3, 2, 1, 1, 2, 1, 2, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 0, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 3, 3, 1, 1, 2, 2, 1, 2, 0, 1, 1, 1, 2, 0, 1, 1, 2, 2, 1, 1, 0, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 0, 1, 0, 0, 3, 2, 2, 2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 3, 0, 0, 2, 1, 1, 2, 1, 1, 2, 0, 0, 1, 2, 0, 2, 0, 2, 0, 1, 1, 2, 3, 2, 1, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0, 1, 1, 2, 1, 2, 1, 2, 1, 0, 2, 0, 1, 0, 3, 1, 1, 3, 2, 0, 1, 1, 0, 2, 1, 0, 2, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 0, 2, 1, 2, 2, 1, 1, 2, 1, 0, 0, 0, 1, 3, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 2, 0, 3, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 2, 0, 1, 0, 3, 1, 0, 2, 1, 0, 2, 0, 1, 1, 1, 0, 0, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 2, 1, 1, 1, 0, 0, 2, 1, 1, 3, 0, 1, 1, 0, 1, 0, 3, 2, 2, 0, 2, 0, 1, 1, 1, 0, 0, 0, 2, 3, 2, 0, 0, 0, 2, 2, 3, 2, 1, 2, 3, 0, 0, 2, 0, 0, 0, 0, 1, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 2, 1, 1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 2, 2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 2, 0, 0, 2, 0, 2, 2, 1, 2, 0, 2, 0, 1, 1, 2, 1, 1, 2, 1, 0, 0, 2, 1, 2, 2, 2, 1, 2, 1, 1, 0, 2, 1, 2, 2, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 2, 1, 2, 1, 2, 1, 1, 0, 3, 1, 0, 1, 1, 0, 3, 0, 1, 2, 0, 3, 1, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 1, 1, 1, 1, 3, 2, 0, 0, 3, 1, 3, 0, 2, 0, 1, 1, 0, 2, 0, 1, 1, 2, 0, 1, 2, 0, 0, 1, 0, 0, 0, 1, 2, 1, 0, 1, 0, 2, 1, 1, 2, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 2, 0, 1, 2, 0, 2, 2, 2, 0, 0, 0, 2, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 2, 0, 3, 2, 1, 2, 1, 0, 1, 1, 1, 1, 0, 2, 0, 3, 0, 0, 2, 1, 1, 0, 4, 1, 0, 1, 1, 1, 0, 0, 1, 3, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 3, 0, 1, 1, 2, 0, 0, 0, 2, 0, 1, 2, 0, 1, 2, 1, 1, 2, 1, 2, 0, 0, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 2, 3, 1, 1, 2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 2, 2, 0, 1, 1, 0, 2, 0, 1, 0, 0, 0, 2, 2, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 3, 1, 1, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 0, 0, 1, 1, 0, 2, 1, 3, 0, 2, 1, 1, 0, 1, 1, 1, 0, 0, 2, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 2, 0, 1, 1, 1, 2, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 2, 3, 0, 1, 2, 1, 1, 1, 0, 1, 2, 1, 0, 1, 0, 2, 1, 2, 0, 1, 1, 1, 1, 1, 2, 1, 0, 2, 1, 2, 3, 1, 3, 1, 0, 1, 0, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 3, 0, 1, 1, 0, 2, 1, 2, 2, 0, 2, 2, 2, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 3, 2, 0, 1, 1, 2, 0, 2, 1, 2, 1, 1, 1, 1, 0, 2, 2, 1, 2, 1, 1, 2, 1, 2, 0, 0, 0, 1, 3, 0, 1, 0, 2, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 3, 0, 0, 1, 2, 0, 2, 1, 0, 3, 0, 1, 0, 2, 1, 3, 0, 2, 1, 1, 0, 1, 1, 1, 1, 2, 0, 2, 1, 0, 1, 0, 0, 0, 2, 2, 1, 3, 1, 3, 2, 1, 1, 1, 0, 1, 0, 2, 1, 0, 0, 0, 3, 2, 1, 1, 0, 1, 1, 1, 1, 1, 2, 3, 1, 0, 3, 2, 2, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 3, 1, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 0, 2, 1, 0, 2, 1, 2, 3, 0, 0, 0, 1, 1, 0, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 1, 2, 1, 2, 3, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 1, 0, 2, 1, 2, 0, 2, 1, 1, 2, 1, 0, 0, 2, 1, 1, 2, 1, 2, 1, 3, 0, 1, 0, 1, 1, 0, 1, 2, 1, 0, 1, 0, 3, 1, 1, 1, 2, 0, 2, 1, 2, 2, 0, 2, 2, 0, 1, 2, 2, 0, 1, 0, 0, 2, 1, 2, 1, 2, 2, 0, 1, 2, 2, 0, 0, 2, 0, 1, 0, 2, 1, 1, 2, 1, 1, 2, 0, 0, 1, 2, 1, 0, 1, 2, 2, 0, 1, 0, 0, 0, 2, 1, 1, 2, 1, 1, 0, 2, 2, 2, 0, 1, 2, 1, 0, 1, 0, 2, 0, 0, 3, 1, 0, 0, 2, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 2, 1, 2, 1, 2, 1, 4, 2, 1, 2, 1, 1, 1, 2, 0, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 0, 2, 4, 0, 0, 3, 0, 0, 1, 1, 1, 0, 1, 1, 3, 2, 0, 1, 0, 1, 1, 0, 0, 3, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 3, 1, 1, 0, 0, 1, 1, 0, 1, 2, 0, 1, 2, 0, 3, 1, 0, 1, 1, 2, 2, 2, 2, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 4, 0, 1, 1, 2, 1, 0, 4, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 2, 0, 2, 2, 2, 3, 2, 1, 0, 2, 1, 1, 1, 1, 2, 2, 0, 1, 1, 0, 1, 3, 2, 1]\n\n\nAnd finally let’s visualize the sample. In order to do this, let us define a chart using the plotters crate.\n\n:dep plotters\nuse plotters::prelude::*;\n\nfn plot_histogram(samples: &[usize], filename: &str) {\n    let max_value = *samples.iter().max().unwrap_or(&0);\n    let frequencies = (0..=max_value)\n        .map(|value| samples.iter().filter(|&&x| x == value).count() as u32)\n        .collect::&lt;Vec&lt;_&gt;&gt;();\n\n    let root = BitMapBackend::new(filename, (1600, 1200)).into_drawing_area();\n    root.fill(&WHITE).unwrap();\n\n    let mut chart = ChartBuilder::on(&root)\n        .margin(10)\n        .x_label_area_size(40)\n        .y_label_area_size(40)\n        .caption(\"Sample Frequencies\", (\"sans-serif\", 30).into_font())\n        .build_cartesian_2d(0..max_value, 0..*frequencies.iter().max().unwrap_or(&0))\n        .unwrap();\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Sample Values\")\n        .y_desc(\"Frequencies\")\n        .draw()\n        .unwrap();\n\n    chart\n        .draw_series(\n            (0..=max_value)\n                .map(|x| {\n                    let freq = frequencies[x];\n                    Rectangle::new(\n                        [(x as usize, 0), ((x + 1) as usize, freq)],\n                        BLUE.filled(),\n                    )\n                }),\n        )\n        .unwrap()\n        .label(\"Frequency\")\n        .legend(|(x, y)| Rectangle::new([(x, y - 10), (x + 20, y)], BLUE.filled()));\n\n    chart\n        .configure_series_labels()\n        .border_style(&BLACK)\n        .background_style(&WHITE)\n        .draw()\n        .unwrap();\n}\n\nLet’s make the plot.\n\nplot_histogram(&samples, \"histogram.png\");\n\n\n\n\n\n\n\nFigure 1: Sample frequencies from a Poisson binomial distribution."
  },
  {
    "objectID": "posts/flask-dynamic-url/index.html",
    "href": "posts/flask-dynamic-url/index.html",
    "title": "Dynamic URLs in Flask",
    "section": "",
    "text": "In a previous post I showed how to start the most basic Flask web application. In this post I show a dynamic URL which renders content that depends on using the URL as user input. The mathematical function known as factorial is defined as\n\\[n! \\triangleq \\prod_{i=1}^n i\\] when \\(i \\geq 1\\) and when \\(n=0\\) then \\(0! \\triangleq 1\\). We will set a dynamic URL which takes an integer for which the factorial will be calculated and displayed. We can specify this by passing '/factorial/&lt;int:number&gt;'. The prefix /factorial/ is just to remind us that we’re calculating the factorial of a number. The angle brackets &lt;TYPE:...&gt; indicate that something is unspecified input in between the brackets with type TYPE. In this case we want &lt;int: ...&gt; because the factorial function (not to be confused with its generalizations, like the gamma function) is defined only for non-negative integers. It is natural to use number since we expect a number, but other names are possible for this.\nfrom math import factorial\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/factorial/&lt;int:number&gt;')\ndef calculate_factorial(number):\n    return str(factorial(number))\n\nif __name__ == '__main__':\n    app.run(debug=True)\nThat’s it. We’re just taking the number, calculating its factorial, casting the result to a string, and returning it. Flask takes care of the rest."
  },
  {
    "objectID": "posts/rust-execution-quarto-evcxr-jupyter/index.html",
    "href": "posts/rust-execution-quarto-evcxr-jupyter/index.html",
    "title": "evcxr_jupyter provides Rust execution in Quarto",
    "section": "",
    "text": "I recently learned that setting up Rust execution in Quarto is easy via evcxr_jupyter.\nThe first thing you need to do is ensure that Rust is installed. See Install Rust.\n\n\n\n\n\n\nTip\n\n\n\nIt doesn’t hurt to run rustup update if you have not updated in a while.\n\n\nThen you can use Cargo to install relevant crate. Expect this step to take a while.\n$ cargo install evcxr_jupyter\nThen, if necessary, add the executable evcxr_jupyter to PATH. On Debian-based Linux put the following in your .bashrc file\nexport PATH=$PATH:~/.cargo/bin/\nand run source .bashrc.\nThen use the binary to install the Jupyter tool.\nevcxr_jupyter --install\nAt this point it should be installed. You’ll see a Rust kernel will be available when you use jupyter notebook and Quarto will similarly recognize Rust code in {rust} as being executable. Start with something like a “Hello, World” example. You’ll find that in the Rust REPL you will not need to specify main {...}.\n\nprintln!(\"Hello, World.\");\n\nHello, World.\n\n\nYou can control the code echo by using //| echo: true and //| echo: false near the beginning of the code block."
  },
  {
    "objectID": "posts/python-ciw-simpy-clock/index.html",
    "href": "posts/python-ciw-simpy-clock/index.html",
    "title": "A Ciw Implementation of SimPy’s Clock Example",
    "section": "",
    "text": "This post looks at implementing a simple simulation described in the SImPy documentation using the Ciw Python package."
  },
  {
    "objectID": "posts/python-ciw-simpy-clock/index.html#introduction",
    "href": "posts/python-ciw-simpy-clock/index.html#introduction",
    "title": "A Ciw Implementation of SimPy’s Clock Example",
    "section": "",
    "text": "This post looks at implementing a simple simulation described in the SImPy documentation using the Ciw Python package."
  },
  {
    "objectID": "posts/python-ciw-simpy-clock/index.html#simpy-simulation",
    "href": "posts/python-ciw-simpy-clock/index.html#simpy-simulation",
    "title": "A Ciw Implementation of SimPy’s Clock Example",
    "section": "SimPy Simulation",
    "text": "SimPy Simulation\nOn the home page of the SimPy documentation there is an example of a clock:\n\nimport simpy\ndef clock(env, name, tick):\n    while True:\n        print(name, env.now)\n        yield env.timeout(tick)\n\nenv = simpy.Environment()\n\nenv.process(clock(env, 'fast', 0.5))\nenv.process(clock(env, 'slow', 1))\n\nenv.run(until=2)\n\nfast 0\nslow 0\nfast 0.5\nslow 1\nfast 1.0\nfast 1.5\n\n\nIt involves two clocks that tick deterministically at two distinct rates. One clock ticks at a rate of 0.5 units of time. The other clock ticks at 1 unit of time. This simulation runs for two units of time total."
  },
  {
    "objectID": "posts/python-ciw-simpy-clock/index.html#ciw-simulation",
    "href": "posts/python-ciw-simpy-clock/index.html#ciw-simulation",
    "title": "A Ciw Implementation of SimPy’s Clock Example",
    "section": "Ciw Simulation",
    "text": "Ciw Simulation\nLet’s do this in Ciw. First, we need to import Ciw, and I would also like Pandas for a later processing step.\n\nfrom typing import NoReturn\n\nimport ciw\nimport pandas\n\nWe ‘could’ just use the ciw.dists.Deterministic and a simple loop achieve the same output, but that would seem to ignore how Ciw is motivated. Ciw is for queueing models, so let’s keep to that aim. Instead of taking the easy way, let’s map our understanding of the clock problem into being a queueing network problem so that we can follow the typical Ciw workflow.\nLet us take each clock to be a node in a queueing network. We will consider the case of two clocks as per the original example.\nOur clocks are not self-contained like ordinary clocks. They require an input signal to tell them when to tick. We will call these discrete units of signal “packets”. The clocks and the inputs/outputs act as completely separate systems, so for our two clocks we can assume the following routing matrix:\n\\[R = \\begin{bmatrix}0 & 0 \\\\ 0 & 0 \\end{bmatrix}\\]\nwhich means that packets that are used at a clock are never seen again. They leave, or are consumed, or… something. 👻\nSuch a routing matrix can be tersely written in Python as a nested list:\n\nrouting = [[0.0] * 2] * 2\n\nThe servers at each clock are some component of the clock that make it tick, called a “ticker”. 😉 Each clock has only one ticker. For Ciw, this means that each node has only one server, which we can represent in a list:\n\nnumber_of_tickers = [1, 1]\n\nWe can treat each completed tick of these clocks as the completion of a service. Each clock completes a tick at an exact deterministic rate, so the inter-arrival times of ticks are constant random variables.We can think of these service times mathematically as\n\\[T_{\\text{service}} \\sim \\delta \\left( s \\right)\\]\nwhere \\(\\delta\\) is the Dirac delta distribution and \\(s\\) the “speed parameter” for the clock’s ticks. We can store our clock speeds in a list:\n\nclock_speeds = [0.5, 1]\n\nIn order to ensure that each ticker has a correctly-timed packet we must consider the arrival times of packets onto the queue. If we provide too few packets then the clock won’t keep time; ticks will be delayed. If we provide more packets than necessary then we will start to have a queue filling up with packets which is a waste of memory. Instead we should have packets arrive at the same rate that they are needed. Thus the arrival rate will also equal:\n\\[T_{\\text{arrivals}} \\sim \\delta \\left( s \\right)\\]\nBecause the order of the packets doesn’t matter, the service disciplinedoesn’t either. We’ll allow Ciw to use its default of first-come, first-serve, but it wouldn’t matter if we used something else.\nJust because we have the rates of change correct doesn’t mean that we are starting in the correct state. The last, and trickiest, business of this exercise is to start the system with the correct number of packets. If it takes \\(s\\) units of time for a packet to arrive and it takes another \\(s\\) units of time for a packet to be processed into a tick, then each packet has a sojourn time of \\(2s\\). Which means that at the start of the simulation there will be a delay before the clocks start ticking. We could chalk this up to simulation warm-up, but let’s get this right. What we can do is have a distribution that\n\\[f(t; s, c, \\ell) = \\begin{cases} \\delta \\left( 0 \\right) & t \\leq 0 \\land c &lt; \\ell \\\\ \\delta \\left( s \\right) & \\text{Otherwise} \\end{cases}\\]\nwhere \\(t\\) is the simulation time, \\(c \\in \\mathbb{N}_0\\) is a count of how many times the distribution has been used at or before \\(t = 0\\), and \\(\\ell \\in \\mathbb{N}_0\\) is the maximum number of times that we will allow this distribution to be used for \\(t \\leq 0\\). Kinda weird, right? What it specifies is that we can pass packets into the system that take zero units of time to arrive and zero units of time to process. That will allow us to have a couple of packets run straight away! One implementation of Python for this looks like this:\n\nclass IASDeterministic(ciw.dists.Distribution):\n    \"\"\"\n    Represents an \"instant at start\" (IAS) deterministic distribution.\n\n    This distribution generates values based on an initial value, and it can be limited\n    to a specified number of samples to be initially generated.\n\n    Parameters:\n        value (float): The fixed value to be returned by the distribution.\n        limit (int, optional): The maximum number of samples to be initially generated. Defaults to 1.\n\n    Attributes:\n        value (float): The fixed value to be returned by the distribution.\n        count (int): The current count of samples generated.\n        limit (int): The maximum number of samples to be initially generated.\n\n    Methods:\n        sample(t=None, ind=None):\n            Generates a sample from the distribution.\n\n    Example:\n        &gt;&gt;&gt; dist = IASDeterministic(value=3.14, limit=2)\n        &gt;&gt;&gt; dist.sample(0)\n        0\n        &gt;&gt;&gt; dist.sample(0)\n        0\n        &gt;&gt;&gt; dist.sample(0)\n        3.14\n    \"\"\"\n\n    def __init__(self, value: float, limit: float = 1) -&gt; NoReturn:\n        \"\"\"\n        Initializes the IASDeterministic distribution with the given parameters.\n\n        Parameters:\n            value (float): The fixed value to be returned by the distribution.\n            limit (int, optional): The maximum number of samples to be initially generated. Defaults to 1.\n        \"\"\"\n        self.value = value\n        self.count = 0\n        self.limit = limit\n\n    def sample(self, t:float = None, ind: ciw.Individual = None) -&gt; float:\n        \"\"\"\n        Generates a sample from the distribution.\n\n        Parameters:\n            t (float, optional): Time parameter, not used in this distribution.\n            ind (int, optional): Index parameter, not used in this distribution.\n\n        Returns:\n            float: The fixed value if conditions are met; otherwise, returns None.\n        \"\"\"\n        if t  &lt;= 0 and self.count &lt; self.limit:\n            self.count += 1\n            return 0\n        else:\n            return self.value\n\n    def __repr__(self):\n        return f\"IASDistribution(value={self.value}, limit={self.limit})\"\n\nOkay, that’s a decent chunk of code but it is mostly just docstrings. All it does is initialize the system as we discussed. We can make one of these for each node’s arrival and service distributions:\n\narrival_dists = [IASDeterministic(speed, 2) for speed in clock_speeds]\nservice_dists = [IASDeterministic(speed) for speed in clock_speeds]\n\nYou may have noticed that for arrival distributions that we put the limit at two packets rather than 1. This is because we need one packet to be processed at \\(t=0\\) but also have another ready at \\(t=0\\) to begin service using the post-initialization rate.\nIf you have not used Ciw before, you may be wondering how we put these pieces together. In Ciw everything about the design of the queueing network goes into the ciw.create_network function.\n\nnetwork = ciw.create_network(\n    arrival_distributions = arrival_dists,\n    service_distributions = service_dists,\n    number_of_servers = number_of_tickers,\n    routing = routing\n    )\n\nNow we can instantiate our simulation and run it for two units of time:\nsimulation = ciw.Simulation(network)\nsimulation.simulate_until_max_time(2)\nThe simulation instance has collected records about the completed packets. Let’s stick that in a Pandas dataframe, and then we’ll print out the results to a markdown table.\nfrom IPython.display import Markdown, display\n\nrecords = pd.DataFrame(simulation.get_all_records())\ndisplay(Markdown(records[['node', 'exit_date']].to_markdown(index=False)))\n\n\n\nnode\nexit_date\n\n\n\n\n1\n0\n\n\n2\n0\n\n\n1\n0.5\n\n\n2\n1\n\n\n1\n1\n\n\n1\n1.5\n\n\n\nNote that node 1 corresponds to the fast clock, and node 2 corresponds to the slow clock. That’s it. We did it."
  },
  {
    "objectID": "posts/python-ciw-simpy-clock/index.html#conclusions",
    "href": "posts/python-ciw-simpy-clock/index.html#conclusions",
    "title": "A Ciw Implementation of SimPy’s Clock Example",
    "section": "Conclusions",
    "text": "Conclusions\nCiw can definitely handle this toy simulation of a pair of clocks, but it is not the most naturally suited tool for this example. It is a bit like coordinating the BFG-10000 to take aim at a squirrel; more work than needed but gets the job done."
  },
  {
    "objectID": "posts/rust-gcd/index.html",
    "href": "posts/rust-gcd/index.html",
    "title": "Calculating the Greatest Common Divisor in Rust",
    "section": "",
    "text": "I recently learned that Rust does not have a greatest common divisor function. See Anonymous (2020) to read some discussion about it.\nSo if the GCD function is not implemented natively in Rust, how can we go about calculating it?\nOne option is to use the implementation available in Blandy and Orendorff (2021).\n\nfn gcd(mut n: u64, mut m: u64) -&gt; u64 {\n    assert!(n != 0 && m != 0);\n    while m != 0 {\n        if m &lt; n {\n            let t = m;\n            m = n;\n            n = t;\n        }\n        m = m % n;\n    }\n    n\n}\n\nlet x: u64 = 2024;\nlet y: u64 = 748;\nprintln!(\"gcd({}, {}) = {}\", x, y, gcd(x, y));\n\ngcd(2024, 748) = 44\n\n\nAnother option is that we could use the gcd crate.\n\n:dep gcd\nuse gcd::Gcd;\n\nprintln!(\"gcd({}, {}) = {}\", x, y, x.gcd(y));\n\ngcd(2024, 748) = 44\n\n\nOften I think going with the existing crate for work projects. There’s no need to reinvent such a simple wheel, and it supports a wider variety of types than the example from Blandy and Orendorff (2021). But it is also a great code example since calculating the GCD is a well-known, and relatively simple, task.\n\n\n\n\nReferences\n\nAnonymous. 2020. “Why No GCD in Standard Lib?” 2020. https://users.rust-lang.org/t/why-no-gcd-in-standard-lib/36490/7.\n\n\nBlandy, Jim, and Jason Orendorff. 2021. Programming Rust: Fast, Safe Systems Development. O’Reilly Media. https://www.amazon.ca/Programming-Rust-Fast-Systems-Development/dp/1491927283."
  },
  {
    "objectID": "posts/notes-order-theory/index.html",
    "href": "posts/notes-order-theory/index.html",
    "title": "Notes on Order Theory",
    "section": "",
    "text": "This section provides an overview of some fundamental concepts in order theory.\n\n\n\nDefinition\nA partial order is a pair \\((X, \\leq)\\) composed of a set \\(X\\) called the ground set and \\(\\leq\\) is a binary relation with the following relation: - reflexivity: \\(x \\leq x\\ \\forall x \\in X\\) - transitivity: \\(x \\leq y \\land y \\leq z \\implies x \\leq z\\  \\forall x,y,z \\in X\\) - antisymmtry: \\(x \\leq y \\land y \\leq x \\implies x = y\\  \\forall x,y \\in X\\)\n\nLet us consider an example of a partial order and some of the ways we can represent them.\n\nExample\nLet \\(X = \\{a, b, c, d, e \\}\\), then we might have a partial order \\(\\\\{(a,a), (a,c), (a,e), (b,b), (b,c), (b,d), (b,e), (c,c), (d,d), (d,e), (e,e)\\\\}\\). This partial order can be represented as a matrix.\n\\[\\begin{array}{c c} &\n\\begin{array}{c c c c c} a & b & c & d & e \\\\\n\\end{array}\n\\\\\n\\begin{array}{c c c c c}\na \\\\\nb \\\\\nc \\\\\nd \\\\\ne \\\\\n\\end{array}\n&\n\\left[\n\\begin{array}{c c c c c}\n1 & 0 & 1 & 0 & 1 \\\\\n& 1 & 1 & 1 & 1 \\\\\n&  & 1 & 0 & 0 \\\\\n& \\huge 0 &  & 1 & 1 \\\\\n&  &  &  & 1 \\\\\n\\end{array}\n\\right]\n\\end{array}\n\\]\nAnother representation is a directed graph (AKA a digraph):\n\n\nfrom graphviz import Digraph\nXxX = 'aa', 'ac', 'ae', 'bb', 'bc', 'bd', 'be', 'cc', 'dd', 'de', 'ee'\nD = Digraph('example_partial_order')\nfor xx in XxX:\n    D.edge(xx[0], xx[1])\nD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe previous example only required a relatively small subset of the Cartesian \\(X \\times X\\). When you a have too many elements in \\(X\\) to deal with by hand, one quick Python script to make the pairs in a format suitable for \\(\\LaTeX\\) is the following:\n\n\nfrom itertools import product\nX = 'a', 'b', 'c', 'd', 'e'\nXxX = ['('+', '.join(i) + ')' for i in product(X,X)]\nXxX = sorted(XxX)\nXxX = str(XxX).replace(\"'\", \"\")\nXxX = XxX.replace(\"[\", \"\\\\\\\\{\")\nXxX = XxX.replace(\"]\", \"\\\\\\\\}\")\nprint(XxX)\n\n\\\\{(a, a), (a, b), (a, c), (a, d), (a, e), (b, a), (b, b), (b, c), (b, d), (b, e), (c, a), (c, b), (c, c), (c, d), (c, e), (d, a), (d, b), (d, c), (d, d), (d, e), (e, a), (e, b), (e, c), (e, d), (e, e)\\\\}\n\n\n\n\n\nDefinition\nA strict partial order is a pair \\((X, &lt;)\\) composed of a set \\(X\\) called the ground set and \\(&lt;\\) is a binary relation with the following relation: - transitivity: \\(x &lt; y \\land y &lt; z \\implies x &lt; z\\  \\forall x,y,z \\in X\\) - asymmetry: \\(\\lnot (x &lt; y \\land y &lt; x) \\  \\forall x,y \\in X\\)\n\n\nProposition\nThe digraph representation of a strict order is a directed acyclic graph (DAG).\n\nOften we can start with a strict order and derive similar results for a corresponding (non-strict) partial order.\n\nDefinition\n\\(\\{x,  y\\}\\) are a comparable pair if \\(x &lt; y \\lor y &gt; x\\), denoted \\(x \\sim y\\).\n\n\nDefinition\nA graph \\(G = (V, E)\\) whose edge set \\(E\\) is the set of comparable pairs of a partial order is called the comparability graph.\n\n\nExample\nSuppose we have the strict partial order \\(\\\\{(a,c), (a,e), (b,c), (b,d), (b,e), (d,e)\\\\}\\), then its comparability graph would look like:\n\n\nflowchart TB\n    a o--o c\n    a o--o e\n    b o--o c\n    b o--o d\n    b o--o e\n    d o--o e\n\n\n\n\nflowchart TB\n    a o--o c\n    a o--o e\n    b o--o c\n    b o--o d\n    b o--o e\n    d o--o e\n\n\n\n\n\n\n\nDefinition\n\\(\\{x,  y\\}\\) are an incomparable pair if \\(\\lnot (x &lt; y \\lor y &gt; x)\\), denoted \\(x \\parallel y\\).\n\n\nDefinition\nA graph \\(G = (V, E)\\) whose edge set \\(E\\) is the set of incomparable pairs of a partial order is called the comparability graph.\n\n\nExample Suppose we have the strict partial order \\(\\\\{(a,c), (a,e), (b,c), (b,d), (b,e), (d,e)\\\\}\\), then its incomparability graph (AKA cocomparability graph) would look like:\n\n\nflowchart TB\n    a o--o b\n    a o--o d\n    c o--o d\n    c o--o e\n\n\n\n\nflowchart TB\n    a o--o b\n    a o--o d\n    c o--o d\n    c o--o e\n\n\n\n\n\n\n\nProposition\nThe edge set of an incomparability graph is the complement of the edge set of the comparability graph.\n\n\nDefinition\nA cover relation \\(x \\prec y\\) is satisfied when \\(x &lt; y\\) and there does not exist \\(z\\) such that \\(x &lt; z &lt; y\\).\n\n\nDefinition\nA graph \\(G = (V, E)\\) is a cover graph when its edge set \\(E\\) is a collection of pairs satisfying a cover relation.\n\n\nExample Suppose we have the strict partial order \\(\\\\{(a,c), (a,e), (b,c), (b,d), (b,e), (d,e)\\\\}\\), then its cover graph would look like:\n\n\nflowchart TB\n    a o--o c\n    a o--o e\n    b o--o c\n    b o--o d\n    d o--o e\n\n\n\n\nflowchart TB\n    a o--o c\n    a o--o e\n    b o--o c\n    b o--o d\n    d o--o e\n\n\n\n\n\n\n\nDefinition\nA directed graph \\(D = (V, E)\\) is a directed cover graph when its edge set \\(E\\) is a collection of pairs satisfying a cover relation and the order of the pairs is represented with arcs.\n\n\nExample\nSuppose we have the strict partial order \\(\\\\{(a,c), (a,e), (b,c), (b,d), (b,e), (d,e)\\\\}\\), then its directed cover graph would look like:\n\n\nflowchart TB\n    e --&gt; a\n    e --&gt; d\n    c --&gt; a\n    c --&gt; b\n    d --&gt; b\n\n\n\n\nflowchart TB\n    e --&gt; a\n    e --&gt; d\n    c --&gt; a\n    c --&gt; b\n    d --&gt; b\n\n\n\n\n\n\n\nDefinition\nA cover diagram is a drawing of the directed graph representing a cover relation such that the edges are cover pairs \\((x,y)\\). Edges are drawn in such a way that \\(x\\) is below \\(y\\) (in the graph embedding) ad the edge is \\(y\\)-monotone.\n\n\nExample\nSuppose we have the strict partial order \\(\\\\{(a,c), (a,e), (b,c), (b,d), (b,e), (d,e)\\\\}\\), then its cover diagram would look like:\n\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport warnings\n\ng = nx.Graph()\n\ng.add_edge('a', 'c')\ng.add_edge('a', 'e')\ng.add_edge('b', 'c')\ng.add_edge('b', 'd')\ng.add_edge('d', 'e')\n\npos = {'a':(0,1),\n       'b':(1,0),\n       'c':(0,3),\n       'd':(2,2),\n       'e':(1,4)}\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    nx.draw(g, pos=pos, labels={i:i for i in g.nodes()}, node_color=(0.5,)*3)\n\n\n\n\n\n\n\n\n\n\n\n\nlinear orders (aka total orders, aka chains).\n\nImportant in computer science problems such as sorting.\nImportant in mathematics for defining sets of numbers such as \\(\\mathbb{N}\\), \\(\\mathbb{Z}\\), \\(\\mathbb{R}\\) and others.\n\nBoolean lattices\n\n\\(B_n = (2^{[n]}, \\subseteq )\\) where \\[[n] = \\{ 1, \\ldots, n \\}\\]\nLooks at subsets of \\(n\\) ordered by inclusion\n\n\n\nProposition\nThe subset relation on any family of sets is an order relation.\n\n\nExample\nIntervals, discs, balls, subtrees, subgraphs, subgroups are all included sets under the order relation of set inclusion.\n\n\nDefinition\nA containment relation is […]\n\n\nDefinition\nFor \\(A \\subseteq X\\) we have the down-set of \\(A\\) being \\[D[A] = \\bigcup_{a \\in A} D[a] = \\{ x | \\exists a \\in A\\ \\text{s.t.}\\ x \\leq a \\}\\].\n\n\nProposition\nEvery finite order is the containment order of a family of sets.\nProof Let \\(P = (X, \\leq)\\) be an order relation.\nFor \\(a \\in X\\) let \\(D[a] = \\{ x : x \\leq a \\}\\) be the down-set of a.\nLet \\[y = \\{ D[a] : a \\in X \\}\\].\nNow we claim that \\(P \\cong (y, \\subseteq )\\) where \\(\\cong\\) is an order-preserving isomorphism.\nSince isomorphisms are bijections, we take the bijection in this case to \\(D : x \\mapsto D[x]\\).\nWe need to verify that \\(D\\) as we have just defined it is actually a bijection.\nIf a mapping is a bijection, then it is both injective and surjective.\nConsidering the injective case, let take two elements \\(x\\) and \\(y\\) such that \\(x \\neq y\\) then there are three possibilities: - \\(x &lt; y \\implies y \\not\\in D[x]\\) - \\(x &gt; y \\implies x \\not\\in D[y]\\) - \\(x \\parallel y\\) (i.e. \\(x\\) and \\(y\\) are not comparable in the order) which implies \\((y \\not\\in D[x]) \\land (x \\not\\in D[y])\\)\nSurjectivity is given by definition of \\(D\\).\nAnd we have that \\(D\\) is order-preserving in the sense that \\(x \\leq y \\iff D[x] \\subseteq D[y]\\). \\[\\blacksquare\\]\n\n\nDefinition\nA lower bound of a subset \\(S\\) of partially ordered set \\((P, \\leq)\\) is an element \\(a \\in P\\) such that \\(a \\leq x \\forall x \\in S\\).\n\n\nDefinition\nA lower bound \\(a \\in S\\) is called an infinum (or greatest lower bound, or meet) of \\(S\\) if for all lower bounds \\(y \\in S\\) we have \\(y \\leq a\\).\n\n\nDefinition\nAn upper bound of a subset \\(S\\) of partially ordered set \\((P, \\leq)\\) is an element \\(b \\in P\\) such that \\(b \\geq x \\forall x \\in S\\).\n\n\nDefinition\nAn upper bound \\(b\\) of \\(S\\) is called a supremum (or least upper bound, or join) of \\(S\\) if for all upper bounds \\(z \\in S\\) we have \\(z \\leq b\\).\n\n\nDefinition\nA finite poset on \\(S\\) is a lattice if every subset of \\(S\\) has an unique least upper bound (aka join or supremum) denoted \\(\\lor\\) and an unique greatest lower bound (aka meet or infinum) denoted \\(\\land\\).\n\n\nDefinition\nThe down-set lattice of \\(P\\) given by \\[D(P) = (\\{ D[A] : A \\subset X \\}, \\subseteq)\\].\n\n\nDefinition\nA lattice \\((L, \\lor, \\land)\\) is distributive if the following identity holds for all \\(x,y,z \\in L\\):\n\\[x \\land (y \\lor z) = (x \\land y) \\lor (x \\land z)\\]\nwhere \\(\\land\\) and \\(\\lor\\) are the meet and join operations.\n\n\nProposition\nThe down-set lattice of \\(P\\) is a distributive lattice.\nProof (Hint) Take \\(\\lor\\) to be \\(\\cup\\) and \\(\\land\\) to be \\(\\cap\\).\n\n\nDefinition\n\\(Q = (Y, \\leq_Q )\\) is a subposet of \\(P = (X, \\leq_P)\\) if \\(Y \\subseteq X\\) and \\((\\leq_Q) = (\\leq_P) \\cap (Y \\times Y)\\)."
  },
  {
    "objectID": "posts/notes-order-theory/index.html#essential-definitions-and-examples",
    "href": "posts/notes-order-theory/index.html#essential-definitions-and-examples",
    "title": "Notes on Order Theory",
    "section": "",
    "text": "Definition\nA partial order is a pair \\((X, \\leq)\\) composed of a set \\(X\\) called the ground set and \\(\\leq\\) is a binary relation with the following relation: - reflexivity: \\(x \\leq x\\ \\forall x \\in X\\) - transitivity: \\(x \\leq y \\land y \\leq z \\implies x \\leq z\\  \\forall x,y,z \\in X\\) - antisymmtry: \\(x \\leq y \\land y \\leq x \\implies x = y\\  \\forall x,y \\in X\\)\n\nLet us consider an example of a partial order and some of the ways we can represent them.\n\nExample\nLet \\(X = \\{a, b, c, d, e \\}\\), then we might have a partial order \\(\\\\{(a,a), (a,c), (a,e), (b,b), (b,c), (b,d), (b,e), (c,c), (d,d), (d,e), (e,e)\\\\}\\). This partial order can be represented as a matrix.\n\\[\\begin{array}{c c} &\n\\begin{array}{c c c c c} a & b & c & d & e \\\\\n\\end{array}\n\\\\\n\\begin{array}{c c c c c}\na \\\\\nb \\\\\nc \\\\\nd \\\\\ne \\\\\n\\end{array}\n&\n\\left[\n\\begin{array}{c c c c c}\n1 & 0 & 1 & 0 & 1 \\\\\n& 1 & 1 & 1 & 1 \\\\\n&  & 1 & 0 & 0 \\\\\n& \\huge 0 &  & 1 & 1 \\\\\n&  &  &  & 1 \\\\\n\\end{array}\n\\right]\n\\end{array}\n\\]\nAnother representation is a directed graph (AKA a digraph):\n\n\nfrom graphviz import Digraph\nXxX = 'aa', 'ac', 'ae', 'bb', 'bc', 'bd', 'be', 'cc', 'dd', 'de', 'ee'\nD = Digraph('example_partial_order')\nfor xx in XxX:\n    D.edge(xx[0], xx[1])\nD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe previous example only required a relatively small subset of the Cartesian \\(X \\times X\\). When you a have too many elements in \\(X\\) to deal with by hand, one quick Python script to make the pairs in a format suitable for \\(\\LaTeX\\) is the following:\n\n\nfrom itertools import product\nX = 'a', 'b', 'c', 'd', 'e'\nXxX = ['('+', '.join(i) + ')' for i in product(X,X)]\nXxX = sorted(XxX)\nXxX = str(XxX).replace(\"'\", \"\")\nXxX = XxX.replace(\"[\", \"\\\\\\\\{\")\nXxX = XxX.replace(\"]\", \"\\\\\\\\}\")\nprint(XxX)\n\n\\\\{(a, a), (a, b), (a, c), (a, d), (a, e), (b, a), (b, b), (b, c), (b, d), (b, e), (c, a), (c, b), (c, c), (c, d), (c, e), (d, a), (d, b), (d, c), (d, d), (d, e), (e, a), (e, b), (e, c), (e, d), (e, e)\\\\}\n\n\n\n\n\nDefinition\nA strict partial order is a pair \\((X, &lt;)\\) composed of a set \\(X\\) called the ground set and \\(&lt;\\) is a binary relation with the following relation: - transitivity: \\(x &lt; y \\land y &lt; z \\implies x &lt; z\\  \\forall x,y,z \\in X\\) - asymmetry: \\(\\lnot (x &lt; y \\land y &lt; x) \\  \\forall x,y \\in X\\)\n\n\nProposition\nThe digraph representation of a strict order is a directed acyclic graph (DAG).\n\nOften we can start with a strict order and derive similar results for a corresponding (non-strict) partial order.\n\nDefinition\n\\(\\{x,  y\\}\\) are a comparable pair if \\(x &lt; y \\lor y &gt; x\\), denoted \\(x \\sim y\\).\n\n\nDefinition\nA graph \\(G = (V, E)\\) whose edge set \\(E\\) is the set of comparable pairs of a partial order is called the comparability graph.\n\n\nExample\nSuppose we have the strict partial order \\(\\\\{(a,c), (a,e), (b,c), (b,d), (b,e), (d,e)\\\\}\\), then its comparability graph would look like:\n\n\nflowchart TB\n    a o--o c\n    a o--o e\n    b o--o c\n    b o--o d\n    b o--o e\n    d o--o e\n\n\n\n\nflowchart TB\n    a o--o c\n    a o--o e\n    b o--o c\n    b o--o d\n    b o--o e\n    d o--o e\n\n\n\n\n\n\n\nDefinition\n\\(\\{x,  y\\}\\) are an incomparable pair if \\(\\lnot (x &lt; y \\lor y &gt; x)\\), denoted \\(x \\parallel y\\).\n\n\nDefinition\nA graph \\(G = (V, E)\\) whose edge set \\(E\\) is the set of incomparable pairs of a partial order is called the comparability graph.\n\n\nExample Suppose we have the strict partial order \\(\\\\{(a,c), (a,e), (b,c), (b,d), (b,e), (d,e)\\\\}\\), then its incomparability graph (AKA cocomparability graph) would look like:\n\n\nflowchart TB\n    a o--o b\n    a o--o d\n    c o--o d\n    c o--o e\n\n\n\n\nflowchart TB\n    a o--o b\n    a o--o d\n    c o--o d\n    c o--o e\n\n\n\n\n\n\n\nProposition\nThe edge set of an incomparability graph is the complement of the edge set of the comparability graph.\n\n\nDefinition\nA cover relation \\(x \\prec y\\) is satisfied when \\(x &lt; y\\) and there does not exist \\(z\\) such that \\(x &lt; z &lt; y\\).\n\n\nDefinition\nA graph \\(G = (V, E)\\) is a cover graph when its edge set \\(E\\) is a collection of pairs satisfying a cover relation.\n\n\nExample Suppose we have the strict partial order \\(\\\\{(a,c), (a,e), (b,c), (b,d), (b,e), (d,e)\\\\}\\), then its cover graph would look like:\n\n\nflowchart TB\n    a o--o c\n    a o--o e\n    b o--o c\n    b o--o d\n    d o--o e\n\n\n\n\nflowchart TB\n    a o--o c\n    a o--o e\n    b o--o c\n    b o--o d\n    d o--o e\n\n\n\n\n\n\n\nDefinition\nA directed graph \\(D = (V, E)\\) is a directed cover graph when its edge set \\(E\\) is a collection of pairs satisfying a cover relation and the order of the pairs is represented with arcs.\n\n\nExample\nSuppose we have the strict partial order \\(\\\\{(a,c), (a,e), (b,c), (b,d), (b,e), (d,e)\\\\}\\), then its directed cover graph would look like:\n\n\nflowchart TB\n    e --&gt; a\n    e --&gt; d\n    c --&gt; a\n    c --&gt; b\n    d --&gt; b\n\n\n\n\nflowchart TB\n    e --&gt; a\n    e --&gt; d\n    c --&gt; a\n    c --&gt; b\n    d --&gt; b\n\n\n\n\n\n\n\nDefinition\nA cover diagram is a drawing of the directed graph representing a cover relation such that the edges are cover pairs \\((x,y)\\). Edges are drawn in such a way that \\(x\\) is below \\(y\\) (in the graph embedding) ad the edge is \\(y\\)-monotone.\n\n\nExample\nSuppose we have the strict partial order \\(\\\\{(a,c), (a,e), (b,c), (b,d), (b,e), (d,e)\\\\}\\), then its cover diagram would look like:\n\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport warnings\n\ng = nx.Graph()\n\ng.add_edge('a', 'c')\ng.add_edge('a', 'e')\ng.add_edge('b', 'c')\ng.add_edge('b', 'd')\ng.add_edge('d', 'e')\n\npos = {'a':(0,1),\n       'b':(1,0),\n       'c':(0,3),\n       'd':(2,2),\n       'e':(1,4)}\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    nx.draw(g, pos=pos, labels={i:i for i in g.nodes()}, node_color=(0.5,)*3)"
  },
  {
    "objectID": "posts/notes-order-theory/index.html#special-classes-of-orders",
    "href": "posts/notes-order-theory/index.html#special-classes-of-orders",
    "title": "Notes on Order Theory",
    "section": "",
    "text": "linear orders (aka total orders, aka chains).\n\nImportant in computer science problems such as sorting.\nImportant in mathematics for defining sets of numbers such as \\(\\mathbb{N}\\), \\(\\mathbb{Z}\\), \\(\\mathbb{R}\\) and others.\n\nBoolean lattices\n\n\\(B_n = (2^{[n]}, \\subseteq )\\) where \\[[n] = \\{ 1, \\ldots, n \\}\\]\nLooks at subsets of \\(n\\) ordered by inclusion\n\n\n\nProposition\nThe subset relation on any family of sets is an order relation.\n\n\nExample\nIntervals, discs, balls, subtrees, subgraphs, subgroups are all included sets under the order relation of set inclusion.\n\n\nDefinition\nA containment relation is […]\n\n\nDefinition\nFor \\(A \\subseteq X\\) we have the down-set of \\(A\\) being \\[D[A] = \\bigcup_{a \\in A} D[a] = \\{ x | \\exists a \\in A\\ \\text{s.t.}\\ x \\leq a \\}\\].\n\n\nProposition\nEvery finite order is the containment order of a family of sets.\nProof Let \\(P = (X, \\leq)\\) be an order relation.\nFor \\(a \\in X\\) let \\(D[a] = \\{ x : x \\leq a \\}\\) be the down-set of a.\nLet \\[y = \\{ D[a] : a \\in X \\}\\].\nNow we claim that \\(P \\cong (y, \\subseteq )\\) where \\(\\cong\\) is an order-preserving isomorphism.\nSince isomorphisms are bijections, we take the bijection in this case to \\(D : x \\mapsto D[x]\\).\nWe need to verify that \\(D\\) as we have just defined it is actually a bijection.\nIf a mapping is a bijection, then it is both injective and surjective.\nConsidering the injective case, let take two elements \\(x\\) and \\(y\\) such that \\(x \\neq y\\) then there are three possibilities: - \\(x &lt; y \\implies y \\not\\in D[x]\\) - \\(x &gt; y \\implies x \\not\\in D[y]\\) - \\(x \\parallel y\\) (i.e. \\(x\\) and \\(y\\) are not comparable in the order) which implies \\((y \\not\\in D[x]) \\land (x \\not\\in D[y])\\)\nSurjectivity is given by definition of \\(D\\).\nAnd we have that \\(D\\) is order-preserving in the sense that \\(x \\leq y \\iff D[x] \\subseteq D[y]\\). \\[\\blacksquare\\]\n\n\nDefinition\nA lower bound of a subset \\(S\\) of partially ordered set \\((P, \\leq)\\) is an element \\(a \\in P\\) such that \\(a \\leq x \\forall x \\in S\\).\n\n\nDefinition\nA lower bound \\(a \\in S\\) is called an infinum (or greatest lower bound, or meet) of \\(S\\) if for all lower bounds \\(y \\in S\\) we have \\(y \\leq a\\).\n\n\nDefinition\nAn upper bound of a subset \\(S\\) of partially ordered set \\((P, \\leq)\\) is an element \\(b \\in P\\) such that \\(b \\geq x \\forall x \\in S\\).\n\n\nDefinition\nAn upper bound \\(b\\) of \\(S\\) is called a supremum (or least upper bound, or join) of \\(S\\) if for all upper bounds \\(z \\in S\\) we have \\(z \\leq b\\).\n\n\nDefinition\nA finite poset on \\(S\\) is a lattice if every subset of \\(S\\) has an unique least upper bound (aka join or supremum) denoted \\(\\lor\\) and an unique greatest lower bound (aka meet or infinum) denoted \\(\\land\\).\n\n\nDefinition\nThe down-set lattice of \\(P\\) given by \\[D(P) = (\\{ D[A] : A \\subset X \\}, \\subseteq)\\].\n\n\nDefinition\nA lattice \\((L, \\lor, \\land)\\) is distributive if the following identity holds for all \\(x,y,z \\in L\\):\n\\[x \\land (y \\lor z) = (x \\land y) \\lor (x \\land z)\\]\nwhere \\(\\land\\) and \\(\\lor\\) are the meet and join operations.\n\n\nProposition\nThe down-set lattice of \\(P\\) is a distributive lattice.\nProof (Hint) Take \\(\\lor\\) to be \\(\\cup\\) and \\(\\land\\) to be \\(\\cap\\).\n\n\nDefinition\n\\(Q = (Y, \\leq_Q )\\) is a subposet of \\(P = (X, \\leq_P)\\) if \\(Y \\subseteq X\\) and \\((\\leq_Q) = (\\leq_P) \\cap (Y \\times Y)\\)."
  },
  {
    "objectID": "posts/my-quarto-blog/index.html",
    "href": "posts/my-quarto-blog/index.html",
    "title": "My Quarto Blog",
    "section": "",
    "text": "I am switching to Quarto for my blog. My Jekyll blog is available here, and I might move some of the posts over to the new blog over time.\nCreating this blog was really easy. Was my Jekyll blog really complicated to setup? There were some technical hurdles around getting extra behaviour on my original blog, but overall it wasn’t extremely hard. Why am I switching to Quarto?\nWhat Quarto provides (that I want) is code execution followed by rendering the output of the code. I can put my code examples right into the blog post, and if something like a plot is produced then that plot will show on my blog.\nIn constrast, with Jekyll, I needed to\n\nmake the plot\nmove the plot to an images folder\nreference to the plot’s path in the blog post.\n\nIt wasn’t terrible, and it is possible that I just didn’t figure out how to make this easier with Jekyll, but it was quickly apparent to me that Quarto makes this easy.\nThis includes mermaid diagrams:\n\n\n\n\n\n---\ntitle: Example Git diagram\n---\ngitGraph\n   commit\n   commit\n   branch develop\n   checkout develop\n   commit\n   commit\n   checkout main\n   merge develop\n   commit\n   commit\n\n\n\n\n\n\nI can also easily prepare Graphiz diagrams provided that I supply some valid dot notation:\n\ndigraph finite_state_machine {\n    fontname=\"Helvetica,Arial,sans-serif\"\n    node [fontname=\"Helvetica,Arial,sans-serif\"]\n    edge [fontname=\"Helvetica,Arial,sans-serif\"]\n    rankdir=LR;\n    node [shape = doublecircle]; 0 3 4 8;\n    node [shape = circle];\n    0 -&gt; 2 [label = \"SS(B)\"];\n    0 -&gt; 1 [label = \"SS(S)\"];\n    1 -&gt; 3 [label = \"S($end)\"];\n    2 -&gt; 6 [label = \"SS(b)\"];\n    2 -&gt; 5 [label = \"SS(a)\"];\n    2 -&gt; 4 [label = \"S(A)\"];\n    5 -&gt; 7 [label = \"S(b)\"];\n    5 -&gt; 5 [label = \"S(a)\"];\n    6 -&gt; 6 [label = \"S(b)\"];\n    6 -&gt; 5 [label = \"S(a)\"];\n    7 -&gt; 8 [label = \"S(b)\"];\n    7 -&gt; 5 [label = \"S(a)\"];\n    8 -&gt; 6 [label = \"S(b)\"];\n    8 -&gt; 5 [label = \"S(a)\"];\n}\n\n\n\n\n\n\nfinite_state_machine\n\n\n\n0\n\n\n0\n\n\n\n2\n\n2\n\n\n\n0-&gt;2\n\n\nSS(B)\n\n\n\n1\n\n1\n\n\n\n0-&gt;1\n\n\nSS(S)\n\n\n\n3\n\n\n3\n\n\n\n4\n\n\n4\n\n\n\n8\n\n\n8\n\n\n\n6\n\n6\n\n\n\n8-&gt;6\n\n\nS(b)\n\n\n\n5\n\n5\n\n\n\n8-&gt;5\n\n\nS(a)\n\n\n\n2-&gt;4\n\n\nS(A)\n\n\n\n2-&gt;6\n\n\nSS(b)\n\n\n\n2-&gt;5\n\n\nSS(a)\n\n\n\n1-&gt;3\n\n\nS($end)\n\n\n\n6-&gt;6\n\n\nS(b)\n\n\n\n6-&gt;5\n\n\nS(a)\n\n\n\n5-&gt;5\n\n\nS(a)\n\n\n\n7\n\n7\n\n\n\n5-&gt;7\n\n\nS(b)\n\n\n\n7-&gt;8\n\n\nS(b)\n\n\n\n7-&gt;5\n\n\nS(a)\n\n\n\n\n\n\n\n\nThis is definitely desired behaviour."
  },
  {
    "objectID": "posts/painting-7/index.html",
    "href": "posts/painting-7/index.html",
    "title": "Painting 7",
    "section": "",
    "text": "I finished my 7th painting!\n\nThere are certaintly a lot of flaws in this painting, but I am glad I got to the end of it. A few things went wrong on me.\nIf you look carefully at the trees in the background you’ll see that some of brush strokes are at a strange angle. That is in part because my brush it itself angled, and that I didn’t compensate for that. But that is kind of subtle.\nThe main thing that sticks out like a sore thumb is that deciduous tree on the left-hand-side. I was told that it competes with the mountains in the background, which makes sense as a critique of the composition. I also feel that the shape of the collections of leaves don’t look right to me, especially the bottom-right branch.\nThere are also tell-tale signs of a disaster earlier in the painting. I accidentally sprayed, yes, sprayed, paint thinner. It wasn’t noticable at first, but I later noticed lots of white dots. I had attempted to use my palette knife to scape away what I could and added more liquid white to attempt to recoat the bottom-half of the painting. This seemed to make this worse, so later while the painting was closer to dry I attempted to paint over some of these blemishes and scrapes. But you can still see some of them if you look carefully.\nAnd there are many other little issues, but overall I think a lot of things went well. Compared to my previous zero-ability to paint, this work honestly amazes me. And it is remarkable how well it turned out even after my many mistakes.\nHope you enjoyed it."
  },
  {
    "objectID": "posts/rust-petgraph-dot-example/index.html",
    "href": "posts/rust-petgraph-dot-example/index.html",
    "title": "Rendering a Dot Representation of a Graph in Quarto Using Petgraph and Petgraph-evcxr",
    "section": "",
    "text": "In this post I want to quickly showcase the use of the use of the Petgraph and Petgraph-evcxr packages. Petgraph is a popular graph data structures and algorithms crate available for Rust. Petgraph-evcxr is a tool that makes it possible to easily visualize graphs created using Petgraph within Jupyter notebooks. This in turn allows us to visualize such graphs within Quarto.\nThis we need to include them as dependencies.\n\n:dep petgraph = \"*\"\n:dep petgraph-evcxr = \"*\"\n\nextern crate petgraph;\nuse petgraph::graph::Graph;\nuse petgraph::dot::Dot;\nuse petgraph_evcxr::{draw_graph, draw_dot};\n\nThen we define the graph.\n\nlet mut g : Graph&lt;&str, &str&gt; = Graph::new();\nlet rock = g.add_node(\"Rock\");\nlet paper = g.add_node(\"Paper\");\nlet scissors = g.add_node(\"Scissors\");\n\ng.add_edge(rock, scissors, \"beats\");\ng.add_edge(paper, rock, \"beats\");\ng.add_edge(scissors, paper, \"beats\");\n\nFinally, we call draw the graph.\n\ndraw_graph(&g);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMermaid is an easy alternative\n\n\n\nOne should be aware of the fact that it is easy to produce similar diagrams with Mermaid.\n\nflowchart TD\n\nRock --&gt;|Beats| Scissors\nPaper --&gt;|Beats| Rock\nScissors --&gt;|Beats| Paper\n\n\n\n\nflowchart TD\n\nRock --&gt;|Beats| Scissors\nPaper --&gt;|Beats| Rock\nScissors --&gt;|Beats| Paper\n\n\n\n\n\n\nPerhaps a small advantage of using Petgraph here is that you can programmatically generate the properties of the diagram, as other tools such as networkx would let us do. Above I just wanted to show that we ‘can’ produce such diagrams starting with Rust."
  },
  {
    "objectID": "posts/combining-kedro-with-rye/index.html",
    "href": "posts/combining-kedro-with-rye/index.html",
    "title": "Combining Kedro with Rye",
    "section": "",
    "text": "I recently asked on the Kedro Slack channel about what experience people have had with combining Kedro with package management tools in Python such as PDM, Poetry, Hatch or Rye. juanlu gave a couple of options. You can either initialize a Kedro project first and then add the package manager, or add the package manager first and use kedro-init to fill in a Kedro project. Which one is more appropriate will depend on what already exists in your project. Kedro should be compatible with PEP-compliant packages (see discussion here) and also Poetry. I’m not sure about Rye.\nkedro-init is in its infancy (e.g. still needing documentation), but I figured I would try it out with Rye since that is what I am currently using on my personal machine."
  },
  {
    "objectID": "posts/combining-kedro-with-rye/index.html#introduction",
    "href": "posts/combining-kedro-with-rye/index.html#introduction",
    "title": "Combining Kedro with Rye",
    "section": "",
    "text": "I recently asked on the Kedro Slack channel about what experience people have had with combining Kedro with package management tools in Python such as PDM, Poetry, Hatch or Rye. juanlu gave a couple of options. You can either initialize a Kedro project first and then add the package manager, or add the package manager first and use kedro-init to fill in a Kedro project. Which one is more appropriate will depend on what already exists in your project. Kedro should be compatible with PEP-compliant packages (see discussion here) and also Poetry. I’m not sure about Rye.\nkedro-init is in its infancy (e.g. still needing documentation), but I figured I would try it out with Rye since that is what I am currently using on my personal machine."
  },
  {
    "objectID": "posts/combining-kedro-with-rye/index.html#example",
    "href": "posts/combining-kedro-with-rye/index.html#example",
    "title": "Combining Kedro with Rye",
    "section": "Example",
    "text": "Example\nFirst, lets initialize a Rye-managed project called try-kedro-init.\n$ rye init try-kedro-init\nsuccess: Initialized project in /home/galen/projects/try-kedro-init\n  Run `rye sync` to get started\nNow change directory into the project path.\n$ cd try-kedro-init/\nAdd the kedro-init package to try-kedro-init’s packages, inlcuding Kedro itself.\n$ rye add kedro-init\nInitializing new virtualenv in /home/galen/projects/try-kedro-init/.venv\nPython version: cpython@3.12.3\nAdded kedro-init&gt;=0.1.0 as regular dependency\nReusing already existing virtualenv\nGenerating production lockfile: /home/galen/projects/try-kedro-init/requirements.lock\nGenerating dev lockfile: /home/galen/projects/try-kedro-init/requirements-dev.lock\nInstalling dependencies\nResolved 55 packages in 12ms\n   Built try-kedro-init @ file:///home/galen/projects/try-kedro-init\n   Built antlr4-python3-runtime==4.9.3\nDownloaded 35 packages in 3.23s\nInstalled 55 packages in 16ms\n + antlr4-python3-runtime==4.9.3\n + arrow==1.3.0\n + attrs==23.2.0\n + binaryornot==0.4.4\n + build==1.2.1\n + cachetools==5.3.3\n + certifi==2024.6.2\n + chardet==5.2.0\n + charset-normalizer==3.3.2\n + click==8.1.7\n + cookiecutter==2.6.0\n + dynaconf==3.2.5\n + fastjsonschema==2.20.0\n + fsspec==2024.6.1\n + gitdb==4.0.11\n + gitpython==3.1.43\n + idna==3.7\n + importlib-metadata==7.2.1\n + importlib-resources==6.4.0\n + installer==0.7.0\n + jinja2==3.1.4\n + kedro==0.19.6\n + kedro-init==0.1.0\n + markdown-it-py==3.0.0\n + markupsafe==2.1.5\n + mdurl==0.1.2\n + more-itertools==10.3.0\n + omegaconf==2.3.0\n + packaging==24.1\n + parse==1.20.2\n + platformdirs==4.2.2\n + pluggy==1.5.0\n + pre-commit-hooks==4.6.0\n + pygetimportables==0.2.1\n + pygments==2.18.0\n + pyproject-hooks==1.1.0\n + python-dateutil==2.9.0.post0\n + python-slugify==8.0.4\n + pytoolconfig==1.3.1\n + pyyaml==6.0.1\n + requests==2.32.3\n + rich==13.7.1\n + rope==1.13.0\n + ruamel-yaml==0.18.6\n + ruamel-yaml-clib==0.2.8\n + six==1.16.0\n + smmap==5.0.1\n + text-unidecode==1.3\n + toml==0.10.2\n + tomlkit==0.12.5\n + try-kedro-init==0.1.0 (from file:///home/galen/projects/try-kedro-init)\n + types-python-dateutil==2.9.0.20240316\n + urllib3==2.2.2\n + validate-pyproject==0.18\n + zipp==3.19.2\nDone!\nNow run kedro-init from within Rye’s virtual environment.\n$ rye run kedro-init .\n[08:33:20] Looking for existing package directories                                                                                                                                                                                cli.py:25\n[08:33:25] Initialising config directories                                                                                                                                                                                         cli.py:25\n           Creating modules                                                                                                                                                                                                        cli.py:25\n           🔶 Kedro project successfully initialised!\nJust for the sake of example, create an example pipeline.\n$ rye run kedro pipeline create example_pipeline\nUsing pipeline template at: '/home/galen/projects/try-kedro-init/.venv/lib/python3.12/site-packages/kedro/templates/pipeline'\nCreating the pipeline 'example_pipeline': OK\n  Location: '/home/galen/projects/try-kedro-init/src/try_kedro_init/pipelines/example_pipeline'\nCreating '/home/galen/projects/try-kedro-init/tests/pipelines/example_pipeline/test_pipeline.py': OK\nCreating '/home/galen/projects/try-kedro-init/tests/pipelines/example_pipeline/__init__.py': OK\nCreating '/home/galen/projects/try-kedro-init/conf/base/parameters_example_pipeline.yml': OK\n\nPipeline 'example_pipeline' was successfully created.\nNow take a look at the path tree to see what has been created.\n$ tree .\n.\n├── conf\n│   ├── base\n│   │   └── parameters_example_pipeline.yml\n│   └── local\n├── pyproject.toml\n├── README.md\n├── requirements-dev.lock\n├── requirements.lock\n├── src\n│   └── try_kedro_init\n│       ├── __init__.py\n│       ├── pipeline_registry.py\n│       ├── pipelines\n│       │   └── example_pipeline\n│       │       ├── __init__.py\n│       │       ├── nodes.py\n│       │       └── pipeline.py\n│       ├── __pycache__\n│       │   ├── __init__.cpython-312.pyc\n│       │   └── settings.cpython-312.pyc\n│       └── settings.py\n└── tests\n    └── pipelines\n        └── example_pipeline\n            ├── __init__.py\n            └── test_pipeline.py\n\n11 directories, 15 files\nThe catalog.yml and parameters.yml files were not made by default, but they are just plaintext files that can be readily added. There is parameters_example_pipeline.yml for the pipeline we just created.\n$ touch conf/base/catalog.yml\nThere also is not a data path by default, which should exist at the root of the project. We can also add that.\n $ mkdir data\nLet us create an example CSV dataset at data/example_data.csv with the following contents:\nID,Name,Age,Email\n1,John Doe,28,john.doe@example.com\n2,Jane Smith,34,jane.smith@example.com\n3,Bob Johnson,45,bob.johnson@example.com\n4,Alice Williams,23,alice.williams@example.com\n5,Michael Brown,37,michael.brown@example.com\nThen add an entry to conf/base/catalog.yml:\nexample_dataset:\n  type: pandas.CSVDataset\n  filepath: ./data/example_data.csv\n  load_args:\n    sep: \",\"\nNow update src/try_kedro_init/pipelines/example_pipeline/pipeline.py from this\n\"\"\"\nThis is a boilerplate pipeline 'example_pipeline'\ngenerated using Kedro 0.19.6\n\"\"\"\n\nfrom kedro.pipeline import Pipeline, pipeline\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([])\nto this:\n\"\"\"\nThis is a boilerplate pipeline 'example_pipeline'\ngenerated using Kedro 0.19.6\n\"\"\"\n\nfrom kedro.pipeline import Pipeline, pipeline, node\n\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([\n        node(\n            func=print,\n            inputs=['example_dataset'],\n            outputs=None\n            )\n        ])\nNow install kedro-datasets and pandas:\n$ rye add kedro-datasets pandas\nAdded kedro-datasets&gt;=3.0.1 as regular dependency\nAdded pandas&gt;=2.2.2 as regular dependency\nReusing already existing virtualenv\nGenerating production lockfile: /home/galen/projects/try-kedro-init/requirements.lock\nGenerating dev lockfile: /home/galen/projects/try-kedro-init/requirements-dev.lock\nInstalling dependencies\nResolved 61 packages in 14ms\n   Built try-kedro-init @ file:///home/galen/projects/try-kedro-init\nDownloaded 1 package in 217ms\nUninstalled 1 package in 0.29ms\nInstalled 5 packages in 45ms\n + numpy==2.0.0\n + pandas==2.2.2\n + pytz==2024.1\n - try-kedro-init==0.1.0 (from file:///home/galen/projects/try-kedro-init)\n + try-kedro-init==0.1.0 (from file:///home/galen/projects/try-kedro-init)\n + tzdata==2024.1\nDone!\nFinally, run the Kedro pipeline:\n$ rye run kedro run\n[07/01/24 09:18:48] INFO     Kedro project try-kedro-init                                                                                                                                                                     session.py:324\n[07/01/24 09:18:49] INFO     Using synchronous mode for loading and saving data. Use the --async flag for potential performance gains.                                                                               sequential_runner.py:64\n                             https://docs.kedro.org/en/stable/nodes_and_pipelines/run_a_pipeline.html#load-and-save-asynchronously                                                                                                          \n                    INFO     Loading data from example_dataset (CSVDataset)...                                                                                                                                           data_catalog.py:508\n                    INFO     Running node: print([example_dataset]) -&gt; None                                                                                                                                                      node.py:361\n   ID            Name  Age                       Email\n0   1        John Doe   28        john.doe@example.com\n1   2      Jane Smith   34      jane.smith@example.com\n2   3     Bob Johnson   45     bob.johnson@example.com\n3   4  Alice Williams   23  alice.williams@example.com\n4   5   Michael Brown   37   michael.brown@example.com\n                    INFO     Completed 1 out of 1 tasks                                                                                                                                                              sequential_runner.py:90\n                    INFO     Pipeline execution completed successfully.                                                                                                                                                        runner.py:119\nMy provisional conclusion is that Kedro and Rye are compatible."
  },
  {
    "objectID": "posts/combining-kedro-with-rye/index.html#versions",
    "href": "posts/combining-kedro-with-rye/index.html#versions",
    "title": "Combining Kedro with Rye",
    "section": "Versions",
    "text": "Versions\nRye configuration:\n$ rye --version\nrye 0.35.0\ncommit: 0.35.0 (a1dbc56d4 2024-06-24)\nplatform: linux (x86_64)\nself-python: cpython@3.12.3\nsymlink support: true\nuv enabled: true\nMy operating system:\n$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 22.04.4 LTS\nRelease:        22.04\nCodename:       jammy"
  },
  {
    "objectID": "posts/quantecon-1-scipy-submodules/index.html",
    "href": "posts/quantecon-1-scipy-submodules/index.html",
    "title": "QuantEcon 1 Scipy Submodules",
    "section": "",
    "text": "As someone with previous background in Python, I’ve been blasting my way through the basics of the Quantecon curricula. One of the joys of self-directed learning is that, with discipline, you can speed through familar material and really camp out with the new material. With that in mind, I’ve decided to further play with finding solutions (x-intercepts) of some single variable functions.\nFirst of all, let’s find ourselves an interesting function. I’ve chosen \\(f(x) = \\sin(x) \\exp(-x)\\) because I’ve always enjoyed its degradating oscillations, but also because I expect this equation to have solutions. Since any integer multiple \\(k\\) of \\(\\pi\\) will result in \\(\\sin(x) = 0\\) when \\(x = k \\pi\\), we know that \\(f(k \\pi) = 0\\) as well. While I’m quite late (or too early, depending on how you see it) for calculating \\(\\pi\\) on \\(\\pi\\) Day, let’s take \\(k = 1\\) to find \\(\\pi\\) anyway!\n\nBisection Method\nThe first method mentioned on QuantEcon is the bisection algorithm, which essentially treats finding solutions to a function as a binary search problem. There are two parameters that are needed to get started with the bisection algorithm, an initial lower bound and an initial upper bound on the search space. Not only do we need two such parameters, but our choice of these two numbers can change what solution is found. Let’s consider the following example where we look on the interval \\([-10, 10]\\).\n\nimport numpy as np\nfrom scipy.optimize import bisect\n# Define a single-variable function to find solutions in\nf = lambda x: np.sin(x) * np.exp(-x)\n# try out the bisection algorithm\nprint(bisect(f, -10, 10))\n\n0.0\n\n\nWe were looking for \\(x = \\pi\\), but we got \\(x = 0\\) instead. If there are multiple solutions within your search interval, the algorithm won’t necessarily converge on the one that you wanted, nor will it report to you there were multiple solutions. Knowing ahead of time that we’d like to calculate \\(\\pi\\), and that \\(3 &lt; \\pi &lt; 4\\), let’s rerun the bisection algorithm on \\([3, 4]\\).\n\nprint(bisect(f, 3, 4))\n\n3.1415926535901235\n\n\nThat gives us a value pretty close to \\(\\pi\\), correct to the \\(11\\)th digit anyway.\n\n\nNewton-Raphson method\nThe Newton-Raphson method is a calculus-based method that iteratively steps towards a solution. Like the bisection method, it requires a number decided ahead of time but this time this chosen number is an initial guess or starting point. Unlike the bisection method, the Newton-Raphson method does not have bounds set on the search so a continuous function over the real numbers can be searched indefinitely. To prevent the algorithm searching for too long, a hyperparameter limiting the number of iterations (steps) is included if a stable solution is not converged upon (default is \\(50\\) steps).\n\nfrom scipy.optimize import newton\n# Define a single-variable function to find solutions in\n# try out the Newton-Raphson algorithm\nprint(newton(f, 0.2))\n\n3.6499361606787994e-14\n\n\nWhile \\(x = 0.2\\) is not that far off from Pi, the local derivatives are going to point the steps to descend toward zero. Notice that the solution we got was not exactly zero, but rather the first solution found within a predefined tolerance of \\(1.48 \\times 10^{-8}\\). What you don’t see from the code is the that shape of the curve, which if you plot our function you’ll see there is a local maxima between \\(x = 0\\) and \\(x = \\pi\\) at \\(x = \\frac{\\pi}{4}\\). Relative to this hill, our estimate is analogous to a ball rolling in the direction of steepest descent. This analogy breaks down for solutions separated by a local minima as the method is not equivalent to steepest descent even though it is based on the local derivative. Another issue that can come about is picking an initial value close to an extrema because the results can be unstable, allowing incredibly large jumps across the domain. Therefore, we should be cautious about our choice of initial guess by doing some exploration of function’s properties before attempting to estimate the solution. Let’s retry with a more suitable initial value.\n\n# try out the Newton-Raphson algorithm\nprint(newton(f, np.pi / 4 + 1))\n\n3.1415926535897936\n\n\nThat is clearly closer to \\(\\pi\\) than \\(3.6499361606787994 \\times 10^{-14}\\), and being accurate for the first \\(16\\) digits suggests that it was more precise than the bisection algorithm under these parameters.\n\n\nBrent’s method\nThe QuantEcon course points out that Bisection is more robust (stable) than Newton-Raphson’s method, but it is also slower. An alternative approach that balances this tradeoff is Brent’s method which includes bounds and garantees solutions for computable functions. Let’s give this approach a try on our function on \\([3, 4]\\).\n\nfrom scipy.optimize import brentq\n# try out the Brent's algorithm\nprint(brentq(f, 3, 4))\n\n3.141592653589788\n\n\nLooks like this estimation of \\(\\pi\\) was correct for the first \\(13\\) digits, which was better than Bisection but worse than Newton-Raphson.\n\n\nPerformance comparison with timeit\nLast of all, it would be interesting to compare the time performance of each of these solution-finding approaches. Let’s do that with timeit.\n\nfrom timeit import timeit\nprint(timeit(stmt='bisect(f, 3, 4)',\\\n    globals={'bisect':bisect, 'f':f},\\\n    number=100000) / 100000)\nprint(timeit(stmt='newton(f, np.pi / 4 + 1)',\\\n    globals={'newton':newton, 'f':f, 'np':np},\\\n    number=100000) / 100000)\nprint(timeit(stmt='brentq(f, 3, 4)',\\\n    globals={'brentq':brentq, 'f':f},\\\n    number=100000) / 100000)\n\n0.00010017173054002341\n0.0001319278220400156\n2.3580086980000487e-05\n\n\nWe find under this setup that the slowest algorithm was the Newton-Raphson’s method, followed by the bisection method by a factor of \\(\\frac{1}{5}\\), and final Brent’s method being about an order of magnitude faster! So Brent’s method gave us more accurate digits in the solution, at least for \\(x = \\pi\\), and also performed faster than the other two methods. Does this mean that Brent’s method is always the best method? Not necessarily. We should be open to the possibility of tradeoffs not discussed on QuantEcon, as well as there being a panoply of algorithms out available in code repositories."
  },
  {
    "objectID": "posts/how-to-compile-python-on-linux/index.html",
    "href": "posts/how-to-compile-python-on-linux/index.html",
    "title": "How to Compile CPython on Debian-Based Linux",
    "section": "",
    "text": "This is a short blog post to remind myself how to compile CPython from its source.\nYou need to get the source files for Python as you need to give the C compiler (and other tools) the needed instructions for producing machine code. The CPython source is available on Github. Using git, you can download with\n$ git clone https://github.com/python/cpython.git\nYou should install build-essential which provides tooling for building Debian packages. This can be done with apt:\n$ sudo apt install build-essential\nNext install these assorted packages:\n$ sudo apt install libssl-dev zlib1g-dev libncurses5-dev libncursesw5-dev libreadline-dev libsqlite3-dev libgdbm-dev libdb5.3-dev libbz2-dev liblzma-dev libffi-dev\nHere is a brief description of each package:\n\n\n\nInstall\nPackage\nDescription\n\n\n\n\nlibssl-dev\nSecure Sockets Layer toolkit - development files\nThis package is part of the OpenSSL project’s implementation of the SSL and TLS crypographic protocols for secure communication over the internet.\n\n\nzlib1g-dev\nCompression library - development\nzlib is a library implementing the deflate compression method found in gzip and PKZIP.\n\n\nlibncurses5-dev\nTransitional pacakge for libncurses-dev\nPackage prociding libncurses5-dev.\n\n\nlibncursesw5-dev\nTransitional package for libncurses-dev\nPackage providing libncursesw5-dev.\n\n\nlibreadline-dev\nGNU readline and history libraries, development files\nThe GNU readline library aids in the consistency of user interface across discrete programs that need to provide a command line interace\n\n\nlibsqlite3-dev\nlibsqlite3-dev\nSQLite is a C library that implements an SQL database engine.\n\n\nlibgdbm-dev\nGNU dbm database routines (development files)\nGNU dbm (‘gdbm’) is a library of database functions that use extensible hashing and works similarly to the standard UNIX ‘dbm’ functions.\n\n\nlibdb5.3-dev\nBerkeley v5.3 Database Libraries [development]\nThis is the development package which contains headers and static libraries for the Berkely v5.3 database library\n\n\nlibbz2-dev\nHigh-quality block-sorting file compressor library - development\nStatic libraries and include files for the bzip2 compressor library\n\n\nliblzma-dev\nXZ-format compression library - development files\nXZ is the successor to the Lempel-Ziv/Markov-chain Algorithm compression format, which provides memory-hungry but powerful compressoin (often better than bzip2) and fast, easy decompression.\n\n\nlibffi-dev\nForeign Fucntion Interface library (development files)\nThis package contains the headers and static library files necessary for building programs which use libffi\n\n\n\nNow run the configuration tool that the Python dev’s have kindly prepared. It will prepare a makefile for everything you need to build CPython.\n$ ./configure --with-pydebug\nThe --with-pydebug tells configure to use a debugging hook.\nFinally, you can just run make.\n$ make\nThat’s pretty much it. If you want to silence the large standard output, you can add an -s. By default make will compile the first target specified in the make file, which for this project is actually the entirety of CPython itself. You can specify special build targets related to building, testing, installation, and other topics."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Galen Seilis",
    "section": "",
    "text": "Galen is a science and technology enthusiast dedicated to life-long learning. Through the application of ethics, reason, and observation, he believes we can tackle problems in medicine, the environment, economics, and exploration.\n\nData Scientist 📈\n\n\n👀 I’m interested in Bayesian inference, causal inference, mathematical modelling, simulation, and network analysis.\n🌱 Always learning.\n\n\nSome Tools I Enjoy\n                  \n\n\nElsewhere"
  }
]